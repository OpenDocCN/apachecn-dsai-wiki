# BAT 机器学习 1000 题 601-700


## 601题

以下对于t-SNE和PCA的陈述中哪个是正确的？

A、t-SNE是线性的，而PCA是非线性的

B、t-SNE和PCA都是线性的

C、t-SNE和PCA都是非线性的

D、t-SNE是非线性的，而PCA是线性的



正确答案是：D

## 602题

在t-SNE算法中，可以调整以下哪些超参数？

A、维度数量

B、平稳测量有效数量的邻居

C、最大迭代次数

D、以上所有



正确答案是：D

解析：

选项中的所有超参数都可以调整。

## 603题

与PCA相比，t-SNE的以下说明哪个正确？

A、数据巨大（大小）时，t-SNE可能无法产生更好的结果。

B、无论数据的大小如何，T-NSE总是产生更好的结果。

C、对于较小尺寸的数据，PCA总是比t-SNE更好。

D、都不是



正确答案是：A

## 604题

Xi和Xj是较高维度表示中的两个不同点，其中Yi和Yj是较低维度中的Xi和Xj的表示。

1)数据点Xi与数据点Xj的相似度是条件概率p（j | i）。

2)数据点Yi与数据点Yj的相似度是条件概率q（j | i）。

对于在较低维度空间中的Xi和Xj的完美表示，以下哪一项必须是正确的？


A、p（j | i）= 0，q（j | i）= 1

B、p（j | i）

C、p（j | i）= q（j | i）

D、P（j | i）> q（j | i）



正确答案是：C

解析：

两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示。

## 605题

对于投影数据为(( √2)，(0)，(√2))。现在如果在二维空间中重建，并将它们视为原始数据点的重建，那么重建误差是多少？

A、0％

B、10％

C、30％

D、40％



正确答案是：A

解析：

重建误差为0，因为所有三个点完全位于第一个主要分量的方向上或者计算重建;

## 606题

LDA的以下哪项是正确的？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibuWFmJHncqVvySUXIsJ3eiaY9xMgSQAoar3YoibAZ5O969vK7rZu2NsldibZuYZiczjHRB7Z2JWHHvqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、LDA旨在最大化之间类别的距离，并最小化类内之间的距离

B、LDA旨在最小化类别和类内之间的距离

C、LDA旨在最大化类内之间的距离，并最小化类别之间的距离

D、LDA旨在最大化类别和类内之间的距离



正确答案是：A

## 607题

LDA的思想是找到最能区分两类别之间的线，下图中哪个是好的投影？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibuWFmJHncqVvySUXIsJ3eiafl9A3e5I4NDOyqoBI8qmQmotoRfm4m55PBobYGBicnmaWA3icMBibQXYg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、LD1

B、LD2

C、两者

D、都不是



正确答案是：A

## 608题

以下哪种情况LDA会失败？

A、如果有辨识性的信息不是平均值，而是数据的方差

B、如果有辨识性的信息是平均值，而不是数据方差

C、如果有辨识性的信息是数据的均值和方差

D、都不是



正确答案是：A

## 609题

PCA和LDA的以下比较哪些是正确的？

1)LDA和PCA都是线性变换技术


2) LDA是有监督的，而PCA是无监督的

3) PCA最大化数据的方差，而LDA最大化不同类之间的分离

A、1和2

B、1和3

C、只有3

D、1、2和3



正确答案是：D

## 610题

PCA是一种很好的技术，因为它很容易理解并通常用于数据降维。获得特征值λ1≥λ2≥•••≥λN并画图。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibuWFmJHncqVvySUXIsJ3eiacI4icDAr2buNkpgKXYP0cRMqiaHm8UyK7VGhCynrmicGExGOwoiar7bKBA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

看看f(M)（贡献率）如何随着M而增加，并且在M = D处获得最大值1，给定两图：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibuWFmJHncqVvySUXIsJ3eiaJfWAonsK52QYUoU3jJeFWbJpehCiane5BYyDia1llSFw1t5Tgz7w0v2w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上述哪个图表显示PCA的性能更好？其中M是主要分量，D是特征的总数。

A、左图

B、右图



正确答案是：A

解析：

如果f（M）渐近线快速到达1，则PCA是好的；如果第一个特征值较大且其余较小，则会发生这种情况。如果所有特征值大致相等，PCA是坏的。

## 611题

以下哪个选项是真的？

A、LDA明确地尝试对数据类别之间的差异进行建模，而PCA没有。

B、两者都试图模拟数据类之间的差异。

C、PCA明确地试图对数据类别之间的差异进行建模，而LDA没有。

D、两者都不试图模拟数据类之间的差异。



正确答案是：A

## 612题

应用PCA后，以下哪项可以是前两个主成分？

1) (0.5,0.5,0.5,0.5)和(0.71,0.71,0,0)

2) (0.5,0.5,0.5,0.5)和(0,0，-0.71,0.71) 

3) (0.5,0.5,0.5,0.5)和(0.5,0.5,-0.5,-0.5) 

4) (0.5,0.5,0.5,0.5)和(-0.5,-0.5,0.5,0.5)

A、1和2

B、1和3

C、2和4

D、3和4



正确答案是：D

解析：

对于前两个选择，两个向量不是正交的。

## 613题

以下哪一项给出了逻辑回归与LDA之间的差异？

1) 如果类别分离好，逻辑回归的参数估计可能不稳定。 

2) 如果样本量小，并且每个类的特征分布是正常的。在这种情况下，线性判别分析比逻辑回归更稳定。


A、1

B、2

C、1和2

D、都不是



正确答案是：C

## 614题

在PCA中会考虑以下哪个偏差？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9GTqFZaJ84kjiaQDGYdkRXx1YasCtia86hkoub8jN03IicTKglCyWYBo5QAVNiaUqbeG2D3Xfwx0xlSA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、垂直偏移

B、正交偏移

C、两者

D、都不是



正确答案是： B

解析：

总是将残差视为垂直偏移，正交偏移在PCA的情况下是有用的。

## 615题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9GTqFZaJ84kjiaQDGYdkRXxyJ19dLhGV84yI6KfZmZZwn8aicXD82rqq6pibcqUqBZOydYB25D1PWJg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图中哪一个显示了决策边界过度拟合训练数据？

A、A

B、B

C、C

D、这些都没有



正确答案是：C

解析：

答案：C由于在图3中，决策边界不平滑，表明其过度拟合数据。

## 616题

假设正在处理10类分类问题，并且想知道LDA最多可以产生几个判别向量。以下哪个是正确答案？

A、20

B、9

C、21

D、11



正确答案是： B

解析：

LDA最多产生c-1个判别向量。

## 617题

给定的数据集包括“胡佛塔”和其他一些塔的图像。现在要使用PCA（特征脸）和最近邻方法来构建一个分类器，可以预测新图像是否显示“胡佛塔”。该图给出了输入的训练图像样本

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9BDmhicibxdcKLQqAGSghs3RGXmbHaN1SLV6GkguknjTf8uCnS0z0O2icu0DFEKc51XjIOHIdOEWGjA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了从“特征脸”算法获得合理的性能，这些图像将需要什么预处理步骤？ 

1)将塔对准图像中相同的位置。

2) 将所有图像缩放或裁剪为相同的大小。

A、1

B、2

C、1和2

D、都不是



正确答案是：C

## 618题

下图中主成分的最佳数量是多少？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9BDmhicibxdcKLQqAGSghs3RvicxeQ4SRnK9cVdu0QYgdobhnJRwaiaFFFTDMia4xVysO0GMUasUXAQXQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、7

B、30

C、40

D、不知道



正确答案是： B

解析：

可以在上图中看到，主成分的数量为30时以最小的数量得到最大的方差。

## 619题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9BDmhicibxdcKLQqAGSghs3REzSFsrPwuTib1iakMYwKNiaED7iaBtMxARk2LxrStegLUdqx4vMaWicLOxw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

正则化项惩罚度最高的是？

A、A

B、B

C、C

D、都具有相同的正则化



正确答案是：A

解析：

答案：A因为正则化意味着更多的罚值和图A所示的较简单的决策界限。

## 620题

下图显示了三个逻辑回归模型的AUC-ROC曲线。不同的颜色表示不同超参数值的曲线。以下哪个AUC-ROC会给出最佳结果？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9BDmhicibxdcKLQqAGSghs3Rux0zWnkl4pMMZmDN6HjJfwcFeso7DfyjbnvLzsDh6kTllDHfLrTgwA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、黄色

B、粉红色

C、黑色

D、都相同



正确答案是：A

解析：

答案：A最佳分类是曲线下区域面积最大者，而黄线在曲线下面积最大。

## 621题

如果对相同的数据进行逻辑回归，将花费更少的时间，并给出比较相似的精度（也可能不一样），怎么办？假设在庞大的数据集上使用Logistic回归模型。可能遇到一个问题，Logistic回归需要很长时间才能训练。

A、降低学习率，减少迭代次数

B、降低学习率，增加迭代次数

C、提高学习率，增加迭代次数

D、增加学习率，减少迭代次数



正确答案是：D

解析：

答案：D如果在训练时减少迭代次数，就能花费更少的时间获得相同的精度，但需要增加学习率。

## 622题

Logistic regression（逻辑回归）是一种监督式机器学习算法吗？

A、是

B、否



正确答案是：A

解析：

当然，Logistic regression是一种监督式学习算法，因为它使用真假标签进行测试。 测试模型时，监督式学习算法应具有输入变量（x）和目标变量（Y）。

## 623题

Logistic Regression主要用于回归吗？

A、是

B、否



正确答案是： B

解析：

逻辑回归是一种分类算法，不要因为名称将其混淆。

## 624题

是否能用神经网络算法设计逻辑回归算法？

A、是

B、否



正确答案是：A

解析：

是的，神经网络是一种通用逼近器，因此能够实现线性回归算法。

## 625题

是否可以对三分问题应用逻辑回归算法？

A、是

B、否



正确答案是：A

解析：

当然可以对三分问题应用逻辑回归，只需在逻辑回归中使用One Vs all方法。

## 626题

以下哪种方法能最佳地适应逻辑回归中的数据？

A、Least Square Error

B、Maximum Likelihood

C、Jaccard distance

D、Both A and B



正确答案是： B

解析：

Logistic Regression使用可能的最大似然估值来测试逻辑回归过程。

## 627题

在逻辑回归输出与目标对比的情况下，以下评估指标中哪一项不适用？

A、AUC-ROC

B、准确度

C、Logloss

D、均方误差



正确答案是：D

解析：

因为Logistic Regression是一个分类算法，所以它的输出不能是实时值，所以均方误差不能用于评估它。

## 628题

如下逻辑回归图显示了3种不同学习速率值的代价函数和迭代次数之间的关系（不同的颜色在不同的学习速率下显示不同的曲线）。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8VRzichNh1904lN1XUibnzy8VQ4R1fN69crJuZcrIA43N3UicDYF4NDwIoylAcciaEztYqlRQxgOiaSibg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了参考而保存图表后，忘记其中不同学习速率的值。现在需要曲线的倾斜率值之间的关系。以下哪一个是正确的？ 

注：

1)蓝色的学习率是L1

2)红色的学习率是L2

3)绿色学习率为lL3

A、L1> L2> L3

B、L1 = L2 = L3

C、L1

D、都不是



正确答案是：C

解析：

答案：C如果学习速率低下，代价函数将缓慢下降，学习速度过高，则其代价函数会迅速下降。

## 629题

分析逻辑回归表现的一个良好的方法是AIC，它与线性回归中的R平方相似。有关AIC，以下哪项是正确的？

A、具有最小AIC值的模型更好

B、具有最大AIC值的模型更好

C、视情况而定

D、以上都不是



正确答案是：A

解析：

AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。 

考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。

综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。有关更多信息，请参阅此来源：www4.ncsu.edu/~shu3/Presentation/AIC.pdf

## 630题

在训练逻辑回归之前需要对特征进行标准化。

A、是

B、否



正确答案是： B

解析：

逻辑回归不需要标准化。功能标准化的主要目标是帮助优化技术组合。

## 631题

选择Logistic回归中的One-Vs-All方法中的哪个选项是真实的。

A、我们需要在n类分类问题中适合n个模型

B我们需要适合n-1个模型来分类为n个类

C、我们需要只适合1个模型来分类为n个类

D、这些都没有



正确答案是：A

解析：

如果存在n个类，那么n个单独的逻辑回归必须与之相适应，其中每个类的概率由剩余类的概率之和确定。

## 632题

使用以下哪种算法进行变量选择？

A、LASSO

B、Ridge

C、两者

D、都不是



正确答案是：A

解析：

使用Lasso的情况下，我们采用绝对罚函数，在增加Lasso中罚值后，变量的一些系数可能变为零。

## 633题

以下是两种不同的对数模型，分别为β0和β1。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8PLXwiaKVUC9Aaibia4MjbJoX6L5aVmjRegBlpU0lKYsX60LayC8jvLQpSLA1tmRVYBgF2vgLticvsqQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对于两种对数模型（绿色，黑色）的β0和β1值，下列哪一项是正确的？注： Y =β0+β1* X。其中β0是截距，β1是系数。

A、绿色的β1大于黑色

B、绿色的β1小于黑色

C、两种颜色的β1相同

D、不能说



正确答案是： B

解析：

β0和β1：β0= 0，β1= 1为X1颜色（黑色），β0= 0，β1= -1为X4颜色（绿色）

## 634题

逻辑回归的以下模型：P（y = 1 | x，w）= g（w0 + w1x）其中g（z）是逻辑函数。在上述等式中，通过改变参数w可以得到的P（y = 1 | x; w）被视为x的函数。

A、（0，inf）

B、（-inf，0）

C、（0,1）

D、（-inf，inf）



正确答案是：C

解析：

对于从-∞到+∞的实数范围内的x的值。逻辑函数将给出（0,1）的输出。

## 635题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8PLXwiaKVUC9Aaibia4MjbJoXJz8m5EhRjLicn34ZggnB8UXxwqnFusc8UYdBlTx1okMjEDX7LoschHg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图中哪一个显示了决策边界过度拟合训练数据？

A、A

B、B

C、C

D、这些都没有



正确答案是：C

解析：

由于在图3中，决策边界不平滑，表明其过度拟合数据。

## 636题

逻辑回归的以下模型：P（y = 1 | x，w）= g（w0 + w1x）其中g（z）是逻辑函数。在上述等式中，通过改变参数w可以得到的P（y = 1 | x; w）被视为x的函数。在上面的问题中，你认为哪个函数会产生（0,1）之间的p？

A、逻辑函数

B、对数似然函数

C、两者的复合函数

D、都不会



正确答案是：A

解析：

对于从-∞到+∞的实数范围内的x的值。逻辑函数将给出（0,1）的输出。

## 637题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PdmoFJpl8jBHB9uu8FRC6Qs1scAcdp79YAX7ibFZL8yEibhDwCNCpBWhg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

根据可视化后的结果，能得出什么结论？

1)与第二和第三图相比，第一幅图中的训练误差最大

2)该回归问题的最佳模型是最后（第三个）图，因为它具有最小的训练误差（零）

3)第二个模型比第一个和第三个更强，它在不可见数据中表现最好

4)与第一种和第二种相比，第三种模型过度拟合了

5)所有的模型执行起来都一样，因为没有看到测试数据。

A、1和3

B、1和3

C、1,3和4

D、5



正确答案是：C

解析：

图中趋势像是自变量X的二次趋势。更高次方的多项式（右图）可能对训练中的数据群具有超高的精度，但预计在测试数据集上将会严重失败。但是在左图中可以测试最大错误值，因为适合训练数据

## 638题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PdmoFJpl8jBHB9uu8FRC6Qs1scAcdp79YAX7ibFZL8yEibhDwCNCpBWhg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

假设上述决策边界是针对不同的正则化（regularization）值生成的。那么其中哪一个显示最大正则化？

A、A

B、B

C、C

D、都具有相同的正则化



正确答案是：A

解析：

因为正则化意味着更多的罚值和图A所示的较简单的决策界限。

## 639题

下图显示了三个逻辑回归模型的AUC-ROC曲线。不同的颜色表示不同超参数值的曲线。以下哪个AUC-ROC会给出最佳结果？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PzcPUXwT5T9yfuT4jg9FSjicq6O5ZicS4kKEvdpPZNX5yzypcJH7ER8Fg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、黄色

B、粉红色

C、黑色

D、都相同



正确答案是：A

解析：

最佳分类是曲线下区域面积最大者，而黄线在曲线下面积最大。

## 640题

假设你在测试逻辑回归分类器，设函数H为

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PgudDBxpAbwGSZvh1IpaqZ0wJtw2j5WS71icVFrme0ygfhhPrrpwh4sw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

下图中的哪一个代表上述分类器给出的决策边界？

A

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PBzIKwsRTR6XUMlPRoB3DCy8x1ia4ql5zLEiaqSbS0DuORYOtNoPFNn0g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

B

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PzTLTfdfQYdkZ6HsIziaJ0yzkdRK76SwGXLJAnpwZQhGcRibhYXWaIWag/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

C

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6P1ibLxa6lZhFQb3Y6ITSbyHrADbTM8PcnRA2f2G0JckGibcguZ5weokng/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



正确答案是： B

解析：

选项B正确。虽然我们的式子由选项A和选项B所示的y = g（-6 + x2）表示，但是选项B才是正确的答案，因为当将x2 = 6的值放在等式中时，要使y = g（0）就意味着y = 0.5将在线上，如果你将x2的值增加到大于6，你会得到负值，所以输出将是区域y = 0。

## 641题

所谓几率，是指发生概率和不发生概率的比值。所以，抛掷一枚正常硬币，正面朝上的几率（odds）为多少？

A、0.5

B、1

C、都不是



正确答案是： B

解析：

几率（odds）是事件发生不发生概率的比率，正面朝上概率为1/2和反面朝上的概率都为1/2，所以几率为1。

## 642题

Logit函数（给定为l（x））是几率函数的对数。域x = [0,1]中logit函数的范围是多少？

A、（ - ∞，∞）

B、（0,1）

C、（0，∞）

D、（ - ∞，0）



正确答案是：A

解析：

为了与目标相适应，几率函数具有将值从0到1的概率函数变换成值在0和∞之间的等效函数的优点。当我们采用几率函数的自然对数时，我们便能范围是-∞到∞的值。

## 643题

如果对相同的数据进行逻辑回归，将花费更少的时间，并给出比较相似的精度（也可能不一样），怎么办？（假设在庞大的数据集上使用Logistic回归模型。可能遇到一个问题，Logistic回归需要很长时间才能训练。）

A、降低学习率，减少迭代次数

B、降低学习率，增加迭代次数

C、提高学习率，增加迭代次数

D、增加学习率，减少迭代次数



正确答案是：D

解析：

如果在训练时减少迭代次数，就能花费更少的时间获得相同的精度，但需要增加学习率。

## 644题

以下哪些选项为真？

A、线性回归误差值必须正态分布，但是在Logistic回归的情况下，情况并非如此

B、逻辑回归误差值必须正态分布，但是在线性回归的情况下，情况并非如此

C、线性回归和逻辑回归误差值都必须正态分布

D、线性回归和逻辑回归误差值都不能正态分布



正确答案是：A

解析：

只有A是真的。请参考教程 czep.net/stat/mlelr.pdf

## 645题

以下哪个图像显示y = 1的代价函数？以下是两类分类问题的逻辑回归（Y轴损失函数和x轴对数概率）的损失函数。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS9j7bZSUXI44JzaJaueJA6PVmOkrIicxdtWsibRKialsv0orQOkkmXqlxabjqkUV4ooTErH7INw48kmQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注：Y是目标类

A、A

B、B

C、两者

D、这些都没有



正确答案是：A

解析：

A正确，因为损失函数随着对数概率的增加而减小

## 646题

以下不属于影响聚类算法结果的主要因素有


A 已知类别的样本质量

B 分类准则

C 特征选取

D 模式相似性测度



正确答案是：A

解析：

都已知了，就不必再进行聚类了。


## 647题

2、模式识别中，不属于马式距离较之于欧式距离的优点的是


A 平移不变性

B 尺度不变性

C 考虑了模式的分布



正确答案是：A

## 648题

3、影响基本K-均值算法的主要因素有


A 样本输入顺序

B 模式相似性测度

C 聚类准则



正确答案是： B

## 649题

4、在统计模式分类问题中，当先验概率未知时，可以使用


A 最小损失准则

B 最小最大损失准则

C 最小误判概率准则



正确答案是： B

## 650题

5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有


A已知类别样本质量

B分类准则

C量纲



正确答案是： B

## 651题

对于任意值“x”，考虑到Logistic（x）：是任意值“x”的逻辑（Logistic）函数Logit（x）：是任意值“x”的logit函数Logit_inv（x）：是任意值“x”的逆逻辑函数以下哪一项是正确的？

A、Logistic（x）= Logit（x）

B、Logistic（x）= Logit_inv（x）

C、Logit_inv（x）= Logit（x）

D、都不是



正确答案是： B

解析：

答案B请参阅此链接以获取答案：https：//en.wikipedia.org/wiki/Logit

## 652题

假设，下图是逻辑回归的代价函数

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8tzXLSBSI2lq7G73kU34mJSnmVURiaNC2nycOxKtJkacwccPFPyTDP0ty9HCI8qIyLicWiaRF9kMbQg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

现在，图中有多少个局部最小值？

A、1

B、2

C、3

D、4



正确答案是：D

解析：

图中总共有四个凹的地方，故有四个局部最小值。

## 653题

使用 high(infinite) regularisation时偏差会如何变化？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8tzXLSBSI2lq7G73kU34mJJA0kSIV8ZFoZt0g2A8kQw2NDibqIbfWtpAVoNZ0WPI6eKG3CUsT6ia9Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

有散点图“a”和“b”两类（蓝色为正，红色为负）。在散点图“a”中，使用了逻辑回归（黑线是决策边界）对所有数据点进行了正确分类。

A、偏差很大

B、偏差很小

C、不确定

D、都不是



正确答案是：A

解析：

模型变得过于简单，所以偏差会很大。

## 654题

Logistic回归分类器是否能对下列数据进行完美分类？

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8tzXLSBSI2lq7G73kU34mJR6xQRF58lU94ic1axcSxYNsicibkv1Rwgjxk5Cs0jo1Bbic7SLbmicicNYuw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注：只可使用X1和X2变量，且只能使用两个二进制值（0,1）。

A、是

B、否

C、不确定

D、都不是



正确答案是： B

解析：

逻辑回归只能形成线性决策面，而图中的例子并非线性可分的。

## 655题

假设对给定数据应用了Logistic回归模型，并获得了训练精度X和测试精度Y。现在要在同一数据中添加一些新特征，以下哪些是错误的选项。注：假设剩余参数相同。

A、训练精度提高

B、训练准确度提高或保持不变

C、测试精度提高或保持不变



正确答案是： B

解析：

将更多的特征添加到模型中会增加训练精度，因为模型必须考虑更多的数据来适应逻辑回归。但是，如果发现特征显着，则测试精度将会增加。

## 656题

选择Logistic回归中的One-Vs-All方法中的哪个选项是真实的。

A、我们需要在n类分类问题中适合n个模型

B、我们需要适合n-1个模型来分类为n个类

C、我们需要只适合1个模型来分类为n个类

D、这些都没有



正确答案是：A

解析：

答案：A如果存在n个类，那么n个单独的逻辑回归必须与之相适应，其中每个类的概率由剩余类的概率之和确定。

## 657题

假设有一个如下定义的神经网络：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8jdbZr7w23u1Dq4Fetf4gkZGhvJ2gTsaufcr7OpnCoMRRA4J9Z0hAplEhUHtibXLRbjWAUiauspsCA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果我们去掉ReLU层，这个神经网络仍能够处理非线性函数，这种说法是：

A、正确的

B、错误的



正确答案是： B

## 658题

假定特征 F1 可以取特定值：A、B、C、D、E 和 F，其代表着学生在大学所获得的评分。在下面说法中哪一项是正确的？

A、特征 F1 是名义变量（nominal variable）的一个实例。

B、特征 F1 是有序变量（ordinal variable）的一个实例。

C、该特征并不属于以上的分类。

D、以上说法都正确。



正确答案是： B

解析：

答案为（B）：有序变量是一种在类别上有某些顺序的变量。例如，等级 A 就要比等级 B 所代表的成绩好一些。

## 659题

下面哪个选项中哪一项属于确定性算法？

A、PCA

B、K-Means

C、以上都不是



正确答案是：A

解析：

答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

## 660题

两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。

A、正确

B、错误



正确答案是：A

解析：

答案为（A）：Y=X2，请注意他们不仅仅相关联，同时一个还是另一个的函数。尽管如此，他们的相关性系数还是为 0，因为这两个变量的关联是正交的，而相关性系数就是检测这种关联。详情查看：https://en.wikipedia.org/wiki/Anscombe's_quartet

## 661题

下面哪一项对梯度下降（GD）和随机梯度下降（SGD）的描述是正确的？

1 在 GD 和 SGD 中，每一次迭代中都是更新一组参数以最小化损失函数。

 2 在 SGD 中，每一次迭代都需要遍历训练集中的所有样本以更新一次参数。

 3 在 GD 中，每一次迭代需要使用整个训练集或子训练集的数据更新一个参数。

A、只有 1

B、只有 2

C、只有 3

D、都正确



正确答案是：A

解析：

答案为（A）：在随机梯度下降中，每一次迭代选择的批量是由数据集中的随机样本所组成，但在梯度下降，每一次迭代需要使用整个训练数据集。

## 662题

下面哪个/些超参数的增加可能会造成随机森林数据过拟合？

1 树的数量 

2 树的深度

3 学习速率

A、只有 1

B、只有 2

C、只有 3

D、都正确



正确答案是： B

解析：

答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率在随机森林中并不是超参数。增加树的数量可能会造成欠拟合。

## 663题

假如你在「Analytics Vidhya」工作，并且想开发一个能预测文章评论次数的机器学习算法。你的分析的特征是基于如作者姓名、作者在 Analytics Vidhya 写过的总文章数量等等。那么在这样一个算法中，你会选择哪一个评价度量标准？

1 均方误差

2 精确度

3 F1 分数

A、 只有 1

B、只有 2

C、只有 3



正确答案是：A

解析：

答案为（A）：你可以把文章评论数看作连续型的目标变量，因此该问题可以划分到回归问题。因此均方误差就可以作为损失函数的度量标准。

## 664题

给定以下三个图表（从上往下依次为1，2，3）. 哪一个选项对以这三个图表的描述是正确的？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8dIWxoibjXhcSCK2OAROv6obWfRELPJKe2eDJGbM0Y6vicbataspsed7jhcbrOabhjQL3HyTCOX8nQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8dIWxoibjXhcSCK2OAROv6omBFOiaUwiarh9rib1lJ2laOwV4eJNEiao26ZiagKp14Xen7JG2HRRQypuGQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8dIWxoibjXhcSCK2OAROv6oOw93tGEWticmzWbVDIGpfKHTTaIHsabm5kOlibXq5LVusNr1oO0LTIcA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、1 是 tanh，2 是 ReLU，3 是 SIGMOID 激活函数

B、1 是 SIGMOID，2 是 ReLU，3 是 tanh 激活函数

C、1 是 ReLU，2 是 tanh，3 是 SIGMOID 激活函数

D、1 是 tanh，2 是 SIGMOID，3 是 ReLU 激活函数



正确答案是：D

解析：

答案为（D）：因为 SIGMOID 函数的取值范围是 [0,1]，tanh 函数的取值范围是 [-1,1]，RELU 函数的取值范围是 [0,infinity]。

## 665题

以下是目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是所少？

A、-(5/8 log(5/8) + 3/8 log(3/8))

B、5/8 log(5/8) + 3/8 log(3/8)

C、3/8 log(5/8) + 5/8 log(3/8)

D、5/8 log(3/8) – 3/8 log(5/8)



正确答案是：A

解析：

答案为（A）：信息熵的公式为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8dIWxoibjXhcSCK2OAROv6otYssvCGbVg0OJA8NzvCauQmogdoxpHKTpSCMn0TlZ5h7bPUneQuZKw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 666题

假定你正在处理类属特征，并且没有查看分类变量在测试集中的分布。现在你想将 one hot encoding（OHE）应用到类属特征中。那么在训练集中将 OHE 应用到分类变量可能要面临的困难是什么？

A、分类变量所有的类别没有全部出现在测试集中

B、类别的频率分布在训练集和测试集是不同的

C、训练集和测试集通常会有一样的分布

D、A 和 B 都正确



正确答案是：D

解析：

答案为（D）：A、B 项都正确，如果类别在测试集中出现，但没有在训练集中出现，OHE 将会不能进行编码类别，这将是应用 OHE 的主要困难。选项 B 同样也是正确的，在应用 OHE 时，如果训练集和测试集的频率分布不相同，我们需要多加小心。

## 667题

Skip gram 模型是在 Word2vec 算法中为词嵌入而设计的最优模型。以下哪一项描绘了 Skip gram 模型？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS878iciaBSLMezRnJLtUYkpUPnfXhnOiakOy3icK7Fyc8gAHR6QWIibXJYJpP9Eia1F7z7CxEMgWz2yaWdw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、A

B、B

C、A和B

D、以上都不是



正确答案是： B

解析：

答案为（B）：这两个模型都是在 Word2vec 算法中所使用的。模型 A 代表着 CBOW，模型 B 代表着 Skip gram。

## 668题

假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？

A、ReLU

B、tanh

C、SIGMOID

D、以上都不是



正确答案是： B

解析：

答案为（B）：该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。

## 669题

对数损失度量函数可以取负值。

A、对

B、错



正确答案是： B

解析：

答案为（B）：对数损失函数不可能取负值。

## 670题

下面哪个/些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？

类型 1 通常称之为假正类，类型 2 通常称之为假负类。 

类型 2 通常称之为假正类，类型 1 通常称之为假负类。 

类型 1 错误通常在其是正确的情况下拒绝假设而出现。

A、只有 1

B、只有 2

C、只有 3

D、1和3



正确答案是：D

解析：

答案为（E）：在统计学假设测试中，I 类错误即错误地拒绝了正确的假设（即假正类错误），II 类错误通常指错误地接受了错误的假设（即假负类错误）。

## 671题

假定你想将高维数据映射到低维数据中，那么最出名的降维算法是 PCA 和 t-SNE。现在你将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？

A、X_projected_PCA 在最近邻空间能得到解释

B、X_projected_tSNE 在最近邻空间能得到解释

C、两个都在最近邻空间能得到解释

D、两个都不能在最近邻空间得到解释



正确答案是： B

解析：

答案为（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

## 672题

给定下面两个特征的三个散点图（从左到右依次为图 1、2、3）

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9tA6GE8GZqDpHiaNiaaI0jibJfn6wUHfnicyvuicC4A4d7uicHg1qA6lebCGdvWcS9lO7Qzj9P3axfgBxw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在上面的图像中，哪一个是多元共线（multi-collinear）特征？

A、图 1 中的特征

B、图 2 中的特征

C、图 3 中的特征

D、图 1、2 中的特征



正确答案是：D

解析：

答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

## 673题

在先前问题中，假定你已经鉴别了多元共线特征。那么下一步你可能的操作是什么？

1 移除两个共线变量

2 不移除两个变量，而是移除一个

3 移除相关变量可能会导致信息损失。为了保留这些变量，我们可以使用带罚项的回归模型（如 ridge 或 lasso regression）。

A、只有 1

B、只有 2

C、只有 3

D、2 或 3



正确答案是：D

解析：

答案为（D）：因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。

## 674题

给线性回归模型添加一个不重要的特征可能会造成：

1 增加 R-square

2 减少 R-square

A、只有 1 是对的

B、只有 2 是对的

C、1 或 2 是对的

D、都不对



正确答案是：A

解析：

答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。

## 675题

假设给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？

A、D1= C1, D2 < C2, D3 > C3

B、D1 = C1, D2 > C2, D3 > C3

C、D1 = C1, D2 > C2, D3 < C3

D、D1 = C1, D2 < C2, D3 < C3

E、D1 = C1, D2 = C2, D3 = C3



正确答案是：E

解析：

答案为（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。

## 676题

假定你现在解决一个有着非常不平衡类别的分类问题，即主要类别占据了训练数据的 99%。现在你的模型在测试集上表现为 99% 的准确度。那么下面哪一项表述是正确的？

1 准确度并不适合于衡量不平衡类别问题 

2 准确度适合于衡量不平衡类别问题 

3 精确率和召回率适合于衡量不平衡类别问题 

4 精确率和召回率不适合于衡量不平衡类别问题

A、1 and 3

B、1 and 4

C、2 and 3

D、2 and 4



正确答案是：A

## 677题

在集成学习中，模型集成了弱学习者的预测，所以这些模型的集成将比使用单个模型预测效果更好。下面哪个/些选项对集成学习模型中的弱学习者描述正确？

1 他们经常不会过拟合

2 他们通常带有高偏差，所以其并不能解决复杂学习问题

3 他们通常会过拟合

A、1 和 2

B、1 和 3

C、2 和 3

D、只有 1



正确答案是：A

解析：

答案为（A）：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。

## 678题

下面哪个/些选项对 K 折交叉验证的描述是正确的

1 增大 K 将导致交叉验证结果时需要更多的时间

2 更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心

3 如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量

A、1 和 2

B、2 和 3

C、1 和 3

D、1、2 和 3



正确答案是：D

解析：

答案为（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。

## 679题

为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？

A、将数据转换成零均值

B、将数据转换成零中位数

C、无法做到

D、以上方法不行



正确答案是：A

解析：

答案为（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。

## 680题

假设存在一个黑箱算法，其输入为有多个观察（t1, t2, t3,…….. tn）的训练数据和一个新的观察（q1）。该黑箱算法输出 q1 的最近邻 ti 及其对应的类别标签 ci。你可以将这个黑箱算法看作是一个 1-NN（1-最近邻）能够仅基于该黑箱算法而构建一个 k-NN 分类算法？注：相对于 k 而言，n（训练观察的数量）非常大。

A、可以

B、不可以



正确答案是：A

解析：

答案为（A）：在第一步，你在这个黑箱算法中传递一个观察样本 q1，使该算法返回一个最近邻的观察样本及其类别，在第二步，你在训练数据中找出最近观察样本，然后再一次输入这个观察样本（q1）。该黑箱算法将再一次返回一个最近邻的观察样本及其类别。你需要将这个流程重复 k 次。

## 681题

假设存在一个黑箱算法，其输入为有多个观察（t1, t2, t3,…….. tn）的训练数据和一个新的观察（q1）。该黑箱算法输出 q1 的最近邻 ti 及其对应的类别标签 ci。你可以将这个黑箱算法看作是一个 1-NN（1-最近邻）我们不使用 1-NN 黑箱，而是使用 j-NN(j>1) 算法作为黑箱。为了使用 j-NN 寻找 k-NN，下面哪个选项是正确的？

A、 j 必须是 k 的一个合适的因子

B、j>k

C、不能办到



正确答案是：A

解析：

用 1NN 实现 KNN 的话，每次找到最近邻，然后把这项从数据中取出来，重新运行 1NN 算法，这样重复 K 次，就行了。所以，少找多的话，少一定要是多的因子。

## 682题

有以下 7 副散点图（从左到右分别编号为 1-7），你需要比较每个散点图的变量之间的皮尔逊相关系数。下面正确的比较顺序是？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibvSSygYDVbKgxHT0MwSNibZz2ibNaC3uEgsoW5sPoKRkachG82XlRFVia8G65XIgJHWTAHshO4ZIFQQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1 1<2<3<4 

2 1>2>3 > 4 

3 7<6<5<4 

4 7>6>5>4

A、1 和 3

B、2 和 3

C、1 和 4

D、2 和 4



正确答案是： B

## 683题

你可以使用不同的标准评估二元分类问题的表现，例如准确率、log-loss、F-Score。让我们假设你使用 log-loss 函数作为评估标准。下面这些选项，哪个／些是对作为评估标准的 log-loss 的正确解释。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibvSSygYDVbKgxHT0MwSNibZ5RHr2aBU0l1NujqqcWZ88TicfKSnTOuVibiaJccstkHWm36scMuzAvrow/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。 

2 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。 

3 log-loss 越低，模型越好。

A、1 和 3

B、2 和 3

C、1 和 2

D、1、2、3



正确答案是：D

## 684题

假设你被给到以下数据，你想要在给定的两个类别中使用 logistic 回归模型对它进行分类。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibvSSygYDVbKgxHT0MwSNibZkbRzfeCqA8FibGUFClDdAGDun6qkBkMGl65w2FicMiaN7NpERricO2sMicw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

你正在使用带有 L1 正则化的 logistic 回归，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibvSSygYDVbKgxHT0MwSNibZ9NsSycmzW8hibqdxWAzaWUsPXUNRx6lTCgZbKIOIJxjtzYH1rRmqhpg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？

A、第一个 w2 成了 0，接着 w1 也成了 0

B、第一个 w1 成了 0，接着 w2 也成了 0

C、w1 和 w2 同时成了 0

D、即使在 C 成为大值之后，w1 和 w2 都不能成 0



正确答案是： B

解析：

答案（B）：通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。

## 685题

假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。注意：所有其他超参数是相同的，所有其他因子不受影响。

1 深度为 4 时将有高偏差和低方差 


2 深度为 4 时将有低偏差和低方差


A、只有 1

B、只有 2

C、1 和 2

D、没有一个



正确答案是：A

解析：

答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。

## 686题

在 k-均值算法中，以下哪个选项可用于获得全局最小？

1 尝试为不同的质心（centroid）初始化运行算法

2 调整迭代的次数 

3 找到集群的最佳数量

A、2 和 3

B、1 和 3

C、1 和 2

D、以上所有



正确答案是：D

解析：

答案（D）：所有都可以用来调试以找到全局最小。

## 687题

假设你正在做一个项目，它是一个二元分类问题。你在数据集上训练一个模型，并在验证数据集上得到混淆矩阵。基于上述混淆矩阵，下面哪个选项会给你正确的预测。

1 精确度是~0.91 

2 错误分类率是~0.91 

3 假正率（False correct classification）是~0.95 

4 真正率（True positive rate）是~0.95

A、1 和 3

B、2 和 4

C、1 和 4

D、2 和 3



正确答案是：C

解析：

答案（C）：精确度（正确分类）是 (50+100)/165，约等于 0.91。真正率是你正确预测正分类的次数，因此真正率将是 100/105 = 0.95，也被称作敏感度或召回。

## 688题

对于下面的超参数来说，更高的值对于决策树算法更好吗？

1 用于拆分的样本量 

2 树深 

3 树叶样本

A、1 和 2

B、2 和 3

C、1 和 3

D、1、2 和 3

E、无法分辨



正确答案是：E

解析：

答案（E）：对于选项 A、B、C 来说，如果你增加参数的值，性能并不一定会提升。例如，如果我们有一个非常高的树深值，结果树可能会过拟合数据，并且也不会泛化。另一方面，如果我们有一个非常低的值，结果树也许与数据欠拟合。因此我们不能确定更高的值对于决策树算法就更好。

## 689题

想象一下，你有一个 28x28 的图片，并使用输入深度为 3 和输出深度为 8 在上面运行一个 3x3 的卷积神经网络。注意，步幅padding是1，你正在使用相同的填充（padding）。当使用给定的参数时，输出特征图的尺寸是多少？

A、28 宽、28 高、8 深

B、13 宽、13 高、8 深

C、28 宽、13 高、8 深

D、13 宽、28 高、8 深



正确答案是：A

解析：

答案（A）

计算输出尺寸的公式是：输出尺寸=(N – F)/S + 1。其中，N 是输入尺寸，F 是过滤器尺寸，S 是步幅。更多可阅读这篇文章（链接：https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/）获得更多了解。

## 690题

假设，我们正在 SVM 算法中为 C（惩罚参数）的不同值进行视觉化绘图。由于某些原因，我们忘记了使用视觉化标注 C 值。这个时候，下面的哪个选项在 rbf 内核的情况下最好地解释了下图（1、2、3 从左到右，图 1 的 C 值 是 C 1，图 2 的 C 值 是 C 2，图 3 的 C 值 是 C 3）中的 C 值。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBPEpwI78Kf8H6jLjSsmYaBgtibUHndiaSAicIDWPZ3CMguJsTibTe76XkVqEFl4BMsgAdk0UyVPoomw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、C1 = C2 = C3

B、C1 > C2 > C3

C、C1 < C2 < C3

D、没有一个



正确答案是：C

解析：

答案 (C)：错误项的惩罚参数 C。它也控制平滑决策边界和训练点正确分类之间的权衡。对于 C 的大值，优化会选择一个较小边距的超平面。更多内容：https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/

## 691题

假设有如下一组输入并输出一个实数的数据，则线性回归（Y = bX+c）的留一法交叉验证均方差为？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicNjic76VvWgzNroJg7pPA6OMhvKLTIRx3Of2d8PqG73JKHwqZibuzPVWNqoOn1Tq255pLZHsFJSutg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、10/27

B、20/27

C、50/27

D、49/27



正确答案是：D

解析：

我们需要计算每个交叉验证点的残差，拟合后得到两点连线和一点用于交叉验证。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicNjic76VvWgzNroJg7pPA6OW0LicJ5ItsxOC6APzFcd7AlEKX0pjqkM371ZiaibGL8pt6WRSbhzJic3zQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

留一法交叉验证均方差为(2^2 +(2/3)^2 +1^2) /3 = 49/27

## 692题

下列哪一项关于极大似然估计（MLE）的说法是正确的？

1)MLE并不总是存在

2)MLE一直存在

3)如果MLE存在，它可能不特异

4)如果MLE存在，它一定是特异的

A、1和4

B、2和3

C、1和3

D、2和4



正确答案是：C

解析：

MLE可能不是一个转折点，即它可能不是一个似然函数的一阶导数消失的点

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicNjic76VvWgzNroJg7pPA6OUpBPnUJdib0MRpxUQVcSbQSEFEn0n973ONd1ecTArOTJ1jaCKYmgpCw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

MLE可能并不特异

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicNjic76VvWgzNroJg7pPA6OF8On8ic5BZOWl1GpsxoJT6liaN4daZiaiamuiclYtxCMxGs0jmOW16ZSyBw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 693题

假设线性回归模型完美拟合训练数据（即训练误差为零），则下列哪项是正确的？

A、测试误差一定为零

B、测试误差一定不为零

C、以上都不对



正确答案是：C

解析：

如果测试数据无干扰，则测试误差可能为零。换言之，如果测试数据是训练数据的典型代表，测试误差即为零，但这种情况并不总是出现。

## 694题

在线性回归问题中，我们用“R方”来衡量拟合的好坏。在线性回归模型中增加特征值并再训练同一模型。下列哪一项是正确的？

A、如果R方上升，则该变量是显著的

B、如果R方下降，则该变量不显著

C、单单R方不能反映变量重要性，不能就此得出正确结论

D、都不正确



正确答案是：C

解析：

单单R方不能表示变量显著性，因为每次加入一个特征值，R方都会上升或维持不变。但在“调整R方”的情况下这也有误（如果特征值显著的话，调整R方会上升）。

## 695题

下列关于回归分析中的残差表述正确的是

A、残差的平均值总为零

B、残差的平均值总小于零

C、残差的平均值总大于零

D、残差没有此类规律



正确答案是：A

解析：

回归的残差之和一定为零，故而平均值也为零

## 696题

下列关于异方差性哪项是正确的？

A、线性回归有变化的误差项

B、线性回归有恒定的误差项

C、线性回归有零误差项

D、以上都不对



正确答案是：A

解析：

在误差项中，非恒定方差的存在导致了异方差性。一般来说，非恒定方差的出现时因为异常值或极端杠杆值的存在。可以参考：https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8MExicGkgZIsmJwseJcYDsh6aXYOM1mvczNxVkvTuBdO1Z1fHdnaUNnYUgHeZD6ZnkQJW4mSG63OA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

留一法交叉验证均方差为(2^2 +(2/3)^2 +1^2) /3 = 49/27

## 697题

下列哪一项说明了X，Y之间的较强关系

A、相关系数为0.9

B、Beta系数为0的空假设的p-value是0.0001

C、Beta系数为0的空假设的t统计量是30

D、都不对



正确答案是：A

解析：

变量间的相关系数为0说明了变量间的较强关系；另一方面，p-value和t统计量仅仅衡量了非零联系的证据有多强。在数据足够多的情况下，哪怕弱影响都可能是显著的。

## 698题

在导出线性回归的参数时，我们做出下列哪种假定？

1)因变量y和自变量x的真实关系是线性的

2)模型误差是统计独立的

3)误差通常服从一个平均值为零，标准差恒定的分布 

4)自变量x是非随机的，无错的


A、1,2和3

B、1,3和4

C、1和3

D、以上都对



正确答案是：D

解析：

当导出回归参数时，我们做出以上全部4种假设，缺少任何一种，模型都会出错。

## 699题

为了检验连续变量x，y之间的线性关系，下列哪种图最合适？

A、散点图

B、条形图

C、直方图

D、都不对



正确答案是：A

解析：

为了检验连续变量的线性关系，散点图是最好的选择，可以看出一个变量如何关于另一个变量变化。散点图反映两个定量变量之间的关系。

## 700题

下列哪种方法被用于预测因变量？

1)线性回归

2)逻辑回归

A、1和2

B、1

C、2

D、都不是



正确答案是： B

解析：

逻辑回归是用于分类问题的
