# 四、图嵌入技术

在本节中，我们基于所使用的技术对图嵌入方法进行分类。 通常，图嵌入旨在在低维空间中表示图，保留尽可能多的图属性信息。 不同图嵌入算法之间的区别在于，它们如何定义要保留的图属性。 不同的算法对节点（边、子结构、整图）的相似性，以及如何在嵌入空间中保留它们，有不同的见解。 接下来，我们将介绍每种图嵌入技术的见解，以及它们如何量化图属性并解决图嵌入问题。

## 矩阵分解

基于矩阵分解的图嵌入，以矩阵的形式表示图特性（例如，节点成对相似性）并对该矩阵进行分解来获得节点嵌入[11]。 图嵌入的开创性研究通常以这种方式解决图嵌入问题。 在大多数情况下，输入是由非关系高维数据特征构成的图，如第 3.1.4 节中所介绍的。输出是一组节点嵌入（Sec.3.2.1）。 因此，图嵌入的问题可以被视为保持结构的降维问题，其假定输入数据位于低维流形中。 有两种类型的基于矩阵分解的图嵌入。 一种是分解图的拉普拉斯特征映射 ，另一种是直接分解节点邻近矩阵 。

### 图的拉普拉斯算子

见解： 要保留的图属性可以解释为成对节点的相似性。 因此，如果两个具有较大相似性的节点相距很远，则会施加较大的惩罚。

**表4：**基于图的拉普拉斯特征映射的图嵌入。

| GE算法 | ![](img/img117.png) | 目标函数 |
| --- | --- | --- |
| MDS [74] | ![](img/img118.png)  欧氏距离 ![](img/img119.png) | 公式 2 |
| Isomap [78] | KNN， ![](img/img120.png)  是沿着 ![](img/img13.png)  到 ![](img/img34.png) 最短路径的边权重之和  | 公式 2 |
| LE [96] | KNN， ![](img/img121.png) | 公式 2 |
| LPP [97] | KNN， ![](img/img122.png) | 公式 4 |
| AgLPP [79] | 锚图， ![](img/img123.png)  ，  ![](img/img124.png)  ，  ![](img/img125.png) | ![](img/img126.png) |
| LGRM [98] | KNN， ![](img/img121.png) | ![](img/img127.png) |
| ARE [88] | KNN， ![](img/img128.png)  ，  ![](img/img129.png) | `<6244>` |
| SR [99] | KNN， ![](img/img133.png)  ![](img/img134.png) | `<6248>` |
| HSL [87] | ![](img/img139.png)  ，其中 ![](img/img140.png)  是归一化的超图的拉普拉斯算子 | ![](img/img141.png)  ，圣  ![](img/img142.png) |
| MVU [100] | KNN， ![](img/img143.png)  ，圣 ![](img/img144.png)  ，  ![](img/img145.png)  和  ![](img/img146.png)  ， | `<6255>` |
| SLE [86] | KNN， ![](img/img148.png) | `<6259>` |
| MSHLRR [76] | 一般图：KNN， ![](img/img149.png) | 公式 2 |
|  | 超图： ![](img/img150.png)  是一个夸张的重量  ![](img/img151.png) |
|  | ![](img/img153.png)  ，  ![](img/img154.png) |
| [77] | ![](img/img155.png) | ![](img/img156.png) |
| PUFS [75] | KNN， ![](img/img157.png) | 公式 4 +（must 和 cannot 链接约束） |
| RF-Semi-NMF-PCA [101] | KNN， ![](img/img149.png) | 公式 2 + ![](img/img6.png)  （PCA）+  ![](img/img6.png)  （k均值） |

基于以上见解，最优的嵌入 ![](img/img158.png) 可以由以下目标函数[99]导出。

![](img/img159.png) (1)

其中 ![](img/img120.png) 是节点 ![](img/img13.png) 和 ![](img/img34.png) 之间的“定义的”相似性；![](img/img160.png) 是图的拉普拉斯。 ![](img/img161.png) 是对角矩阵，其中  ![](img/img162.png)。 ![](img/img163.png) 的值越大，![](img/img48.png) 就更重要[97]。 约束 ![](img/img164.png) 通常加于 Eq.1，来删除嵌入中的任意缩放因子。  Eq.1 然后化简为：

![](img/img165.png) (2)

最优的 ![](img/img158.png) 是特征问题 ![](img/img166.png) 的最大特征值对应的特征向量。

上面的图嵌入是渐进式的，因为它只能嵌入训练集中存在的节点。 在实践中，它可能还需要嵌入未在训练中看到的新节点。 一种解决方案是设计线性函数 ![](img/img167.png) 这样只要提供了节点特征，就可以导出嵌入。 因此，对于归纳性的图嵌入，Eq.1 变为在以下目标函数中找到最的 ![](img/img168.png)：

![](img/img169.png) (3)

与 Eq.2 相似，通过添加约束 ![](img/img170.png)  ，公式 3 中的问题变成：

![](img/img171.png) (4)

最优的 ![](img/img168.png) 是 ![](img/img172.png) 的解的最大特征值的特征向量。

现有研究的差异主要在于它们如何计算成对节点的相似性 ![](img/img120.png)  ，以及它们是否使用线性函数 ![](img/img167.png) 或不。 已经进行了一些尝试[85,81]以使用一般框架总结现有的基于拉普拉斯特征图的图嵌入方法。 但他们的综述只涵盖了有限的工作量。 在表 4 中 ，我们总结了现有的基于拉普拉斯特征图的图嵌入研究，并比较了它们的 ![](img/img173.png) 的计算方法，以及他们采用了什么样的目标函数。

最初的研究 MDS [74]直接采用了两个特征向量 ![](img/img46.png) 和 ![](img/img112.png) 之间的欧几里德距离，作为 ![](img/img120.png)。公式 2 用于找到 ![](img/img158.png) 的最佳嵌入。  MDS不考虑节点的邻域，即，任何一对训练实例都被认为是连接的。 后续研究（例如，[78,102,96,97]）通过首先从数据特征构建 k 最近邻（KNN）图来克服该问题。 每个节点仅与其前 k 个相似的邻居连接。 之后，利用不同的方法来计算相似度矩阵 ![](img/img173.png)，以便尽可能多地保留所需的图属性。 最近设计了一些更高级的模型。 例如，AgLPP [79]引入了锚图，显着提高早期矩阵分解模型 LPP 的效率。  LGRM [98]学习局部回归模型来掌握图结构，和样本外数据外插值的全局回归项。 最后，与以前的工作保留局部几何不同，LSE [103]使用局部样条回归来保持全局几何。

当辅助信息（例如，标签，属性）可用时，调整目标函数以保留更丰富的信息。 例如，[99]构造邻接图 ![](img/img173.png) 和标记图 ![](img/img174.png)。 目标函数由两部分组成，一部分侧重于保留数据集的局部几何结构，如LPP [97]，另一部分试图在标记的训练数据上获得具有最佳类的可分性的嵌入。 类似地，[88]也构造了两个图，即邻接图 ![](img/img173.png) 编码局部几何结构，反馈关系图 ![](img/img175.png) 编码用户相关反馈中的成对关系。  RF-Semi-NMF-PCA [101]通过构建由三个部分组成的目标函数：PCA，k-means和图的拉普拉斯正则化，同时考虑聚类，降维和图嵌入。

其他一些工作认为 ![](img/img173.png) 不能通过容易枚举成对节点关系来构造。 相反，他们采用半定规划（SDP）来学习 ![](img/img173.png)  。 具体而言，SDP [104]的目的是找到一个内积矩阵，它最大化在图中没有连接的任何两个输入之间的成对距离，同时保留最近的邻居距离。  MVU [100]构造这样的矩阵，然后在习得的内积矩阵上应用MDS [74]。  [2]证明正则化LPP [97]相当于正则化SR [99]，如果 ![](img/img173.png) 是对称的，双随机的，PSD并且秩为 ![](img/img105.png)  。 它构造了这种相似矩阵，从而有效地解决了类似LPP的问题。


**表5：**基于节点邻近矩阵分解的图嵌入。`O(*)`表示目标函数；例如，`O(SVM分类器)`表示SVM分类器的目标函数。


| GE算法 | ![](img/img117.png) | 目标函数 |
| --- | --- | --- |
| [50] | ![](img/img176.png) | 公式 5 |
| SPE [105] | KNN， ![](img/img177.png)  ，约束为  ![](img/img178.png) | 公式 5 |
| HOPE [106] | Katz 指数 ![](img/img180.png)  ;  个性化的 Pagerank  ![](img/img181.png) | 公式 5 |
| GraRep [21] | ![](img/img184.png)  ，其中  ![](img/img185.png)  ，  ![](img/img186.png) | 公式 5 |
| CMF [43] | PPMI | 公式 5 |
| TADW [56] | PMI | 公式 5 和文本特征矩阵 |
| [24] | `A` | ![](img/img187.png) |
| MMDW [48] | PMI | 公式 5 + `O(SVM分类器)` |
| HSCA [57] | PMI | `O(MMDW)`+（ 一阶邻近度约束） |
| MVE [107] | KNN， ![](img/img189.png) | 公式 5 |
| M-NMF [1] | ![](img/img191.png) | 公式 5 + `O(社区检测)` |
| ULGE [2] | ![](img/img192.png)  ，其中  ![](img/img193.png) | ![](img/img194.png) |
| LLE [102] | KNN， ![](img/img196.png) | ![](img/img197.png) |
| RESCAL [108] | ![](img/img198.png) | ![](img/img199.png) |
| FONPE [109] | KNN， ![](img/img196.png) | ![](img/img200.png)  ，约束为 ![](img/img201.png) |

### 节点邻近矩阵分解

除了解决上述广义特征值问题外，另一系列研究试图直接分解节点邻近矩阵。

见解： 使用矩阵分解可以在低维空间中近似节点邻近度。 保持节点邻近度的目标是最小化近似的损失。

给定节点邻近矩阵 ![](img/img173.png)  ，目标是：

![](img/img202.png) (5)

其中 ![](img/img203.png) 是节点嵌入，和  ![](img/img204.png) 是上下文节点的嵌入[21]。

公式 5 旨在找到一个最优的秩为`d`的邻近度矩阵`W`的近似（ ![](img/img51.png) 是嵌入的维度）。 一种流行的解决方案是对 ![](img/img173.png) 应用 SVD（奇异值分解）[110]。从形式上看，

![](img/img205.png) (6)

其中 ![](img/img206.png) 是按降序排序的奇异值， ![](img/img207.png) 和 ![](img/img208.png) 是 ![](img/img209.png)  的奇异向量 。 最佳嵌入使用最大的`d`个奇异值获得 ![](img/img51.png)，相应的奇异向量如下：

![](img/img210.png) (7)

根据是否保留非对称属性，节点 ![](img/img37.png) 的嵌入是 ![](img/img211.png) [21,50]，或 ![](img/img212.png) 和 ![](img/img213.png) 连接，即 ![](img/img214.png)  [106]。 公式 5 存在其他解决方案，如正则化高斯矩阵分解[24]，低秩矩阵分解[56]，并加入其他正则化器来施加更多约束[48]。 我们总结了表 5 中所有基于节点邻近度矩阵分解的图嵌入。

总结：矩阵分解（MF）主要用于嵌入由非关系数据构建的图（第 3.1.4 节），用于节点嵌入（第 3.2.1 节），这是图的拉普拉斯特征映射问题的典型设定。 MF也用于嵌入同构图[50,24]（第 3.1.1 节）。

## 深度学习

深度学习（DL）在各种研究领域表现出色，如计算机视觉，语言建模等。基于DL的图嵌入在图上应用DL模型。 这些模型要么直接来自其他领域，要么是专门为嵌入图数据设计的新神经网络模型。 输入是从图中采样的路径或整个图本身。 因此，我们基于是否采用随机游走来从图中采样路径，将基于DL的图嵌入分为两类。

### 带有随机游走的基于 DL 的图嵌入

见解： 通过最大化以自身嵌入为条件的，节点邻域的观测概率，可以在嵌入空间中保留图中的二阶邻近度。

在第一类基于深度学习的图嵌入中，图被表示为从其采样的一组随机游走路径。 然后将深度学习方法应用于用于图嵌入的采样路径，保留路径所承载的图属性。

鉴于上述见解，DeepWalk [17]采用神经语言模型（SkipGram）进行图嵌入。 SkipGram [111]旨在最大化窗口内出现的单词之间的共现概率 ![](img/img215.png)  。  DeepWalk首先使用截断的随机游走，从输入图中采样一组路径（即，均匀地采样最后访问节点的邻居，直到达到最大长度）。 从图中采样的每个路径相当于来自语料库的句子，其中节点相当于单词。 然后将SkipGram应用于路径，最大化节点邻域的观测概率，以自身嵌入为条件。 以这种方式，邻域相似（二阶邻近度较大）的节点的嵌入相似。DeepWalk的目标函数如下：

![](img/img216.png) (8)

其中 ![](img/img215.png) 是窗口大小，它限制随机游走上下文的大小。  SkipGram删除了排序约束，并且 公式 8转换为：

![](img/img217.png) (9)

其中 ![](img/img218.png) 使用softmax函数定义：

![](img/img219.png) (10)

请注意，计算公式 10 是昂贵的，因为标准化因子（即，图中每个节点的所有内积的总和），所以图 10 的方法是不可行的。 通常有两种解近似完全softmax的解决方案：分层softmax [112]和负采样[112]。

分层softmax ：有为了效地解决中公式 10，构造二叉树，其中节点被分配给叶子。 不像公式 10 那样枚举所有节点，仅需要求解从根到相应叶子的路径。 优化问题变得最大化树中特定路径的概率。 假设到叶子 ![](img/img13.png) 的路径是一系列节点  ![](img/img220.png)  ，其中`b0`为根，  ![](img/img222.png)  。 公式 10 然后变成：

![](img/img223.png)  (11)

其中 ![](img/img224.png) 是二分类器：![](img/img225.png)。![](img/img226.png) 表示 S 形函数。 ![](img/img227.png) 是树节点 ![](img/img228.png)  的父节点的嵌入 。 分层softmax减少了SkipGram的时间复杂度，从 ![](img/img229.png) 至  ![](img/img230.png)。

负采样 ： 负采样的关键思想是，使用逻辑回归将目标节点与噪声区分开来。 即，对于一个节点 ![](img/img13.png)  ，我们想区分它的邻居 ![](img/img231.png) 来自其他节点。 噪音分布  ![](img/img232.png) 用于绘制节点的负样本 ![](img/img13.png)  。公式 9 中的每个 ![](img/img233.png) 然后计算为：

![](img/img234.png) (12)

其中 ![](img/img235.png) 是采样的负节点数。 ![](img/img236.png) 是一种噪声分布，例如均匀分布（![](img/img237.png)）。 具有负采样的SkipGram的时间复杂度是 ![](img/img238.png)。

**表6：**带有随机游走路径的基于深度学习的图嵌入。

| GE算法 | 随机游走方法 | 保留的邻近度 | DL模型 |
| --- | --- | --- | --- |
| DeepWalk [17] | 截断随机游走 | ![](img/img239.png) | SkipGram 和 分层 softmax（公式 11） |
| [34] | 截断随机游走 | ![](img/img239.png)  （词语-图像） | 同上 |
| GenVector [66] | 截断随机游走 | ![](img/img239.png)  （用户 - 用户和概念 - 概念） | 同上 |
| 受限制的DeepWalk [25] | 边权重采样 | ![](img/img239.png) | 同上 |
| DDRW [47] | 截断随机游走 | ![](img/img239.png)  +分类一致性 | 同上 |
| TriDNR [73] | 截断随机游走 | ![](img/img239.png)  （节点，单词和标签之间） | 同上 |
| node2vec [28] | BFS + DFS | ![](img/img239.png) | SkipGram 和负采样（公式 12） |
| UPP-SNE [113] | 截断随机游走 | ![](img/img239.png)  （用户 - 用户和个人资料 - 个人资料） | 同上 |
| Planetoid [62] | 按标签和结构对节点对进行采样 | ![](img/img239.png)  +标签标识 | 同上 |
| NBNE [19] | 对节点的直接邻居进行采样 | ![](img/img239.png) | 同上 |
| DGK [93] | graphlet 核：随机采样[114] | ![](img/img239.png)  （通过graphlet） | SkipGram（公式11 - 12 ） |
| metapath2vec [46] | 基于元路径的随机游走 | ![](img/img239.png) | 异构 SkipGram |
| ProxEmbed [44] | 截断随机游走 | 节点排名元组 | LSTM |
| HSNL [29] | 截断随机游走 | ![](img/img239.png)  + QA排名元组 | LSTM |
| RMNL [30] | 截断随机游走 | ![](img/img239.png)  +用户问题质量排名 | LSTM |
| DeepCas [63] | 基于马尔可夫链的随机游走 | 信息级联序列 | GRU |
| MRW-MN [36] | 截断随机游走 | ![](img/img239.png)  +跨模态特征差异 | DCNN + SkipGram |

DeepWalk [17]的成功激发了许多后续研究，这些研究将深度学习模型（例如，SkipGram或长短期记忆（LSTM）[115]）应用于图嵌入的采样路径。 我们在表 6中对它们进行了总结。 如表中所示，大多数研究遵循DeepWalk的想法，但改变随机游戏的采样方法（[25,28,62,62]）或要保留的邻近度（定义 5和定义 6）的设定（[34,66,47,73,62]）。 [46]设计基于元路径的随机游走来处理异构图和异构 SkipGram，它最大化了给定节点具有异构上下文的概率。 除了SkipGram之外，LSTM是图嵌入中采用的另一种流行的深度学习模型。 请注意，SkipGram只能嵌入一个节点。 然而，有时我们可能需要将一系列节点嵌入为固定长度向量，例如，将句子（即，一系列单词）表示为一个向量，就要在这种情况下采用LSTM来嵌入节点序列。 例如，[29]和[30]嵌入cQA站点中的问题/答案中的句子，[44]在两个节点之间嵌入一系列节点，用于邻近度嵌入。 在这些工作中优化排名损失函数，来保持训练数据中的排名分数。 在[63]中，GRU [116]（即，类似于LSTM的递归神经网络模型）用于嵌入信息级联路径。

#### 不带随机游走的基于 DL 的图嵌入

见解： 多层学习架构是一种强大而有效的解决方案，可将图编码为低维空间。

第二类基于深度学习的图嵌入方法直接在整个图（或整个图的邻近矩阵）上应用深度模型。 以下是图嵌入中使用的一些流行的深度学习模型。

自编码器 ：自编码器旨在最小化其编码器输入和解码器输出的重建误差。 编码器和解码器都包含多个非线性函数。 编码器将输入数据映射到表示空间，并且解码器将表示空间映射到重建空间。 采用自编码器进行图嵌入的思想，与邻域保持方面的节点邻近矩阵分解（Sec.4.1.2）相似。 具体而言，邻接矩阵捕获节点的邻域。 如果我们将邻接矩阵输入到自编码器，则重建过程将使具有相似邻域的节点具有类似的嵌入。

深度神经网络 ：作为一种流行的深度学习模型，卷积神经网络（CNN）及其变体已广泛应用于图嵌入。 一方面，他们中的一些人直接使用为欧几里德域设计的原始CNN模型，并重新格式化输入图以适应它。 例如，[55]使用图标记，从图中选择固定长度的节点序列，然后使用 CNN 模型，组装节点的邻域来学习邻域表示。 另一方面，一些其他工作试图将深度神经模型推广到非欧几里德域（例如，图）。 [117]在他们的综述中总结了代表性研究。 通常，这些方法之间的差异在于，它们在图上形成类似卷积的操作的方公式 一种方法是模拟卷积定理以定义谱域中的卷积 [118,119]。 另一种方法是将卷积视为空域中的邻域匹配 [82,72,120]。

其他 ：还有一些其他类型的基于深度学习的图嵌入方法。 例如，[35]提出了DUIF，它使用分层softmax作为前向传播来最大化模块性。 HNE [33]利用深度学习技术来捕获异构成分之间的交互，例如，用于图像的CNN和用于文本的FC层。 ProjE [40]设计了一个具有组合层和投影层的神经网络。 它定义了知识图嵌入的逐点损失（类似于多分类）和列表损失（即softmax回归损失）。

我们在表 7 中总结了所有基于深度学习的图嵌入方法（没有随机游走），并比较了它们使用的模型以及每个模型的输入。

**表7：**基于深度学习的图嵌入， 没有随机游走路径。

| GE 算法 | 深度学习模型 | 模型输入 |
| --- | --- | --- |
| SDNE [20] | 自编码器 | ![](img/img35.png) |
| DNGR [23] | 堆叠去噪自编码器 | PPMI |
| SAE [22] | 稀疏自编码器 | ![](img/img240.png) |
| [55] | CNN | 节点序列 |
| SCNN [118] | 谱 CNN | 图 |
| [119] | 带有光滑谱乘法器的谱 CNN | 图 |
| MoNet [80] | 混合模型网络 | 图 |
| ChebNet [82] | 图CNN又名ChebNet | 图 |
| GCN [72] | 图卷积网络 | 图 |
| GNN [120] | 图神经网络 | 图 |
| [121] | 自适应图神经网络 | 分子图 |
| GGS-NNs [122] | 自适应图神经网络 | 图 |
| HNE [33] | CNN + FC | 带图像和文本的图 |
| DUIF [35] | 分层深度模型 | 社会管理网络 |
| ProjE [40] | 神经网络模型 | 知识图 |
| TIGraNet [123] | 图卷积网络 | 从图像构造的图 |

总结：由于它的威力和效率，深度学习已广泛应用于图嵌入。 在基于深度学习的图嵌入方法中，已经观察到三种类型的输入图（除了从非关系数据构建的图（第 3.1.4 节））和所有四种类型的嵌入输出。

## 基于边重构的优化问题

总体见解： 基于节点嵌入建立的边应尽可能与输入图中的边相似。

第三类图嵌入技术通过最大化边重建概率，或最小化边重建损失，来直接优化基于边重建的目标函数。 后者进一步分为基于距离的损失和基于边距的排名损失。 接下来，我们逐一介绍这三种类型。

### 最大化边重建概率

见解： 良好的节点嵌入最大化了在图中观察到的边的生成概率。

良好的节点嵌入应该能够重新建立原始输入图中的边。 这可以通过使用节点嵌入最大化所有观察到的边（即，节点成对接近）的生成概率来实现。

节点对 ![](img/img13.png) 和 ![](img/img34.png) 之间的直接边，表示它们的一阶邻近度 ，可以使用嵌入来计算 ![](img/img13.png) 和 ![](img/img34.png) 的联合概率：

![](img/img241.png) (13)

上述一阶邻近度存在于图中的任何一对连接节点之间。 为了学习嵌入，我们最大化了在图中观察这些邻域的对数似然。 然后将目标函数定义为：

![](img/img242.png) (14)

同样，![](img/img13.png) 和 ![](img/img34.png) 的二阶邻近度是条件概率 ![](img/img34.png) 由 ![](img/img13.png) 使用 ![](img/img48.png) 和 ![](img/img243.png) 生成：

![](img/img244.png) (15)

它可以被解释为在图中随机游走的概率，它开始于 ![](img/img13.png) 结束于 ![](img/img34.png)。 因此图嵌入目标函数是：

![](img/img245.png) (16)

其中 ![](img/img246.png) 是从图中采样的路径中，![](img/img247.png) 的集合。即来自每个采样路径的两个端节点。 这模拟了二阶邻近度，作为从 ![](img/img248.png) 到 ![](img/img249.png) 的随机游走的概率。

### 最小化基于距离的损失

见解： 基于节点嵌入计算的节点邻近度，应尽可能接近基于观察到的边计算的节点邻近度。

具体来说，可以基于节点嵌入来计算节点邻近度，或者可以基于观察到的边凭经验计算节点邻近度。 最小化两种类型的邻近度之间的差异，保持了相应的邻近度。

对于一阶邻近度，可以使用公式 13 中定义的节点嵌入来计算它。 经验概率是 ![](img/img250.png)  ，其中 ![](img/img251.png) 是边 ![](img/img31.png) 的权重。![](img/img252.png) 和  ![](img/img253.png) 两者之间的距离越小，就能保持更好的一阶邻近度。 使用KL-散度作为距离函数来计算 ![](img/img252.png) 和  ![](img/img253.png) 间的差异，并且省略了一些常量，在图嵌入中保留一阶邻近度的目标函数是：

![](img/img254.png) (17)

同样，![](img/img13.png) 和 ![](img/img34.png) 的二阶邻近度是由节点 ![](img/img13.png) 生成的条件概率 ![](img/img34.png)（公式 15）。 ![](img/img255.png) 的经验概率计算为  ![](img/img256.png)，其中  ![](img/img257.png) 是节点的出度（无向图的情况中是度） ![](img/img13.png)。与公式 10 相似，计算公式 15 非常昂贵。 再次将负采样用于近似计算来提高效率。 通过最小化 ![](img/img258.png) 和  ![](img/img259.png) 之间的 KL 差异，保持二阶邻近度的目标函数是：

![](img/img260.png) (18)

**表8：**基于边重建的图嵌入。 ![](img/img9.png) 是指公式 14，16~19 之一。例如 ，![](img/img11.png)  （word-label）是指 公式 18，带有单词节点和标签节点。  ![](img/img12.png) 表示节点 ![](img/img13.png) 的类型。

| GE算法 | 目标 | 邻近度阶数 |
| --- | --- | --- |
| PALE [18] | ![](img/img261.png)  （节点，节点） | 1 |
| NRCL [4] | ![](img/img262.png)  （节点，邻居节点）+  ![](img/img6.png)  （属性损失） |
| PTE [124] | ![](img/img11.png)  （单词，单词）+  ![](img/img11.png)  （单词，文档）+  ![](img/img11.png)  （单词，标签） |  |
| APP [3] | ![](img/img263.png)  （节点，节点）） |  |
| GraphEmbed [83] | ![](img/img11.png)  （单词，单词）+  ![](img/img11.png)  （单词，时间）+  ![](img/img11.png)  （单词，位置）+  ![](img/img11.png)  （时间，地点）+  ![](img/img11.png)  （位置，位置）+  ![](img/img11.png)  （时间，时间） | 2 |
| [41,42] | ![](img/img11.png)  （车站，公司），  ![](img/img11.png)  （车站，角色），  ![](img/img11.png)  （目的地，出发地） |
| PLE [84] | ![](img/img262.png)  （提示，类型）+  ![](img/img11.png)  （提示，特性）+  ![](img/img11.png)  （类型，类型） |
| IONE [26] | ![](img/img11.png)  （节点，节点）+  ![](img/img6.png)  （锚对齐） |
| HEBE [45] | ![](img/img11.png)  （节点，超边中的其他节点） |
| GAKE [38] | ![](img/img263.png)  （节点，邻居上下文）+  ![](img/img263.png)  （节点，路径上下文）+  ![](img/img263.png)  （节点，边上下文） |
| CSIF [64] | ![](img/img263.png)  （用户对，扩散内容） |
| ESR [69] | ![](img/img263.png)  （实体，作者）+  ![](img/img263.png)  （实体，实体）+  ![](img/img263.png)  （实体，单词）+  ![](img/img263.png)  （实体，场地） |
| LINE [27] | ![](img/img264.png)  （节点，节点）+  ![](img/img11.png)  （节点，节点）） |
| EBPR [71] | ![](img/img6.png)  （AUC 排名）+  ![](img/img263.png)  （节点，节点）+  ![](img/img263.png)  （节点，节点上下文） | 1 和 2 |
| [94] | ![](img/img262.png)  （问题，答案） | 1，2 和 更高 |

### 最小化基于边距的排名损失

在基于边距的排名损失优化中，输入图的边指代节点对之间的相关性。 图中的一些节点通常与一组相关节点相关联。 例如，在cQA网站中，一组答案被标记为与给定问题相关。 对损失的见解是直截了当的。

见解： 节点的嵌入更类似于相关节点的嵌入，而不是任何其他不相关节点的嵌入。

![](img/img266.png) 表示节点 ![](img/img13.png) 和 ![](img/img34.png) 的相似性得分， ![](img/img267.png) 表示与 ![](img/img13.png) 相关的节点集， ![](img/img268.png) 表示不相关的节点集。 基于边距的排名损失定义为：

![](img/img269.png) (19)

其中 ![](img/img132.png) 是边距。 减少损失排名，可以促进 ![](img/img270.png) 和  ![](img/img271.png) 之间的巨大边距，从而保证 ![](img/img13.png) 的嵌入更接近其相关节点而不是任何其他不相关节点。

在表 8 中 ，我们基于其目标函数和保留的节点邻近度，总结了基于边重建的现有图嵌入方法。 通常，大多数方法使用上述目标函数之一（公式 14，16~19）。  [71]优化 AUC 排名损失，这是基于边距的排名损失的替代损失（公式 19 ）。 请注意，当在图嵌入期间同时优化另一个任务时，该任务特定的目标将被纳入总体目标中。 例如，[26]旨在对齐两个图。 因此，网络对齐的目标函数与 ![](img/img11.png)（公式 18）一起优化。

值得注意的是，大多数现有知识图嵌入方法选择优化基于边距的排名损失。 回想一下知识图 ![](img/img24.png) 由三元组  ![](img/img52.png) 组成，表示头部实体 ![](img/img53.png) 通过关系 ![](img/img55.png) 链接到尾部实体 ![](img/img54.png)。 嵌入 ![](img/img24.png) 可以解释为，保留真正三元组的排名 ![](img/img52.png) ，优于 ![](img/img24.png) 中不存在的假的三元组 ![](img/img272.png)。 特别是在知识图嵌入中，类似于公式 19 的 ![](img/img270.png)，能量函数 ![](img/img273.png) 为三元组 ![](img/img52.png) 而设计。 这两个函数之间略有不同。![](img/img270.png) 表示节点嵌入 ![](img/img13.png) 和 ![](img/img274.png) 之间的相似性得分，而 ![](img/img273.png) 是嵌入 ![](img/img53.png) 和 ![](img/img54.png) 在关系 ![](img/img55.png) 方面的距离得分。![](img/img273.png) 的一个例子是 ![](img/img275.png)，其中关系表示为嵌入空间中的变换 [91]。![](img/img273.png) 的其他选项总结在表 9 中。 因此，对于知识图嵌入，公式 19 变为：

![](img/img276.png) (20)

其中 ![](img/img277.png) 是输入知识图中的三元组。 现有的知识图嵌入方法主要是在他们的工作中优化公式 20。它们之间的区别在于 ![](img/img273.png) 的定义，如表 9 所示。 知识图嵌入相关工作的更多细节，已在 [13] 中进行了详细的回顾。

**表9：**使用基于边距的排名损失的知识图嵌入。

| GE算法 | 能量函数 ![](img/img278.png) |
| --- | --- |
| TransE [91] | ![](img/img275.png) |
| TKRL [53] | ![](img/img279.png) |
| TransR [15] | ![](img/img280.png) |
| CTransR [15] | ![](img/img281.png) |
| TransH [14] | ![](img/img282.png) |
| SePLi [39] | ![](img/img283.png) |
| TransD [125] | ![](img/img284.png) |
| TranSparse [126] | ![](img/img285.png) |
| m-TransH [127] | ![](img/img286.png) |
| DKRL [128] | ![](img/img287.png) |
| ManifoldE [129] | 球面： ![](img/img288.png) |
|  | 超平面： ![](img/img289.png) |
|  | ![](img/img290.png)  是希尔伯特空间的映射函数 |
| TransA [130] | ![](img/img291.png) |
| puTransE [43] | ![](img/img291.png) |
| KGE-LDA [60] | ![](img/img275.png) |
| SE [90] | ![](img/img292.png) |
| SME [92]线性 | ![](img/img293.png) |
| SME [92]双线性 | ![](img/img293.png) |
| SSP [59] | ![](img/img294.png)，![](img/img295.png) |
| NTN [131] | ![](img/img296.png) |
| HOLE [132] | ![](img/img297.png)  ，其中 ![](img/img298.png)  是环形相关度 |
| MTransE [133] | ![](img/img275.png) |

请注意，一些研究联合优化排名损失（公式式20 ）和其他目标来保留更多信息。 例如，SSP [59]使用公式 20 联合优化了主题模型的丢失，将文本节点描述用于嵌入。 [133]对单语关系进行分类，并使用线性变换来学习实体和关系的跨语言对齐。 还存在一些工作，为三元组  ![](img/img52.png)   定义匹配度分数而不是能量函数。 例如，[134]定义了双线性分数函数 ![](img/img299.png) 它增加了常态约束和交换约束，在嵌入之间加入类比结构。  ComplEx [135]将嵌入扩展到复数域并将 ![](img/img299.png) 的实部定义为得分。

总结：基于边重建的优化适用于大多数图嵌入设定。 据我们所知，只有非关系数据（第 3.1.4 节）和整图嵌入（第 3.2.4 节）尚未尝试过。 原因是重建手动构造的边不像其他图那样直观。 此外，由于该技术侧重于直接观察到的局部边，因此不适合于整图嵌入。

## 图核

见解： 整个图结构可以表示为一个向量，包含从中分解的基本子结构的数量。

图核是 R-convolution 核的一个实例[136]，它是定义离散复合对象上的核的通用方法，通过递归地将结构化对象分解为“原子”子结构，并比较它们的所有对[93]。 图核将每个图示为向量，并且使用两个向量的内积来比较两个图。 图核中通常定义了三种类型的“原子”子结构。

Graphlet。graphlet 是一个大小为 K 的感应的和非同构子图 [93]。 假设图  ![](img/img24.png) 被分解为一组 graphlet ![](img/img300.png)。然后  ![](img/img24.png) 嵌入为标准化计数的`d`维向量（表示为  ![](img/img301.png)）。 该 ![](img/img37.png) 的维度  ![](img/img301.png) 是 ![](img/img24.png) 中 Graphlet ![](img/img302.png) 的出现频率。

子树模式。 在此核中，图被分解为其子树模式。 一个例子是 Weisfeiler-Lehman 子树[49]。 特别是，在标记图（即，具有离散节点标签的图）上进行重新标记的迭代过程。 在每次迭代中，基于节点及其邻居的标签生成多集标签。 新生成的多集标签是一个压缩标签，表示子树模式，然后用于下一次迭代。 基于图同构的 Weisfeiler-Lehman 检验，计算图中标签的出现等同于计算相应的子树结构。 假设 ![](img/img53.png) 在图上执行重新标记的迭代  ![](img/img24.png)  。 它的嵌入  ![](img/img301.png) 包含 ![](img/img53.png) 块。 该 ![](img/img37.png) 中的维度 ![](img/img39.png) 第一块  ![](img/img301.png) 是频率 ![](img/img37.png)  -th标签被分配给一个节点 ![](img/img39.png) 第二次迭代。

随机游走 。 在第三种类型的图核中，图被分解为随机游走或路径，并表示为随机游走的出现次数[137]或其中的路径[138]。 以路径为例，假设图 ![](img/img24.png) 被分解成 ![](img/img51.png) 个最短路径。将第`i`个路径表示为三元组 ![](img/img303.png)，其中 ![](img/img304.png) 和 ![](img/img305.png) 是起始节点和结束节点的标签， ![](img/img306.png) 是路径的长度。 然后  ![](img/img24.png) 表示为`d`维向量 ![](img/img301.png)，其中第`i`个维度是 ![](img/img24.png) 中第`i`个三元组的频率。

简介：图核专为整图嵌入（Sec.3.2.4）而设计，因为它捕获整个图的全局属性。 输入图的类型通常是同构图（第 3.1.1 节）[93]或带有辅助信息的图（第 3.1.3 节）[49]。

## 生成模型

生成模型可以通过规定输入特征和类标签的联合分布来定义，以一组参数为条件[139]。 一个例子是 Latent Dirichlet Allocation（LDA），其中文档被解释为主题上的分布，主题是单词上的分布[140]。 采用生成模型进行图嵌入有以下两种方法。

### 潜在语义空间中的图嵌入

见解： 节点嵌入到潜在的语义空间中，节点之间的距离解释了观察到的图结构。

第一种基于生成模型的图嵌入方法，直接在潜在空间中嵌入图。 每个节点表示为潜在变量的向量。 换句话说，它将观察到的图视为由模型生成的。 例如，在LDA中，文档嵌入在“主题”空间中，其中具有相似单词的文档具有类似的主题向量表示。 [70]设计了类似LDA的模型来嵌入基于位置的社交网络（LBSN）图。 具体来说，输入是位置（文档），每个位置包含访问该位置的一组用户（单词）。 由于某些活动（主题），用户访问相同的位置（单词出现在同一文档中）。 然后，模型被设计为将位置表示为活动的分布，其中每个活动具有对用户的吸引力分布。 因此，用户和位置都表示为“活动”空间中的向量。

### 包含潜在语义的图嵌入

见解： 图中接近且具有相似语义的节点的嵌入应该更紧密。 可以通过生成模型，从节点描述中检测节点语义。

在这一系列方法中，潜在语义用于利用辅助节点信息进行图嵌入。 嵌入不仅由图结构信息决定，而且由从其他节点信息源发现的潜在语义决定。 例如，[58]提出了一个统一的框架，它共同集成了主题建模和图嵌入。 其原理是如果嵌入空间中两个节点接近，它们也具有相似的主题分布。 设计从嵌入空间到主题语义空间的映射函数，以便关联两个空间。 [141]提出了一种生成模型（贝叶斯非参数无限混合嵌入模型），以解决知识图嵌入中的多关系语义问题。 它发现了关系的潜在语义，并利用混合关系组件进行嵌入。 [59]从知识图三元组和实体和关系的文本描述中嵌入知识图。 它使用主题建模来学习文本的语义表示，并将三元组嵌入限制在语义子空间中。

上述两种方法的区别在于嵌入空间是第一种方式的潜在空间。相反，在第二种方式中，潜在空间用于整合来自不同来源的信息，并有助于将图嵌入到另一个空间。

简介：生成模型可用于节点嵌入（Sec.3.2.1）[70]和边嵌入（Sec.3.2.2）[141]。 在考虑节点语义时，输入图通常是异构图（第 3.1.2 节）[70]或带有辅助信息的图（第 3.1.3 节）[59]。

## 混合技术和其它

有时在一项研究中结合了多种技术。 例如，[4]通过最小化基于边的排序损失来学习基于边的嵌入（第 4.3 节），并通过矩阵分解来学习基于属性的嵌入（第 4.1 节）。 [51]优化基于边距的排名损失（第 4.3 节），基于矩阵分解的损失（第 4.1 节）作为正则化项。 [32]使用LSTM（第 4.2节）来学习cQAs的句子的嵌入，以及基于边际的排名损失（第[4.3](#sec:ml)节）来结合好友关系。 [142]采用CBOW / SkipGram（第 4.2 节）进行知识图实体嵌入，然后通过最小化基于边际的排名损失来微调嵌入（第 4.3 节）。 [61]使用word2vec（第 4.2 节）嵌入文本上下文和TransH（第 4.3 节）嵌入实体/关系，以便在知识图嵌入中利用丰富的上下文信息。 [143]利用知识库中的异构信息来提高推荐效果。 它使用TransR（第 4.3 节）进行网络嵌入，并使用自编码器进行文本和视觉嵌入（第 4.2 节）。 最后，提出了一个生成框架（第 4.5 节），结合协同过滤与项目的语义表示。

除了引入的五类技术之外，还存在其他方法。 [95]提出了根据原型图距离的图的嵌入。 [16]首先使用成对最短路径距离嵌入一些标志性节点。 然后嵌入其他节点，使得它们到标志性子集的距离尽可能接近真实的最短路径。 [4]联合优化基于链接的损失（最大化节点的邻居而不是非邻居的观测似然）和基于属性的损失（基于基于链接的表示学习线性投影）。 KR-EAR [144]将知识图中的关系区分为基于属性和基于关系的关系。 它构造了一个关系三元编码器（TransE，TransR）来嵌入实体和关系之间的相关性，以及一个属性三元编码器来嵌入实体和属性之间的相关性。 Struct2vec [145]根据用于节点嵌入的分层指标，来考虑节点的结构性标识。 [146]通过近似高阶邻近矩阵提供快速嵌入方法。

## 总结

我们现在总结并比较表10中所有五类图嵌入技术的优缺点。

**表10：**图嵌入技术的比较。

|  类别 | 子类别 | 优点 | 缺点 |
| --- | --- | --- | --- |
|  矩阵分解 | 图拉普拉斯算子 | 考虑全局节点邻近度 | 大量的时间和空间开销 |
|  | 节点邻近矩阵分解 |  |
|  深度学习 | 带有随机游走 | 有效而强大， | a）仅考虑路径中的局部上下文 |
|  |  | b）难以发现最优采样策略 |
|  | 没有随机游走 |  | 高计算开销 |
|  边重构 | 最大化边重建概率 |  | 仅使用观察到的局部信息来优化 |
|  | 最小化基于距离的损失 | 相对有效的的训练 | 例如边（一跳的邻居） |
|  | 最小化基于边距的排名损失 |  | 或者排序节点对 |
|  图核 | 基于graphlet | 有效，只计算所需的原子子结构 | a）子结构不是独立的 |
|  | 基于子树模式 | | b）嵌入维度指数性增长 |
|  | 基于随机游走 |  |
|  生成模型 | 在潜在的空间中嵌入图 | 可解释的嵌入 | a）难以证明分布的选择 |
|  | 将潜在语义合并到图嵌入中 | 自然地利用多个信息源 | b）需要大量训练数据 |

基于矩阵分解的图嵌入，基于全局成对相似性的统计量学习表示。 因此，它可以胜过某些任务中基于深度学习的图嵌入（涉及随机游走），因为后者依赖于单独的局部上下文窗口 [147,148]。 然而，邻近度矩阵构造或矩阵的特征分解时间和空间开销大[149]，使得矩阵分解效率低且对于大图不可扩展。

深度学习（DL）已经在不同的图嵌入方法中显示出有希望的结果。 我们认为DL适合于图嵌入，因为它能够自动识别复杂图结构中的有用表示。 例如，具有随机游走的DL（例如，DeepWalk [17]，node2vec [28]，metapath2vec [46]）可以通过图上的采样路径自动利用邻域结构。 没有随机游走的DL可以模拟同构图中可变大小的子图结构（例如，GCN [72]，struc2vec [145]，GraphSAGE [150]），或者异构图中类型灵活的节点之间的丰富交互（例如，HNE [33]，TransE [91]，ProxEmbed [44]），变为有用的表示。 另一方面，DL也有其局限性。 对于具有随机游走的DL，它通常观测同一路径中的节点的局部邻居，从而忽略全局结构信息。 此外，很难找到“最优采样策略”，因为嵌入和路径采样不是在统一框架中联合优化的。 对于没有随机游走的DL，计算成本通常很高。 传统的深度学习架构假设输入数据在1D或2D网格上，来利用GPU [117]。 然而，图没有这样的网格结构，因此需要不同的解决方案来提高效率[117]。

基于边重建的图嵌入，基于观察到的边或排序三元组来优化目标函数。 与前两类图嵌入相比，它更有效。 然而，使用直接观察到的局部信息来训练这一系列方法，因此所获得的嵌入缺乏对全局图结构的认识。

基于图核的图嵌入将图转换为单个向量，以便于图级别的分析任务，例如图分类。 它比其他类别的技术更有效，因为它只需要在图中枚举所需的原子子结构。 然而，这种“基于结构袋”的方法有两个局限[93]。 首先，子结构不是独立的。 例如，大小为`k+1`的 graphlet 可以从大小为`k` graphlet 的派生，通过添加新节点和一些边。 这意味着图表示中存在冗余信息。 其次，当子结构的大小增加时，嵌入维度通常呈指数增长，导致嵌入中的稀疏问题。

基于生成模型的图嵌入可以自然地在统一模型中利用来自不同源（例如，图结构，节点属性）的信息。 直接将图嵌入到潜在语义空间中，会生成可以使用语义解释的嵌入。 但是使用某些分布对观察进行建模的假设很难证明是正确的。 此外，生成方法需要大量的训练数据来估计适合数据的适当模型。 因此，它可能不适用于小图或少量图。
