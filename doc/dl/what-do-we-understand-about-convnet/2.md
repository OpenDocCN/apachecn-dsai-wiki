## 二、多层网络

本章简要概述了计算机视觉中应用最突出的多层结构。值得注意的是，虽然本章涵盖了文献中最重要的贡献，但它不会对这些结构进行全面的介绍，因为这样的介绍可以在别处获得（例如参考文献[17,56,90]）。相反，本章的目的是为文档的其余部分奠定基础，并详细介绍和讨论目前对应用于视觉信息处理的卷积网络的理解。

### 2.1多层架构

在最近成功开发基于深度学习的网络之前，用于识别的最先进的计算机视觉系统依赖于两个独立但互补的步骤。首先，通过一组手工设计的操作（例如。具有基组，局部或全局编码方法的卷积）将输入数据转换成合适的形式。输入引起的变换通常需要找到输入数据的紧凑和/或抽象表示，同时根据手头的任务注入若干不变性。这种转换的目标是以一种更容易被分类器分开的方式改变数据。其次，变换后的数据用于训练某种分类器（例如支持向量机）来识别输入信号的内容。使用的任何分类器的性能通常都会受到使用的转换的严重影响。

具有学习的多层体系结构通过提出不仅仅使用分类器来学习，而且还直接从数据中学习所需的转换操作，从而对问题产生了不同的展望。这种学习形式通常被称为表示学习[7,90]，当在深层多层体系结构的前后层结构中使用时，称为深度学习。

多层体系结构可以定义为允许从输入数据中提取多个抽象级别有用信息的计算模型。通常，多层架构被设计为放大较高层输入的重要成分，同时对较不显着的变化变得越来越稳健。大多数多层架构交替使用线性和非线性函数堆叠简单构建模块。多年来，学者们提出了多种多样的多层架构，本节将介绍计算机视觉应用中最突出的这种架构。特别是由于显著性，人工神经网络架构将成为焦点。为了简洁起见，下面将更简单地将这种网络称为神经网络。

#### 2.1.1神经网络

典型的神经网络架构由输入层![](img/tex.gif)，输出层![](img/tex1.gif)，和多个隐藏层组成的堆栈![](img/tex2.gif)组成，其中每层由多个单元或单元组成，如图2.1所示。通常，每个隐藏单元![](img/tex3.gif)接收来自前一层所有单元的输入，其输出由输入的加权组合与非线性计算得到，计算公式见（2.1）

![](img/tex4.gif)（2.1）

其中，![](img/tex5.gif)是控制输入单元和隐藏单元之间连接强度的权重系数，![](img/tex6.gif)是隐藏单元的微小偏移量，![](img/tex7.gif)是饱和非线性函数，如Sigmoid函数。

![](img/x1.png)

图2.1：典型神经网络架构的图示。复制自文献[17]。

深度神经网络可以看作是Rosenblatt所提出的感知器[122]和多层感知器[123]的现代版实例。虽然，神经网络模型已存在多年（神经网络由1960年代首次提出），它们直到最近才被大量使用。神经网络的沉寂多年原因复杂。最初，实验的负面结果表明感知器无法对XOR这样的简单操作进行建模，这一失败在一定时间内阻碍了对感知器的进一步研究，直到感知器由单层推广到多层[106]。此外，缺乏适当的训练算法亦使得研究进展缓慢，直到反向传播算法的普及[125]。然而，阻碍多层神经网络发展的更大障碍是它们依赖于非常大量的参数，这反过来意味着多层神经网络需要大量的训练数据和计算资源来支持参数的学习。

通过使用受限玻尔兹曼机（RBM）[68]，深度神经网络领域在分层无监督预训练方面取得重大进展。受限玻尔兹曼机可以看作受限只允许前馈连接的两层神经网络。当应用于图像识别时，用于训练RBM的无监督学习方法可以归纳为三个步骤。首先，每个像素![](img/tex8.gif)作为输入，随机权重设为![](img/tex5.gif)，偏差为![](img/tex6.gif)，每个单位的隐藏状态![](img/tex3.gif)值为![](img/tex9.gif)的概率设为![](img/tex10.gif)。概率由式（2.2定义）

![](img/tex11.gif)（2.2）

其中![](img/tex12.gif)。其次，一旦基于等式2.2随机地设置了所有隐藏状态，通过以概率![](img/tex13.gif)将每个像素![](img/tex8.gif)设置为![](img/tex9.gif)以尝试重建图像。第三，通过基于由式（2.3）给出的重建误差更新权重和偏差来校正隐藏单元

![](img/tex14.gif)（2.3）

其中![](img/tex15.gif)是学习率，![](img/tex16.gif)是像素![](img/tex8.gif)和隐藏单元![](img/tex3.gif)在一起的次数。整个迭代过程最大重复为![](img/tex17.gif)次，或当直到误差下降到预设阈值![](img/tex18.gif)时迭代结束。当完成一层的训练后，该层的输出将作为层次结构中下一层的输入。下一层亦将循环该过程。通常，在完成所有网络层的预训练之后，使用梯度下降，通过误差反向传播进一步对标记数据进行微调[68]。通过使用该分层无监督预训练方法，深度神经网络不需要大量标记数据亦可进行训练，因为无监督RBM预训练提供了用于经验上有用的初始化各种网络参数的方式。

依赖于堆叠RBM的神经网络首先成功地在人脸识别应用中作为一种降维方法进行部署[69]，其中它们被用作一种自动编码器。简而言之，自动编码器可以定义为由两个主要部分组成的多层神经网络：首先，其中编码器将输入数据转换为特征向量;第二，解码器将生成的特征向量映射回输入空间;见图2.2。通过最小化输入与其重建版本之间的、误差来学习自动编码器的参数。

![](img/x2.png)

图2.2：典型自编码器网络的结构。复制自文献[17]。

除了基于RBM的自动编码器之外，后来学者们又提出了几种类型的自动编码器。每个自动编码器都引入了一种不同的正则化方法，即使在执行不同的不变性时，也能阻止网络学习不重要的解决方案。主要范例包括稀疏自动编码器（SAE）[8]，去噪自动编码器（DAE）[141,142]和压缩自动编码器（CAE）[118]。稀疏自动编码器[8]允许中间表示的大小（即由编码器部分生成的）大于输入的大小，同时通过惩罚负输出来强制实现稀疏。相比之下，去噪自动编码器[141,142]通过尝试从人为损坏的版本重建干净的输入来改变重建本身的目标，其目的是学习稳健的表示能力。类似地，压缩自动编码器[118]通过进一步惩罚对注入噪声最敏感的单元来构建去噪自动编码器。各种类型自动编码器的更详细介绍可以在其他地方找到，如参考文献[7]。

#### 2.1.2循环神经网络

在解决依赖于顺序输入的任务时，其中一种最成功的多层架构是循环神经网络（RNN）[9]。如图2.3所示，RNN可以看作是一种特殊的神经网络，其中每个隐藏单元从当前时间步骤观察到的数据以及前一时间点的状态中获取输入。 RNN的输出定义为

![](img/tex19.gif)（2.4）

其中![](img/tex20.gif)是非线性挤压函数，![](img/tex21.gif)和![](img/tex22.gif)是控制当前和过去信息相对重要性的网络参数。

![](img/x3.png)

图2.3：标准回归神经网络的工作流程说明。每个RNN单元在当前时间帧![](img/tex23.gif)处获取新输入，并且从之前的时间步长![](img/tex24.gif)和当前单元的新输出可根据式（2.4）计算，并可被输入至多层RNN的另一层处理中。

虽然RNN看似是强大的架构，但它们的主要问题之一是它们对长期依赖性建模的能力有限。这种限制归咎于由于在通过多个时间步骤传播误差时可能发生的梯度爆炸或者消失导致的训练困难[9]。特别是，在训练期间，反向传播的梯度将与从当前时间步长一直倒退到初始时间步长的网络权重相乘。因此，由于这种乘法累加，权重可以对传播的梯度具有重要的影响。如果权重很小，则梯度消失，而较大的权重导致梯度爆炸。为了解决这个难题，学者们提出了长期短期记忆（LSTM）[70]。

LSTM是配备了存储或者记忆器件的循环网络，如图2.4所示，它可以随时间累积信息。 LSTM的存储器单元可被门控，以便允许从中读写信息。值得注意的是，LSTM还包含一个遗忘门，允许网络在不再需要时擦写信息。 LSTM由三个不同的门（输入门![](img/tex25.gif)，遗忘门![](img/tex26.gif)和输出门![](img/tex27.gif)）以及存储器单元状态![](img/tex28.gif)控制。输入门由当前输入![](img/tex23.gif)和先前状态![](img/tex24.gif)控制，计算公式定义为

![](img/tex29.gif)（2.5）

其中，![](img/tex21.gif)，![](img/tex22.gif)，![](img/tex30.gif)分别表示控制与输入门的连接的权重和偏移量，![](img/tex20.gif)通常是S形函数。遗忘门同样被定义为

![](img/tex31.gif)（2.6）

它由相应的权重和偏差![](img/tex32.gif)，![](img/tex33.gif)，![](img/tex34.gif)控制。可以说，LSTM最重要的方面是它可以应对梯度消失和爆炸渐变的挑战。在确定存储器单元的状态时，通过遗忘门和输入门状态的相加组合来实现该能力，该状态又控制信息是否经由输出门传递到另一个单元。具体地，以两个步骤计算单元状态。首先，根据估计候选结构状态

![](img/tex35.gif)（2.7）

其中![](img/tex36.gif)通常是双曲正切函数。其次，最终的单元状态最终由当前估计的单元状态![](img/tex37.gif)和先前的单元状态![](img/tex38.gif)控制，由输入和遗忘门调制根据式(2.8)计算得到

![](img/tex39.gif)（2.8）

最后，使用单元的状态以及当前和先前的输入，输出门的值和LSTM单元的输出根据

![](img/tex40.gif)（2.9）

其中

![](img/tex41.gif)（2.10）

![](img/x4.png)

图2.4：典型LSTM单元的图示。该单元在当前时间输入![](img/tex23.gif)，从之前的时间![](img/tex24.gif)获取输入，并返回下一次输入的输出![](img/tex42.gif)。 LSTM单元的最终输出由输入门![](img/tex25.gif)，遗忘门![](img/tex26.gif)和输出门![](img/tex27.gif)以及存储单元状态![](img/tex28.gif)控制，它们由分别式（2.5），（2.6），（2.9）和（2.8）定义。本图复制自文献[33]。

#### 2.1.3卷积网络

卷积网络（ConvNets）是一种特别适用于计算机视觉应用的特殊类型神经网络，因为它们能够通过本地操作进行分层抽象表示。两个关键的设计理念推动计算机视觉中卷积体系结构的成功。首先，卷积网络利用图像的2D结构以及邻域内的像素通常高度相关的事实。因此，卷积网络避免在所有像素单元之间使用一对一连接（即大多数神经网络的情况一样），这有利于使用分组本地连接。此外，卷积网络架构依赖于特征共享，因此每个通道（或输出特征图）由在所有位置使用相同滤波器的卷积生成，结构如图2.5所述。卷积网络的这一重要特性，使得其与标准神经网络相比依赖于更少参数的架构。其次，卷积网络还引入了一个池化步骤，该步骤提供了一定程度的平移不变性，使得架构受到位置的微小变化的影响较小。值得注意的是，由于网络感知字段的大小增加，池化还允许网络逐渐看到输入的较大部分。接收场大小的增加（加上输入分辨率的降低）允许网络在网络深度增加时表达输入的更抽象的特征。例如，对于对象识别的任务，卷积网络层首先将边缘聚焦到对象部分以最终覆盖层次结构中较高层的整个对象。

![](img/x5.png)

图2.5：标准卷积网络结构的图示。本图复制自文献[93]

卷积网络的体系结构很大程度上受到了视觉皮层中处理过程的启发，如Hubel和Wiese在文献l [74]的开创性工作所述（这将在第3章中进一步讨论）。事实上，最早的卷积网络实例似乎是Fukushima在文献[49]中提出的神经认知机(Neocognitron)，它也依赖于本地连接，其中每个特征图最大限度地响应特定特征类型。 神经认知机由一系列![](img/tex43.gif)层组成，其中每层交替出现S细胞单元![](img/tex44.gif)和复杂细胞单位![](img/tex45.gif)，它们松散地模仿生物简单和复杂细胞中发生的过程，分别如图2.6所示。简单细胞单元执行类似于局部卷积的操作，然后执行线性整流单元（ReLU）非线性函数![](img/tex46.gif)，而复杂单元执行类似于平均合并的操作。该模型还包括一个分裂的非线性过程，以实现类似于当代卷积网络中规范化的过程。

![](img/x6.png)

图2.6：神经认知机结构图。本图复制自文献[49]

与大多数标准卷积网络架构（例如[91,88]）相反，神经认知机不需要标记数据进行学习，因为它是基于自组织映射设计的，通过重复学习连续层之间的局部连接一组激励图像的演示。具体地，训练神经认知机以学习输入特征图和简单细胞层之间的连接（简单细胞层和复杂细胞层之间的连接是预先固定的），并且学习过程可以在两个步骤中概括地概括。首先，每次在输入处呈现新的激励时，选择最大响应它的简单细胞作为该激励类型的代表性细胞。其次，每次响应相同的输入类型时，输入和那些代表性单元之间的连接就会得到加强。值得注意的是，简单的单元层被组织在不同的组或平面中，使得每个平面仅响应一种刺激类型（即类似于现代卷积网络架构中的特征映射）。对神经认知机的后续扩展包括监督学习的允许[51]以及自上而下的注意力机制[50]。

在最近计算机视觉应用中部署的大多数卷积网络架构都受到LeCun在1998年所提出的成功架构的启发，现在称为LeNet，用于手写识别[91]。如关键文献[77,93]所述，经典卷积网络由四个基本处理层组成：（i）卷积层，（ii）非线性或整流层，（iii）归一化层和（iv）池化层。如上所述，这些成分主要存在于神经认知机中。 LeNet的一个关键附加功能是结合反向传播，以便相对有效地学习卷积参数。

虽然允许优化架构的卷积网络，与完全连接的神经网络相比，所需要的参数要少得多，但它们的主要缺点仍然在于它们严重依赖学习和标记数据。这种数据依赖性可能是直到2012年卷积网络未被广泛使用的主要原因之一，因为大型ImageNet数据集的可用性[126]和相应的计算资源使得学者恢复对卷积网络的兴趣成为可能[88]。 卷积网络在ImageNet上的成功引发了各种卷积网络架构研究的突飞猛进，并且该领域的大多数贡献仅仅基于卷积网络的基本构建块的不同变化，稍后将在2.2节中讨论。

#### 2.1.4生成对抗网络

生成性对抗网络（GAN）是利用多层体系结构强大的代表性功能的相对较新的模型。 GAN最初是在2014年推出的[57]虽然它们本身没有提出不同的架构（_即_。例如，在新型网络构建模块方面），它们具有一些特殊性，这使得它们略有不同不同类型的多层架构。 GAN响应的一个关键挑战是采用无监督的学习方法，不需要标记数据。

典型的GAN由两个竞争块或子网组成，如图2.7所示;发生器网络![](img/tex47.gif)和鉴别器网络![](img/tex48.gif)，其中![](img/tex49.gif)是输入随机噪声，![](img/tex50.gif)是实际输入数据（_，例如_。图像）和![](img/tex51.gif)和![](img/tex52.gif)分别是两个块的参数。每个块可以由任何先前定义的多层体系结构构成。在原始论文中，发生器和鉴别器都是多层全连接网络。鉴别器![](img/tex53.gif)被训练为识别来自发生器的数据并且以概率![](img/tex54.gif)分配标签“假”，同时以概率![](img/tex55.gif)将标签“真实”分配给真实输入数据。作为补充，生成器网络被优化以生成能够欺骗鉴别器的伪表示。这两个块在几个步骤中交替训练，其中训练过程的理想结果是鉴别器，其将![](img/tex56.gif)的概率分配给真实和伪造数据。换句话说，在收敛之后，生成器应该能够从随机输入生成实际数据。

![](img/x7.png)

图2.7：通用生成性对抗网络（GAN）结构的图示。

自原始论文以来，许多贡献都参与了通过使用更强大的多层架构作为网络的主干来增强GAN的能力[114]（_，例如_。用于鉴别器和反卷积网络的预训练卷积网络，学习发电机的上采样滤波器。 GAN的一些成功应用包括：文本到图像合成（其中网络的输入是要渲染的图像的文本描述[115]），图像超分辨率，其中GAN从较低的生成逼真的高分辨率图像分辨率输入[94]，图像修复GAN的作用是从输入图像填充缺失信息的孔[149]和纹理合成，其中GAN用于从输入噪声合成真实纹理[10]。

#### 2.1.5多层网络培训

如前几节所述，各种多层架构的成功在很大程度上取决于其学习过程的成功与否。虽然神经网络通常首先依赖于无监督的预训练步骤，如2.1.1节所述，但它们通常遵循最广泛使用的多层架构训练策略，这是完全监督的。训练过程通常基于使用梯度下降的误差反向传播。梯度下降因其简单性而广泛用于训练多层架构。它依赖于最小化平滑误差函数![](img/tex57.gif)，遵循定义为的迭代过程

![](img/tex58.gif)（2.11）

其中![](img/tex59.gif)表示网络参数，![](img/tex15.gif)是可以控制收敛速度的学习速率，![](img/tex60.gif)是在训练集上计算的误差梯度。这种简单的梯度下降方法特别适用于训练多层网络，这要归功于使用链规则进行反向传播并计算相对于不同层的各种网络参数的误差导数。虽然反向传播可追溯到多年[16,146]，但它在多层架构的背景下得到了普及[125]。在实践中，使用随机梯度下降[2]，其包括从连续的相对小的子集中近似整个训练集上的误差梯度。

梯度下降算法的主要问题之一是学习率![](img/tex15.gif)的选择。学习率太小会导致收敛缓慢，而较大的学习率会导致围绕最佳状态的过冲或波动。因此，提出了几种方法来进一步改进简单的随机梯度下降优化方法。最简单的方法，称为随机梯度下降与动量[137]，跟踪从一次迭代到另一次迭代的更新量，并通过进一步推动更新，如果梯度从一个方向指向同一方向，进一步推动学习过程时间步骤到另一个定义，

![](img/tex61.gif)（2.12）

用![](img/tex62.gif)控制动量。另一种简单的方法涉及根据固定的时间表以递减的方式设置学习速率，但这远非理想，因为该时间表必须在训练过程之前预先设定并且完全独立于数据。其他更复杂的方法（_，例如_ .Adagrad [34]，Adadelta [152]，Adam [86]）建议通过执行较小的更新，在训练期间调整学习率到每个参数![](img/tex21.gif)。频繁变化的参数和不常见的更新。这些算法的不同版本之间的详细比较可以在其他地方找到[124]。

使用梯度下降及其变体进行训练的主要缺点是需要大量标记数据。解决这一困难的一种方法是采用无监督学习。用于训练一些浅层ConvNet架构的流行的无监督方法基于预测稀疏分解（PSD）方法[85]。 Predictive Sparse Decomposition学习一组过度完整的滤波器，其组合可用于重建图像。该方法特别适用于学习卷积体系结构的参数，因为该算法被设计用于学习以补丁方式重建图像的基函数。具体地，预测稀疏分解（PSD）建立在稀疏编码算法的基础上，该算法试图通过与基组B的线性组合来找到输入信号X的有效表示Y.形式上，稀疏编码的问题广泛地存在。制定为最小化问题，定义为，

![](img/tex63.gif)（2.13）

PSD通过最小化定义为的重构误差，在卷积框架中调整稀疏编码的思想，

![](img/tex64.gif)（2.14）

其中![](img/tex65.gif)和![](img/tex66.gif)，![](img/tex53.gif)和![](img/tex67.gif)分别是网络的权重，偏差和增益（或归一化因子）。通过最小化方程2.14中定义的损失函数，算法学习表示![](img/tex68.gif)，重建输入补丁![](img/tex69.gif)，同时类似于预测表示![](img/tex70.gif)。由于等式的第二项，学习的表示也将是稀疏的。在实践中，误差在两个交替步骤中被最小化，其中参数![](img/tex71.gif)是固定的并且在![](img/tex68.gif)上执行最小化。然后，表示![](img/tex68.gif)被固定，同时最小化其他参数。值得注意的是，PSD以补片程序应用，其中每组参数![](img/tex72.gif)是从输入图像重建不同的补丁中学习的。换句话说，通过将重建聚焦在输入图像的不同部分上来学习不同的内核集。

#### 2.1.6关于转学的一个词

培训多层体系结构的一个意想不到的好处是学习特征在不同数据集甚至不同任务中的令人惊讶的适应性。例子包括使用ImageNet训练的网络识别：其他物体识别数据集，如Caltech-101 [38]（_，例如_。[96,154]），其他识别任务，如纹理识别（_例如_。[25]），其他应用，如物体检测（_，例如_。[53]）甚至基于视频的任务，如视频动作识别（_，例如_）。 [134,41,144]）。

使用多层体系结构在不同数据集和任务中提取的特征的适应性可以归因于它们的层次性，其中表示从简单和局部到抽象和全局。因此，在层次结构的较低层提取的特征往往在不同的任务中是共同的，从而使多层架构更适合于转移学习。

对不同网络和任务中特征的有趣可转移性进行系统探索，揭示了考虑转移学习时需要考虑的几个良好实践[150]。首先，它表明仅微调更高层，与微调整个网络相比，系统性能更好。其次，这项研究表明，任务越多，转移学习效率就越低。第三，更令人惊讶的是，人们发现，即使经过微调，网络在初始任务下的表现也不会受到特别的阻碍。

最近，一些新兴的努力试图通过将学习问题作为连续的两步程序，_，例如_ [3,127]来强制执行网络“转移学习能力。首先，所谓的快速学习步骤如通常所做的那样，在网络针对特定任务进行优化的情况下执行。其次，在全局学习步骤中进一步更新网络参数，该步骤尝试最小化不同任务之间的错误。

### 2.2空间卷积网络

理论上，卷积网络可以应用于任意维度的数据。它们的二维实例非常适合于单个图像的结构，因此在计算机视觉中受到了相当大的关注。随着大规模数据集和功能强大的计算机的可用性，视觉界最近看到ConvNets在各种应用中的使用激增。本节介绍最突出的2D ConvNet架构，它将相对新颖的组件引入2.1.3节中描述的原始LeNet。

#### 2.2.1最近ConvNets演变中的关键架构

重新引起对ConvNet架构兴趣的工作是Krishevsky的AlexNet [88]。 AlexNet能够在ImageNet数据集上实现破纪录的对象识别结果。它由8层组成，5个卷积和3个完全连接，如图2.8所示。

AlexNet介绍了几种架构设计决策，允许使用标准随机梯度下降进行有效的网络训练。特别是，四项重要贡献是AlexNet成功的关键。首先，AlexNet考虑使用ReLU非线性而不是先前最先进的ConvNet架构中使用的饱和非线性（如sigmoids）（_，例如_ .LeNet [91]）。 ReLU的使用减少了消失梯度的问题，并导致更快的训练。其次，注意到网络中最后一个完全连接的层包含最多参数的事实，AlexNet使用了丢失，首先在神经网络[136]的背景下引入，以减少过度拟合的问题。在AlexNet中实现的Dropout，包括随机丢弃（_，即_。设置为零）一个层参数的给定百分比。该技术允许在每次通过时训练略微不同的架构并且人为地减少每次通过时要学习的参数的数量，这最终有助于破坏单元之间的相关性，从而防止过度拟合。第三，AlexNet依靠数据增强来提高网络学习不变表示的能力。例如，网络不仅训练在训练集中的原始图像上，而且还训练通过随机移动和反射训练图像而产生的变化。最后，AlexNet还依靠几种技术使训练过程更快地收敛，例如使用动量和预定学习率降低，从而每次学习停滞时学习率都会降低。

![](img/x8.png)

图2.8：AlexNet架构。值得注意的是，虽然描述建议采用双流体系结构，但实际上它是单流体系结构，这种描述仅反映了AlexNet在2个不同的GPU上并行训练的事实。图[88]再版。

AlexNet的出现导致试图通过可视化了解网络正在学习什么的论文数量急剧增加，如所谓的DeConvNet [154]，或者通过对各种架构的系统探索[22,23]。这些探索的直接结果之一是认识到更深的网络可以实现更好的结果，如19层深VGG-Net中首次证明的那样[135]。 VGG-Net通过简单堆叠更多层来实现其深度，同时遵循AlexNet引入的标准实践（_，例如_。依靠ReLU非线性和数据增强技术进行更好的培训）。在VGG-Net中呈现的主要新颖性是使用空间范围较小的滤波器（_即_。![](img/tex73.gif)在整个网络中过滤而不是_，例如_。在AlexNet中使用![](img/tex74.gif)过滤器），它允许增加深度而不会显着增加网络需要学习的参数数量。值得注意的是，在使用较小的过滤器时，VGG-Net每层需要更多的过滤器。

VGG-Net是遵循AlexNet的许多深度ConvNet架构中的第一个也是最简单的。后来提出了一个更深层的架构，通常称为GoogLeNet，有22层[138]。由于使用了所谓的初始模块（如图2.9（a）所示）作为构建块，因此GoogLeNet比VGG-Net更深，所需参数要少得多。在初始模块中，各种尺度的卷积运算和空间池并行发生。该模块还增加了![](img/tex75.gif)卷积（_，即_。跨通道池），用于降低维数以避免或衰减冗余过滤器，同时保持网络的大小可管理。这种跨渠道汇集的想法是由以前的一项名为网络网络（NiN）[96]的研究结果推动的，该研究揭示了学习网络中的大量冗余。堆叠许多初始模块导致现在广泛使用的GoogLeNet架构如图2.9（b）所示。

| ![](img/x9.png) |
| （一个） |
| ![](img/x10.png) |
| （b）中 |

图2.9：GoogLeNet架构。 （a）典型的初始模块，显示顺序和并行发生的操作。 （b）典型“初始”架构的图示，其包括堆叠许多初始模块。图[......]转载

GoogLeNet是第一个偏离简单堆叠卷积和汇集层的策略的网络，很快就出现了迄今为止最深层的架构之一，称为ResNet [64]，它还提出了一个超过150层的新架构。 ResNet代表剩余网络，主要贡献在于它依赖于剩余学习。特别是，构建ResNet使得每个层在输入![](img/tex77.gif)之上学习增量变换![](img/tex76.gif)，根据

![](img/tex78.gif)（2.15）

而不是像其他标准ConvNet架构那样直接学习转换![](img/tex79.gif)。这种剩余学习是通过使用跳过连接来实现的，如图2.10（a）所示，它使用身份映射连接不同层的组件。信号的直接传播![](img/tex77.gif)在反向传播期间对抗消失的梯度问题，从而能够训练非常深的架构。

| ![](img/x11.png) |
| (a) |
| ![](img/x12.png) |
| (b) |

图2.10：ResNet架构。 （a）剩余模块。 （b）典型ResNet架构的图示，包括堆叠许多剩余模块。图[64]转载。

最近，一个关于ResNet成功的密切相关的网络就是所谓的DenseNet [72]，它进一步推动了剩余连接的概念。在DenseNet中，每个层通过跳过连接连接到密集块的所有后续层，如图2.11所示。具体地，密集块将所有层与相同大小的特征图连接（_，即空间池层之间的_。块）。与ResNet不同，DenseNet不会添加前一层的特征映射（2.15），而是连接特征映射，以便网络根据以下内容学习新的表示。

![](img/tex80.gif)（2.16）

作者声称，这种策略允许DenseNet在每一层使用更少的过滤器，因为通过将在一层提取的特征推送到层次结构中更高层的其他层，可以避免可能的冗余信息。重要的是，这些深度跳过连接允许更好的梯度流，因为较低层可以更直接地访问损耗函数。使用这个简单的想法，DenseNet可以与其他深层架构竞争，例如ResNet，同时需要更少的参数并减少过度拟合。

| ![](img/x13.png) |
| (a) |
| ![](img/x14.png) |
| (b) |

图2.11：DenseNet架构。 （a）密集的模块。 （b）典型的DenseNet架构的图示，该架构包括堆叠许多密集模块。图[72]再版。

#### 2.2.2走向ConvNet不变性

使用ConvNets的挑战之一是需要非常大的数据集来学习所有基础参数。即使像ImageNet [126]这样拥有超过一百万张图像的大型数据集也被认为太小而无法训练某些深层架构。处理大数据集要求的一种方法是通过例如随机翻转，旋转和抖动来改变图像来人为地增加数据集。这些增强的主要优点是所得到的网络对各种变换变得更加不变。事实上，这项技术是AlexNet取得巨大成功背后的主要原因之一。因此，除了改变网络架构以便于培训的方法之外，如前一节所述，其他工作旨在引入能够产生更好培训的新型构建模块。具体而言，本节讨论的网络引入了新的块，这些块直接包含来自原始数据的学习不变表示。

一个明确解决不变性最大化的着名ConvNet是空间变压器网络（STN）[76]。特别地，该网络利用了一种新的学习模块，该模块增加了对不重要的空间变换的不变性，_，例如_。在物体识别过程中由不同视点产生的那些。该模块由三个子模块组成：定位网，网格生成器和采样器，如图2.12（a）所示。执行的操作可以分为三个步骤。首先，定位网络（通常是一个小的2层神经网络）将一个特征图![](img/tex81.gif)作为输入，并从该输入中学习变换参数![](img/tex82.gif)。例如，转换![](img/tex83.gif)可以定义为一般的仿射变换，允许网络学习翻译，缩放，旋转和剪切。其次，给定变换参数和预定义大小的输出网格![](img/tex84.gif)，网格生成器为每个输出坐标![](img/tex85.gif)计算应从输入中采样的相应坐标![](img/tex86.gif)，![](img/tex81.gif)，根据

![](img/tex87.gif)（2.17）

最后，采样器采用特征图![](img/tex81.gif)和采样网格，并插入像素值![](img/tex86.gif)，以填充位置![](img/tex85.gif)的输出特征图![](img/tex88.gif)，如图2.12所示（b） ）。在任何ConvNet架构的每一层添加此类模块，使其能够从输入中自适应地学习各种变换，以增加其不变性，从而提高其准确性。

| ![](img/x15.png) | ![](img/x16.png) |
| （一个） | （b）中 |

图2.12：空间变压器网络运营。 （a）空间变换器模块的描述，典型的变换操作在（b）中示出。图[〜]复制。

为了增强ConvNets的几何变换建模能力，两种现代方法，即Deformable ConvNet [29]和Active ConvNet [78]，引入了灵活的卷积块。这些方法的基本思想是避免在卷积过程中使用刚性窗口，以有利于学习进行卷积的感兴趣区域（RoI）。这个想法类似于空间变换器模块的定位网络和网格生成器所做的事情。为了确定每层的RoI，修改卷积块以使其从初始刚性卷积窗口学习偏移。具体来说，从在给定的刚性窗口上的卷积运算的标准定义开始

![](img/tex89.gif)（2.18）

其中![](img/tex90.gif)是执行卷积的区域，![](img/tex91.gif)是区域内的像素位置![](img/tex90.gif)，![](img/tex92.gif)是相应的滤波器权重，添加新术语以包括根据的偏移量

![](img/tex93.gif)（2.19）

其中![](img/tex94.gif)是偏移量，现在最终的卷积步骤将在变形窗口上执行，而不是传统的刚性![](img/tex95.gif)窗口。为了学习偏移量![](img/tex94.gif)，可以修改可变形ConvNets的卷积块，使其包含一个新的子模块，其作用是学习偏移，如图2.13所示。与可交替学习子模块参数和网络权重的空间变换器网络不同，可变形控制器可同时学习权重和偏移，从而使其在各种体系结构中的部署速度更快，更容易。

![](img/x17.png)

图2.13：可变形或主动卷积。从固定的窗口大小开始，网络通过一个小的子网络（在图的顶部显示为绿色）学习偏移，最后在变形的窗口上执行卷积。图[29]再版。

#### 2.2.3走向ConvNet本地化

除了简单的分类任务，例如对象识别，最近ConvNets在需要精确定位的任务中也表现出色，例如语义分割和对象检测。用于语义分割的最成功的网络是所谓的完全卷积网络（FCN）[98]。顾名思义，FCN没有明确地使用完全连接的层，而是将它们转换为卷积层，其感知域覆盖整个底层特征映射。重要的是，网络学习上采样或反卷积滤波器，恢复最后一层图像的完整分辨率，如图2.14所示。在FCN中，通过将问题转换为密集像素分类来实现分割。换句话说，softmax层附加到每个像素，并且通过对属于相同类的像素进行分组来实现分割。值得注意的是，在这项工作中报告说，在上采样步骤中使用来自架构的较低层的特征起着重要作用。它允许更精确的分割，因为较低层特征倾向于捕获更细粒度的细节，与分类相比，这对于分割任务而言更为重要。学习反卷积滤波器的替代方法依赖于使用atrou或扩张卷积[24]，_，即_。上采样稀疏滤波器，有助于恢复更高分辨率的特征映射，同时保持可学习的参数数量可管理。

![](img/x18.png)

图2.14：完全卷积网络。在上采样以在最后一层恢复图像全分辨率之后，使用softmax对每个像素进行分类以最终生成片段。图[98]再版。

在对象本地化方面，ConvNet框架中最早的方法之一被称为Region CNN或R-CNN。该网络将区域提议方法与ConvNet架构相结合[53]。尽管R-CNN是围绕简单的想法而建立的，但它产生了最先进的物体检测结果。特别地，R-CNN首先使用用于区域提议的现成算法（_，例如_。选择性搜索[140]）来检测可能包含对象的潜在区域。然后将这些区域扭曲以匹配所使用的ConvNet的默认输入大小，并将其馈送到ConvNet以进行特征提取。最后，每个区域的特征用SVM分类，并在后处理步骤中通过非最大抑制进行细化。

在其天真的版本中，R-CNN简单地使用ConvNets作为特征提取器。然而，它的突破性结果带来了改进，更多地利用了ConvNets的强大代表性。例如，快速R-CNN [52]，更快的R-CNN [116]和掩模R-CNN [61]。快速R-CNN ，提出通过网络传播独立计算区域提议，以提取最后一个特征映射层中的相应区域。这种技术避免了从图像中提取的每个区域的昂贵的网络传输。此外，快速R-CNN避免了重载 - 处理步骤通过改变网络的最后一层，使其学习对象类和精炼的边界框坐标。重要的是，在R-CNN和快速R-CNN中，检测瓶颈在于区域提议步骤，该步骤是在ConvNet范例。

更快的R-CNN通过在ConvNet的最后一个卷积层之后添加称为区域提议网络（RPN）的子模块（或子网络）来进一步推动使用ConvNets。 RPN模块使网络能够学习区域提议，作为网络优化的一部分。具体来说，RPN被设计为一个小的ConvNet，由卷积层和一个小的完全连接层组成，两个输出返回潜在的物体位置和物体分数（_，即_。属于一个物体类的概率）。按照迭代的两步程序完成整个网络的培训。首先，使用RPN单元优化网络以进行区域建议提取。其次，保持提取的区域提议固定，网络被微调用于对象分类和最终对象边界框位置。最近，引入掩模R-CNN以增强更快的R-CNN，其具有分割检测区域的能力，从而在检测到的物体周围产生紧密掩模。为此，掩码R-CNN将分段分支添加到更快的R-CNN的分类和边界框回归分支。特别是，新分支实现为一个小FCN，它被优化用于将任何边界框中的像素分类为两个类中的一个;前景或背景。图2.15说明了从简单的R-CNN到屏蔽R-CNN的差异和进展。

| ![](img/x19.png) | ![](img/x20.png) |
| (a) | (b) |
| ![](img/x21.png) | ![](img/x22.png) |
| （C） | （d） |

图2.15：突出区域提案网络的进展情况。 （a）原始R-CNN的结构。图[...]转载。 （b）快速R-CNN的结构。图[...]转载。 （c）更快的R-CNN的结构。从[116]再现的图。（d）掩模R-CNN的结构。图[〜]复制。

### 2.3时空卷积网络

通过使用ConvNets为各种基于图像的应用程序带来的显着性能提升，如第2.2节所述，引发了对将2D空间ConvNets扩展到用于视频分析的3D时空ConvNets的兴趣。通常，文献中提出的各种时空架构只是试图将2D架构从空间域![](img/tex96.gif)扩展到时域![](img/tex97.gif)。在基于训练的时空ConvNets领域，有三种不同的建筑设计决策是突出的：基于LSTM（_例如_。[112,33]），3D（_例如_。[139] ，84]和双流会议（_，例如_。[134,43]），将在本节中描述。

#### 2.3.1基于LSTM的时空ConvNet

基于LSTM的时空转换，_，例如_。 [112,33]是将2D网络扩展到时空处理的早期尝试。他们的操作可以分为三个步骤，如图2.16所示。首先，使用2D网络处理每个帧，并从其最后一层提取特征向量。其次，这些来自不同时间步骤的特征然后被用作产生时间结果的LSTM的输入，![](img/tex98.gif)。第三，然后将这些结果平均或线性组合并传递给softmax分类器以进行最终预测。

![](img/x23.png)

图2.16：基于LSTM的时空ConvNet示例。在该网络中，输入包括来自视频流的连续帧。图[33]再版。

基于LSTM的ConvNets的目标是逐步整合时态信息，同时不受限于严格的输入大小（暂时）。这种架构的一个好处是使网络能够生成可变大小的文本描述（_，即_ .LSTMs优秀的任务），如[33]中所做的那样。然而，虽然LSTM可以捕获全局运动关系，但它们可能无法捕获更精细的运动模式。此外，这些模型通常较大，需要更多数据，因此难以训练。迄今为止，除了正在整合视频和文本分析的情况（_，例如_。[33]），LSTM通常在时空图像分析中取得了有限的成功。

#### 2.3.2 3D ConvNet

第二种突出类型的时空网络为标准2D ConvNet处理提供了最简单的图像时空概括。它直接与RGB图像的时间流一起工作，并通过应用学习的3D，![](img/tex97.gif)，卷积滤波器对这些图像进行操作。这种泛化形式的一些早期尝试使用过滤器，这些过滤器延伸到具有非常浅的网络的时间域[80]或仅在第一卷积层[84]。当仅在第一层使用3D卷积时，在每3或4个连续帧上应用小抽头时空滤波器。为了捕获更长距离的运动，并行使用多个这样的流，并且由堆叠这样的流产生的层级增加了网络的时间接收场。但是，由于时空滤波仅限于第一层，因此这种方法与基于朴素帧的2D ConvNets应用相比没有产生显着的改进。现在广泛使用的C3D网络提供了更强的泛化，它在所有层使用3D卷积和汇集操作[139]。 C3D从2D到3D架构的直接推广需要大量增加要学习的参数数量，这可以通过在所有层使用非常有限的时空支持来补偿（_即_。![](img/tex99.gif)卷积）。最近略有不同的方法提出通过修改ResNet架构[64]来集成时间过滤，以成为时间ResNet（T-ResNet）[42]。特别是，T-ResNet使用![](img/tex100.gif)滤波器增加剩余单位（如图2.10（a）所示），该滤波器沿时间维度应用一维学习滤波操作。

最终，此类3D ConvNet架构的目标是在整个模型中直接集成时空滤波，以便同时捕获外观和运动信息。这些方法的主要缺点是其参数数量的增加。

#### 2.3.3双流ConvNet

第三种类型的时空架构依赖于双流设计。标准的双流结构[134]，如图2.17所示，在两个平行路径中运行，一个用于处理外观，另一个用于运动，类似于生物视觉系统研究中的双流假设[55]。输入到外观路径是RGB图像;输入到运动路径是光流场的堆栈。基本上，每个流都使用相当标准的2D ConvNet架构单独处理。每个途径进行单独分类，后期融合用于实现最终结果。原始双流网络的各种改进遵循相同的基本思想，同时为各个流使用各种基线架构（_，例如_。[43,443,144]）或提出连接两个流的不同方式（_例如_。[43,40,41]）。值得注意的是，最近的工作称为I3D [20]，建议通过在两个流上使用3D卷积来使用3D滤波和双流架构。然而，除了网络在基准动作识别数据集上获得稍好的结果之外，作者还没有提出令人信服的论据来支持除3D过滤之外的冗余光流流的需求。

![](img/x24.png)

图2.17：原始的双流网络。网络将RGB帧和光流堆栈作为输入。图[134]转载。

总体而言，双流控制系统支持外观和运动信息的分离，以便了解时空内容。值得注意的是，这种架构似乎是时空ConvNets中最受欢迎的，因为它的变化导致了各种动作识别基准的最新结果（_，例如_。[43,40,41,144]） 。

### 2.4总体讨论

多层表示一直在计算机视觉中发挥重要作用。实际上，即使是标准广泛使用的手工制作的特征，如SIFT [99]，也可以看作浅层多层表示，松散地说，它包括卷积层，然后是汇集操作。此外，前ConvNet最先进的识别系统通常遵循手工制作的特征提取与（学习的）编码，然后是空间组织的池和学习的分类器[_，例如_。[39]），也是一种多层代表性方法。现代多层体系结构推动了分层数据表示的思想更深层次，同时通常避开手工设计的特征，转而采用基于学习的方法。在计算机视觉应用方面，ConvNets的特定架构使它们成为最具吸引力的架构之一。

总的来说，虽然处理多层网络的文献非常庞大，每个派系都提倡一种架构优于另一种架构，但已经出现了一些常见的“最佳实践”。突出的例子包括：大多数架构依赖于四个常见构建块（_即_。卷积，整流，规范化和池化），深度架构与小型支持卷积内核的重要性，以实现具有可管理数量的抽象参数，残余连接以应对学习过程中误差梯度传播的挑战。更一般地，文献同意关键点，输入数据的良好表示是分层的，如前面几个贡献中所述[119]。

重要的是，虽然这些网络在许多计算机视觉应用中取得了竞争性成果，但它们的主要缺点依然存在：对学习表示的确切性质的理解有限，对大量训练数据集的依赖性，缺乏支持精确性能界限的能力以及缺乏关于网络超参数选择的清晰度。这些选择包括滤波器大小，非线性选择，池功能和参数以及层数和架构本身。在ConvNets“构建块”的背景下，其中几个选择背后的动机将在下一章中讨论。