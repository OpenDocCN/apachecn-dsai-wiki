## 三、了解ConvNets构建块

鉴于ConvNets领域中过多的未解答的问题，本章研究了典型卷积网络中每层处理的作用和重要性。为此，将审查处理这些问题的最突出的努力。特别是，各种ConvNet组件的建模将从理论和生物学角度进行介绍。每个组成部分的介绍以讨论结束，总结了我们目前的理解水平。

### 3.1卷积层

可以说，卷积层是ConvNet架构中最重要的步骤之一。基本上，卷积是线性的移位不变运算，包括在输入信号上执行局部加权组合。根据所选择的权重集（_，即_。所选择的点扩展函数），显示输入信号的不同属性。在频域中，点扩散函数的相关性是调制函数，其告知如何通过缩放和相移来修改输入的频率分量。因此，选择正确的内核来捕获输入信号中包含的最重要和最重要的信息是至关重要的，这些信息允许对信号内容做出强有力的推断。本节讨论了处理内核选择步骤的一些不同方法。

#### 3.1.1生物学观点

在mamalian视觉皮层中进行分层处理的神经生理学证据为空间和时空ConvNets提供了潜在的启发。特别是，假设一系列简单和复杂细胞逐渐提取视觉输入的更抽象属性的研究[74]特别重要。在视觉皮层处理的最早阶段，简单细胞显示出能够检测原始特征，例如定向光栅，条形和边缘，在后续阶段出现更复杂的调谐。

用于对皮质简单细胞的所述特性进行建模的流行选择是一组定向的Gabor滤波器或各种尺度的高斯导数。更一般地，在该处理级别选择的滤波器通常是定向带通滤波器。几十年后，大多数生物模型仍然依赖于层次结构初始层的同一组简单单元[117,130,131,79,5,48]。事实上，这些相同的Gabor核也被扩展到色彩[155]和时间[79]域，分别解释了颜色和运动敏感神经元。

然而，当涉及在视觉皮层的更高区域表示细胞时，问题变得更加微妙，并且在Hubel和Wiesel的工作上建立的大多数贡献都在努力寻找这些区域的适当表示。 HMAX模型是解决这个问题的最着名的模型之一[117]。 HMAX模型的主要思想是通过组合来自先前层的滤波器获得层次结构的较高层处的滤波器，使得较高层处的神经元响应先前神经元的共激活。这种方法最终应该允许模型在更高层响应越来越复杂的模式，如图3.1所示。这种方法与Hebbian理论很好地相关，即“一起发射，连接在一起的细胞”[65]。

![](img/x25.png)

图3.1：HMAX模型的图示。该模型由具有交替的简单（![](img/tex101.gif)）和复杂（![](img/tex102.gif)）细胞的细胞层次组成。过滤操作发生在![](img/tex101.gif)单元的级别。在该图中显示，初始层的简单细胞（![](img/tex103.gif)）通过使用定向Gabor过滤器检测简单定向条（_，即_。）。另一方面，较高层的简单单元（![](img/tex104.gif)）响应模板的过滤，模板是前一个（![](img/tex103.gif)）层使用的过滤器组合，这样层次结构中较高层的单元检测到比定向更复杂的形状酒吧。复合复合单元（C1，C2）介于简单单元层之间，以跨越空间位置聚集类似调谐的单元，从而实现一定程度的移位不变性。图[......]转载。

HMAX模型的另一个标志是假设学习发挥作用以便识别相似视觉序列的各种观点。此后该工作的直接扩展明确地介绍了学习在更高层建模过滤器。最成功的此类方法之一是由Serre _等人_引入的生物学动机网络。 [131]试图模拟在视觉皮层的初始层发生的过程，其中由![](img/tex105.gif)层组成的网络，其中简单（![](img/tex101.gif)）和复杂（![](img/tex102.gif)）细胞交替，如图3.2所示。可以看出，每个简单细胞后面都是复杂细胞，因此网络的整体结构可以概括为![](img/tex106.gif)。在这个网络中，卷积发生在![](img/tex103.gif)和![](img/tex104.gif)单元的层面。虽然![](img/tex103.gif)单元依赖于![](img/tex107.gif)导向的Gabor滤波器，但第二层使用的内核基于学习组件。这种选择的动机是生物学证据表明学习发生在皮层的较高层[130]，尽管也有证据表明学习在视觉皮层的早期层面起作用[11]。在这种情况下，学习过程对应于从![](img/tex110.gif)层的训练集中选择随机的![](img/tex108.gif)补丁![](img/tex109.gif)，其中![](img/tex111.gif)是补丁的空间范围，![](img/tex112.gif)对应到方向的数量。 ![](img/tex104.gif)层特征图是通过在每个尺度中的![](img/tex110.gif)特征与所有方向上的学习补丁![](img/tex109.gif)的同时执行模板匹配而获得的。

视频处理[79]存在对这项工作的直接扩展。用于视频处理的内核旨在模拟背侧流中细胞的行为。在这种情况下，![](img/tex103.gif)单元涉及具有3D定向滤波器的卷积。特别地，使用三阶高斯导数滤波器，因为它们具有良好的可分离性，并且采用类似的学习过程来为![](img/tex104.gif)和![](img/tex113.gif)单元选择卷积核。

![](img/x26.png)

图3.2：Serre _等_提出的网络架构。与HMAX模型[117]类似，它由交替的简单和复杂小区组成，因此所提出的网络的总体架构可以概括为![](img/tex106.gif)。然而，与HMAX模型相反，在训练集中明确地学习在S2单元级别使用的模板，使得该层检测复杂对象（_，即_。当用对象识别数据集训练时）。该过程的细节总结在该图的第二行。图[131]转载。

已经提出了上述基本思想的许多变化，包括更高层的各种学习策略[147,145]，基于小波的滤波器[71]，不同的特征稀疏化策略[73,147,110]和滤波器参数的优化[147， 107]。

另一个相关但有些不同的思路表明，在层次结构的更高层次上存在更复杂的单元，这些单元专用于捕获中间形状表示，_，例如_。曲率[120,121]。虽然HMAX类模型通过来自先前层的特征类型的组合建议建模形状，但是这些研究提出了一种直接模拟超复杂细胞（也称为终止细胞）而不求助于学习的方法。特别地，属于该范例模型的模型通过简单和复杂细胞的组合来复杂化细胞以产生新细胞，其能够最大程度地响应不同程度和符号的曲率以及不同位置处的不同形状。在建议超复杂细胞进行曲率计算时，这项工作建立在早期工作的基础上，这些工作表明了类似的功能，_，例如_。 [32]。

另一个研究机构，主张在视觉皮层中发生的分层处理（称为![](img/tex114.gif)）逐步处理高阶图像结构[5,48,108]。因此提倡在第一层（_，即_取向的带通滤波器）存在的同一组核在较高层重复。然而，假设相同的内核集合现在对从前一层获得的不同输入进行操作，则每层的处理揭示了输入信号的不同属性。因此，在连续层提取的特征从简单和局部变为抽象和全局，同时捕获更高阶的统计。此外，联合统计也通过跨不同尺度和方向的分层响应的组合来解释。

##### 讨论

人类视觉皮层在识别世界的同时对各种变化保持不变的能力一直是该领域许多研究人员的驱动力。尽管已经提出了几种方法和理论来模拟视觉皮层的不同层，但是跨越这些努力的共同点是存在将视觉任务分成更小块的分层处理。然而，虽然大多数模型都同意初始层的内核集合的选择，但是由Hubel和Wiesel [74]的开创性工作推动，负责识别更抽象特征的建模区域似乎更复杂和有争议。此外，这些生物学上合理的模型通常会对其设计决策的理论基础留下开放的关键问题。这个缺点也适用于更多的理论驱动模型，将在下一节中讨论。

#### 3.1.2理论视角

更多的理论驱动方法通常受到生物学的启发，但努力为其模型注入更多的理论依据。这些方法通常根据其内核选择策略而有所不同。

查看内核选择问题的一种方法是考虑自然界中的对象是一组原始形状的集合，从而采用基于形状的解决方案[47,45,46]。在这种情况下，所提出的算法首先使用一组定向Gabor滤波器在图像中找到最原始的形状（_，即_。定向边缘）。使用这些边缘，或更一般地说是部分，算法通过查看每个部分周围越来越大的邻域来找到下一层中的部分的潜在组合。基本上，每次向网络呈现新图像时，收集关于在前一层中给定部分的直接邻域中存在其他部分类型的投票。在网络看到训练集中存在的所有图像之后，使用来自前一层的部分的组合来构建网络的每个层。组合的选择基于在无监督训练期间学习的概率。实际上，这种基于形状的方法更多地是概念证明，其中只有层级的较低层可以以这种无监督的方式学习，而较高层是使用类别特定图像学习的，如图3.3所示。因此，只有当网络仅从该对象类中看到示例时，才能在更高层中获得对象的良好表示。但是，由于这种限制，这种算法无法合理地部署在具有以前未曾见过的不同类别的对象的更具挑战性的数据集上。

![](img/x27.png)

图3.3：由Fidler _等_提出的多层结构学习的样本部分。第1行（从左到右）：第2层和第3层样本部分。第2行和第3行：使用面部，汽车和马克杯学习第4层和第5层部分。图[......]转载。

内核选择过程的另一个展望是基于许多基于训练的卷积网络学习冗余过滤器的观察。此外，在这些网络的前几层中的许多学习过滤器类似于定向带通滤波器; _例如_。见图3.8。因此，最近的一些调查旨在将先验注入其网络设计中，特别关注卷积滤波器选择。一种方法提出在2D导数算子[75]的基础集上学习分层滤波器，如图3.4所示。虽然此方法使用固定基础的滤波器集，但它依赖于监督学习来在每个层的基础中线性组合滤波器以产生有效的分层滤波器，因此它与数据集相关。尽管如此，使用一组过滤器和学习组合可以很好地与生物模型保持一致，例如HMAX [117]及其后继者（_，例如_。[131,79]），并简化网络“架构，同时另外，由于学习是现代ConvNets的瓶颈之一，使用基础集也可以通过极大地减少要学习的参数数量来简化这一过程。由于这些原因，这些方法在最近的文献中越来越受欢迎[75] ，28,148,100,158]。

有趣的是，跨越这些最近努力的共同点是减少冗余内核的目标，特别注重建模旋转不变性（尽管它不一定是生物视觉的属性）。关注旋转的动机是观察到，通常，学习过滤器是彼此的旋转版本。例如，一项工作通过对一组圆谐波进行训练来有针对性地学习旋转等效[148]。或者，其他方法试图通过改变网络结构本身来硬编码旋转不变性，使得对于每个学习的滤波器，直接基于预定义的一组取向_例如_自动生成一组对应的旋转版本。 [158]，或者通过将每个学习的滤波器与定向Gabor滤波器的基组进行卷积[100]。

![](img/x28.png)

图3.4：接收域CNN（也称为RFNN）的示意图。在该网络中，所有层使用的滤波器（通过学习）构建为基本滤波器组![](img/tex115.gif)的线性组合，其是一组![](img/tex116.gif)阶高斯导数。该网络不是学习过滤器的内核参数，而是学习用于线性组合基组中的过滤器的参数![](img/tex117.gif)。图[〜]复制。

其他方法通过将内核选择问题作为基于群论的_例如_的不变性最大化问题，通过完全手工制作其网络，进一步推动将先验注入其网络设计的想法。 [15,113,28]。例如，可以选择内核，使得它们最大化对小变形和变换的不变性，以进行纹理识别[15]或最大化对象识别的旋转不变性[113]。

可以说，散射变换网络（ScatNet）具有迄今为止最严格的数学定义之一[15]。散射变换的构造从断言开始，即根据手头的任务，良好的图像表示应该对小的局部变形和各种变换组不变。此方法中使用的内核是一组扩张和旋转的小波![](img/tex118.gif)，其中![](img/tex119.gif)是小波的频率位置，定义为![](img/tex120.gif)，其中![](img/tex121.gif)表示扩张，![](img/tex122.gif)表示旋转。网络由卷积层次构成，使用以不同频率为中心的各种小波，以及下一节中讨论的各种非线性。所选择的内核的频率位置选择为在每层较小。整个过程总结在图3.5中。

![](img/x29.png)

图3.5：散射变换网络。在该网络中，[15]中提出的散射变换![](img/tex123.gif)在来自前一层的所有输出![](img/tex125.gif)的每一层![](img/tex124.gif)上重复应用。实质上，每层的输出反复进行相同的变换，然而，变换针对每层的不同有效频率，从而在每一层提取新的信息。在该图中，具有![](img/tex126.gif)层的网络的实例化被描绘为图示。图[15]转载。

一个名为SOE-Net的相关ConvNet被提议用于时空图像分析[60]。 SOE-Net依赖于理论动机，分析定义的过滤器。特别是，它的卷积块依赖于一组3D定向高斯导数滤波器，这些滤波器在遵循类似于ScatNet的频率减小路径时重复应用，如图3.6所示。然而，在这种情况下，网络设计是根据时空定向分析进行的，并且通过所使用的基组的多尺度实例化来实施不变性。

松散地说，SOE-Net和ScatNet都属于某些基于生物学模型所倡导的![](img/tex114.gif)范式[5]。由于这些网络基于严格的数学分析，因此在每层处理时，它们还会考虑信号的频率成分。这种设计的直接结果之一是能够做出关于网络中使用的层数的理论驱动决策。特别地，假设使用频率减小路径计算网络的不同层的输出，则信号最终衰减。因此，一旦信号中剩余的能量很少，就停止迭代。此外，通过选择允许有限基组（高斯导数）的滤波器，SOE-Net可以分析地指定所需的方向数。

![](img/x30.png)

图3.6：SOE-Net架构。使用初始处理层![](img/tex127.gif)提取各种方向的局部时空特征。 ![](img/tex128.gif) - ![](img/tex129.gif) - ![](img/tex130.gif) - ![](img/tex131.gif)表示卷积，整流，归一化和时空汇集，而R和L表示向右和向左滤波数据，分别为符号串（_，例如_ ]。LR）表示多次过滤。仅示出了具有2个滤波器（_，即_ .2方向）的网络用于说明。层![](img/tex127.gif)处的每个特征图被视为新的单独信号并被反馈到层![](img/tex132.gif)以与同一组滤波器卷积，但由于时空池而具有不同的有效分辨率。图[〜]复制。

内核选择过程的另一个简单而强大的前景依赖于使用PCA学习的预先固定的过滤器[21]。在这种方法中，有人认为PCA可以被视为最简单的自动编码器类，可以最大限度地减少重建误差。只需在整个训练数据集上使用PCA学习过滤器。特别地，对于每个图像中的每个像素![](img/tex133.gif)，拍摄尺寸为![](img/tex134.gif)的贴片并进行去义操作以产生一组贴片![](img/tex135.gif)。来自每个图像的这种重叠贴片的集合被堆叠在一起以形成体积![](img/tex136.gif)。使用的滤波器对应于![](img/tex138.gif)的第一个![](img/tex137.gif)主特征向量。将这些载体重新整形以形成大小![](img/tex134.gif)的核![](img/tex139.gif)并与每个输入图像![](img/tex133.gif)卷积以获得特征图![](img/tex140.gif)。对于网络的更高层重复相同的过程。

与ScatNet [15]和SOE-Net [60]相比，PCA方法的工作在数学上要少得多，并且更多地依赖于学习。然而，值得强调的是，最基本的自动编码器形式能够在包括人脸识别，纹理识别和物体识别在内的多个任务上获得可观的结果。一种密切相关的方法也依赖于通过k均值聚类学习的无监督内核选择[35]。再一次，尽管与基于标准学习的体系结构相比，这种方法不会产生最先进的结果，但值得注意的是，即使在像MNIST这样经过深入研究的数据集上，它仍然具有竞争力[91]。更一般地说，这种纯无监督方法的有效性表明存在可以简单地从数据的固有统计数据中利用的非平凡信息。

##### 3.1.2.1最佳内核数量

如前所述，多层体系结构的最大瓶颈是学习过程需要大量的训练数据，这主要是由于需要学习大量参数。因此，仔细设计网络架构并确定每层的内核数量至关重要。不幸的是，即使是手工制作的ConvNets通常也会随机选择内核数量（_，例如_。[15,113,131,79,21,45]）。先前讨论的分析定义的ConvNets中的一个例外是SOE-Net，其如前所述，由于其使用有限基组（_，即_。取向的高斯导数）而分析地指定滤波器的数量。

最近建议使用基组来减少每层内核数量的方法[75,28]提供了解决这个问题的优雅方法，尽管过滤器集的选择和集合中的过滤器数量很大基于经验考虑。解决此问题的其他最突出的方法旨在在培训过程中优化网络架构。处理这种优化问题的简单方法，称为最佳脑损伤[92]，是从合理的架构开始，逐步删除小幅度参数，其删除不会对训练过程产生负面影响。更复杂的方法[44]基于印度自助餐过程[59]。通过训练网络以最小化作为三个目标的组合的损失函数![](img/tex141.gif)来确定最佳滤波器数量

![](img/tex142.gif)（3.1）

其中![](img/tex143.gif)是卷积层的数量，![](img/tex144.gif)是层的总数。在（3.1）中，![](img/tex145.gif)和![](img/tex146.gif)分别是完全连接和卷积层的无监督损失函数。他们的作用是尽量减少重建错误，并使用未标记的数据进行培训。相比之下，![](img/tex147.gif)是为目标任务设计的监督损失函数，并且经过训练以使用标记的训练数据最大化分类准确度。因此，通过最小化重建误差和任务相关的损失函数来调整每层中的滤波器![](img/tex43.gif)的数量。该方法允许所提出的网络使用标记和未标记的数据。

实际上，三种损失函数可以最小化。首先，滤波器参数![](img/tex148.gif)是固定的，并且使用所有可用训练数据（即标记和未标记），使用Grow-And-Prune（GAP）算法学习滤波器![](img/tex149.gif)的数量。其次，通过使用标记的训练数据最小化任务特定的损失函数来更新滤波器参数。 GAP算法可以描述为双向贪心算法。正向传递增加了滤波器的数量。反向传递通过删除冗余过滤器来减小网络大小。

##### Discussion

总的来说，大多数理论驱动的卷积核选择方法旨在将先验引入其层次表示中，最终目标是减少对大规模训练的需求。在这样做时，这些方法或者依赖于通过基于群论的方法最大化不变性，或者依赖于基组上的组合。有趣的是，类似于更具生物学启发的实例化，通常还观察到存在明显倾向于使用具有定向带通滤波器外观的滤波器对早期层进行建模。然而，更高层“内核”的选择仍然是一个开放的关键问题。

### 3.2整改

多层网络通常是高度非线性的，并且整流通常是将非线性引入模型的第一阶段处理。整流是指将点状非线性（也称为激活函数）应用于卷积层的输出。该术语的使用借鉴了信号处理，其中整流是指从交流到直流的转换。这是另一个处理步骤，从生物学和理论点观点中找到动机。计算神经科学家引入整流步骤，以寻找最佳解释手头神经科学数据的适当模型。另一方面，机器学习研究人员使用整改来获得学习更快更好的模型。有趣的是，两个研究流程都倾向于同意，不仅仅是需要纠正，而且它们也趋同于同一类型的整改。

#### 3.2.1生物学观点

从生物学的角度来看，整流非线性通常被引入到神经元的计算模型中，以便解释它们作为输入函数的激发率[31]。生物神经元的射击率一般被广泛接受的模型被称为漏泄积分和火（LIF）[31]。该模型解释了任何神经元的输入信号必须超过某个阈值才能使细胞发射。研究视皮层细胞的研究也特别依赖于类似的模型，称为半波整流[74,109,66]。

值得注意的是，Hubel和Wiesel的开创性工作已经证明，简单单元包括线性滤波后半波整流的非线性处理[74]。如前面3.1节所述，线性算子本身可以被认为是卷积运算。众所周知，根据输入信号，卷积可以产生正或负输出。然而，实际上细胞的“放电速率是定义为正。这就是为什么Hubel和Wiesel建议采用剪切操作形式的非线性，只考虑正反应。更符合LIF模型，其他研究建议略有不同的半波整流，其中削波操作基于某个阈值（_即_。除了零之外）[109]。另一个更完整的模型也考虑了可能出现的负面反应在这种情况下，作者提出了一种双路半波整流方法，其中正负输入信号分别被截断并在两条不同的路径中传输。另外，为了处理负数响应两个信号之后是逐点平方操作，因此整流被称为半平方（虽然生物神经元不一定共享这个属性）。在这个mo del将细胞视为编码正负输出的相反相的能量机制。

##### 讨论

值得注意的是，这些具有生物学动机的神经元激活功能模型已成为当今卷积网络算法的常见做法，并且部分地对其成功的大部分负责，这将在下面讨论。

#### 3.2.2理论观点

从理论的角度来看，机械学习研究人员通常会引入整改，主要有两个原因。首先，它通过允许网络学习更复杂的功能来用于增加提取的特征的区分能力。其次，它允许控制数据的数字表示以便更快地学习。历史上，多层网络依赖于使用逻辑非线性或双曲正切的逐点S形非线性[91]。虽然逻辑函数在生物学上更合理，因为它没有负输出，但更常使用双曲正切，因为它具有更好的学习性质，例如![](img/tex150.gif)周围的稳态（见图3.7（a）和（b），分别）。为了说明双曲正切激活函数的负部分，通常后面是模数运算（也称为绝对值整流AVR）[77]。然而，最近由Nair _等_首次引入的整流线性单元（ReLU）。 [111]，很快成为许多领域的默认整流非线性（_，例如_。[103]），尤其是计算机视觉以来，它首次成功应用于ImageNet数据集[88]。在[88]中显示，与传统的S形整流功能相比，ReLU在过度拟合和加速训练过程中起着关键作用，即使在导致更好的性能的同时也是如此。

数学上，ReLU定义如下，

![](img/tex151.gif)（3.2）

并在图3.7（c）中描述。对于任何基于学习的网络，ReLU运营商有两个主要的理想属性。首先，由于正输入的导数为![](img/tex9.gif)，因此ReLU不会对正输入饱和。这种特性使得ReLU特别具有吸引力，因为它消除了依赖于S形非线性的网络中通常存在的消失梯度的问题。其次，鉴于当输入为负时，ReLU将输出设置为![](img/tex150.gif)，它引入了稀疏性，这有利于更快的训练和更好的分类准确性。实际上，为了改进分类，通常希望具有线性可分的特征，稀疏表示通常更容易分离[54]。然而，负输入的硬![](img/tex150.gif)饱和度具有其自身的风险。这里有两个互补的问题。首先，由于硬零点激活，如果从未激活到这些部分的路径，则网络的某些部分可能永远不会被训练。其次，在退化的情况下，给定层的所有单元都有负输入，反向传播可能会失败，这将导致类似消失梯度问题的情况。由于这些潜在的问题，已经提出了对ReLU非线性的许多改进以更好地处理负输出的情况，同时保持ReLU的优点。

ReLU激活函数的变化包括漏泄整流线性单元（LReLU）[103]及其密切相关的参数整流线性单元（PReLU）[63]，它们在数学上被定义为

![](img/tex152.gif)（3.3）

并在图3.7（d）中描述。在LRelu中![](img/tex153.gif)是固定值，而在PReLU中学习。最初引入LReLU是为了避免反向传播期间的零梯度，但没有显着改善测试网络的结果。此外，它在选择参数![](img/tex153.gif)时严重依赖交叉验证实验。相比之下，PReLU在训练期间优化了该参数的值，从而提高了性能。值得注意的是，PReLU最重要的结果之一是网络中的早期层倾向于学习更高的参数值![](img/tex153.gif)，而对于网络层次结构中的更高层，该数字几乎可以忽略不计。作者推测这个结果可能是由于在不同层次学习过滤器的性质。特别是，由于第一层内核通常是带状滤波器，所以响应的两个部分都保持不变，因为它们代表输入信号的潜在显着差异。另一方面，更高层的内核被调整为检测特定对象并且被训练为更加不变。

| ![](img/x31.png) | ![](img/x32.png) | ![](img/x33.png) |
| （a）物流 | （b）tanh | （c）ReLU |
| ![](img/x34.png) | ![](img/x35.png) | ![](img/x36.png) |
| （d）LReLU / PReLU | （e）SReLU | （f）EReLU |

图3.7：多层网络文献中使用的非线性校正函数。

有趣的是，基于类似的观察[132]提出了另一种整流功能，称为级联整流线性单元（CReLU）。在这种情况下，作者建议CReLU从观察开始，在大多数ConvNets的初始层学习的内核倾向于形成负相关对（_即_。过滤器![](img/tex154.gif)度异相）如图所示在图3.8中。这一观察意味着ReLU非线性消除的负响应被相反相位的学习核所取代。通过用CReLU替换ReLU，作者能够证明设计用于编码双路径校正的网络可以带来更好的性能，同时通过消除冗余来减少要学习的参数数量。

![](img/x37.png)

图3.8：由ImageNet数据集训练的AlexNet学习的Conv1过滤器的可视化。图[132]转载。

ReLU系列的其他变化包括：S形整流线性单元（SReLU）[82]，定义为

![](img/tex155.gif)（3.4）

并在图3.7（e）中描述，其被引入以允许网络学习更多的非线性变换。它由三个分段线性函数和4个可学习参数组成。 SReLU的主要缺点是它引入了几个要学习的参数（_，即_。特别是如果参数不在多个通道之间共享），这使得学习变得更加复杂。考虑到这些参数的错误初始化可能会损害学习，这种担忧尤其如此。另一种变体是指数线性单位（ELU）[26]，定义为

![](img/tex156.gif)（3.5）

并且如图3.7（f）所示，其动机是希望通过迫使信号饱和到由负输入的变量![](img/tex15.gif)控制的值来增加噪声的不变性。 ReLU家族中所有变体的共同点是，也应该考虑负面输入并进行适当处理。

在散射网络[15]中提出了对整流非线性选择的另一种展望。如前面3.1节所述，ScatNet是手工制作的，其主要目标是增加表示对各种变换的不变性。由于它在卷积层中广泛依赖于小波，因此它对于小的变形是不变的;但是，它仍然适用于翻译。因此，作者依赖于定义为的积分运算

![](img/tex157.gif)（3.6）

并实现为平均合并，以增加移位不变性水平。因此，预期随后的汇集操作可以将响应推向零，_即_。在正响应和负响应相互抵消的情况下，![](img/tex158.gif)范数运算符用于整流步骤以使所有响应为正。再一次，值得注意的是，依赖于双曲正切激活函数的传统ConvNets也使用类似的AVR校正来处理负输出[91,77]。此外，更多生物学动机模型，如半平方整流[66,67]，依赖于信号的逐点平方来处理负响应。该平方操作还允许在能量机制方面对响应进行推理。有趣的是，最近一个理论驱动的卷积网络[60]也提出了一个定义为两路径整流策略

（3.7）

其中![](img/tex159.gif)是卷积运算的输出。该整流策略结合了保持滤波信号的两个相位和逐点平方的思想，从而允许在考虑光谱能量方面的结果信号的同时保护信号幅度和相位。

##### Discussion

有趣的是，从理论的角度来看，广泛的ReLU非线性明显成为整流阶段最受欢迎的选择。值得注意的是，完全忽略负输入（_即_。在ReLU中所做的）的选择似乎更值得怀疑，因为提出替代选择的许多贡献证明了这一点[103,63,82,26,132] ]。将ReLU的行为与ScatNet [15]和旧的ConvNet架构[77]中使用的AVR校正进行比较也很重要。虽然AVR保留了能量信息但是擦除了相位信息，但另一方面，ReLU通过仅保留信号的正部分来保持某种意义上的相位信息;然而，它不会保留能量，因为它会丢弃一半的信号。值得注意的是，尝试保留两者的方法（_，例如_ .CReLU [132]和在SOE-Net [60]中使用（3.7））能够在多个任务中获得更好的性能，并且这些方法是也与生物学研究结果一致[66]。

### 3.3规范化

如前所述，由于在这些网络中发生的非线性操作的级联，多层体系结构是高度非线性的。除了上一节中讨论的整流非线性之外，归一化是另一个非线性处理块，它在ConvNet架构中起着重要作用。 ConvNets中使用最广泛的标准化形式是所谓的Divisive Normalization或DN（也称为局部响应标准化）。本节阐述了归一化步骤的作用，并描述了它如何纠正前两个处理块（_，即_。卷积和整流）的一些缺点。再次从生物学和理论的角度讨论规范化的作用。

#### 3.3.1生物学观点

早期由神经生理学家提出标准化来解释视网膜中光适应的现象[13]，后来扩展到解释哺乳动物视皮层中神经元的非线性特性[66]。实际上，从生物学的角度来看，对标准化步骤的需求源于两个主要观察[67,66]。首先，虽然细胞反应被证明是刺激特异性的[74]，但也表明细胞反应可以相互抑制，并且存在交叉定向抑制现象，其中神经元对其优选刺激的反应被减弱。如果它与另一种无效的刺激叠加[67,14,19]。线性模型（卷积步骤中的_即_。）和前一节中讨论的不同形式的整流，例如计算神经科学家提出的半波整流，都没有解释这种交叉定向抑制和抑制行为。 。其次，虽然已知细胞响应在高对比度下饱和，但仅依赖于卷积和无界整流器（例如ReLU）的模型将具有随着对比度增加而不断增加的值。这两个观察结果表明，需要一个折扣其他刺激反应的步骤，以保持每个细胞的特异性，并使其对比不变，同时解释细胞的其他抑制行为。

处理这些问题的一种流行模型包括在数学上描述如下的分裂归一化块

![](img/tex160.gif)（3.8）

其中![](img/tex161.gif)是平方半波整流卷积运算的输出，汇集在一组方向和尺度上![](img/tex162.gif)和![](img/tex163.gif)是一个饱和常数，可以根据两种适应机制中的任何一种来选择[66] ]。在第一种情况下，从细胞的反应历史中学习的每个细胞可能是不同的值。第二种可能性是从所有细胞的反应统计数据中推导出来。这种分裂归一化方案丢弃了关于对比度大小的信息，有利于在归一化操作中，根据输入响应![](img/tex164.gif)的相对对比度对基础图像模式进行编码，（3.8）。使用该模型似乎可以很好地适应哺乳动物视皮层的神经元反应[67]。还表明它也很好地解释了交叉方向抑制现象[14]。

##### 讨论

有趣的是，大多数研究分裂归一化作用的研究表明，包括它的神经元模型很好地符合记录数据（_，例如_。[66,67,14,19]）。事实上，最近的研究表明，分裂正常化也可以解释IT皮层的适应现象，其中神经反应随着刺激重复而降低（_，例如_。[83]）。此外，在皮质的几个区域中建议的分裂正常化的普遍性导致了这样的假设：分裂归一化可以被视为哺乳动物视觉皮层的规范操作，类似于卷积的操作[19]。

#### 3.3.2理论观点

从理论的角度来看，归一化已被解释为在表示自然图像时实现有效编码的方法[102]。在这项工作中，标准化步骤的动机是关于自然图像统计的发现[102]，这些结果已知是高度相关的并且包含非常冗余的信息。根据这些发现，引入了归一化步骤，目的是找到最小化图像中统计依赖性的表示。为了实现这一目标，在[102,101]中彻底讨论的流行推导开始于使用基于高斯尺度混合的统计模型来表示图像。使用该模型和目标函数，其作用是最小化依赖性，非线性以形式导出

![](img/tex165.gif)（3.9）

其中![](img/tex8.gif)和![](img/tex166.gif)分别是输入和输出图像，而![](img/tex167.gif)，![](img/tex153.gif)和![](img/tex168.gif)是可以从训练集中学习的分裂归一化的参数。值得注意的是，在处理冗余时引入的分裂归一化的定义与自然图像中的高阶依赖性之间存在直接关系，（3.9），并且建议最佳地拟合视觉皮层中的神经元响应，（3.8）。特别是，随着我们设置![](img/tex169.gif)的变量的变化，我们看到这两个方程通过元素运算（_即_。平方，与![](img/tex170.gif)相关，受平方根差异的影响。 ]），从而两个模型都达到了在满足神经科学观察的同时最大化独立性的目标。

另一种看待ConvNets标准化的方法是将其视为一种强化特征之间局部竞争的方式[77,91]，类似于生物神经元中发生的竞争。该竞争可以通过减法归一化在特征地图内的相邻特征之间实施，或者通过在特征地图上的相同空间位置处操作的分裂归一化在特征地图之间实施。或者，分裂归一化可被视为一种最小化对乘法对比度变化的敏感性的方法[60]。在更深层次的网络架构中，还发现分裂归一化有助于提高网络的泛化能力[88]。

最近的ConvNets依赖于所谓的批量标准化[129]。批量标准化是另一种分裂标准化，它考虑了一批训练数据来学习标准化参数（_即_。方程式（3.10）中的均值和方差）并且它还引入了新的超参数，![](img/tex171.gif)和![](img/tex172.gif)，以控制每层所需的标准化量。

批量标准化可以分为两个步骤。首先，在具有![](img/tex173.gif)维输入![](img/tex174.gif)的任何层，每个标量特征根据以下内容独立标准化。

![](img/tex175.gif)（3.10）

![](img/tex176.gif)是小批量平均值![](img/tex177.gif)计算的小批量平均值，![](img/tex178.gif)是![](img/tex179.gif)计算的相同小批量的方差。其次，等式（3.10）中归一化的输出经历线性变换，使得所提出的批量归一化块的最终输出由![](img/tex180.gif)给出，其中![](img/tex171.gif)和![](img/tex172.gif)是在参数期间学习的超参数。训练。

批量标准化的第一步旨在确定每层输入的均值和方差。但是，由于该规范化策略可以改变或限制层可以表示的内容，因此包括第二个线性转换步骤以维持网络的表示能力。例如，如果输入处的原始分布已经是最优的，则网络可以通过学习身份映射来恢复它。因此，归一化输入![](img/tex181.gif)可以被认为是在网络的每一层添加的线性块的输入。

批量归一化首先被引入作为对传统分裂归一化的改进，其最终目标是减少内部协变量移位问题，这是指每层输入分布的连续变化[129]。每层输入的变化规模和分布意味着网络必须在每一层显着调整其参数，因此训练必须缓慢（_即_。使用小学习率）以保持损失在训练期间减少（_即_。以避免训练期间的分歧）。因此，引入批量标准化以保证所有输入处的更规则分布。

这种规范化策略的灵感来自为ConvNets的有效培训而建立的一般经验法则。特别是，为了在ConvNets中获得良好的泛化性能，通常的做法是强制所有训练和测试集样本具有相同的分布（_，即_。通过归一化）。例如，已经表明，当输入始终变白时，网络收敛得更快[91,77]。通过考虑每个层可以被视为浅层网络，批量标准化建立在这个想法的基础上。因此，确保输入在每一层保持相同的分布是有利的，并且这通过学习训练数据的分布（使用小批量）并使用训练集的统计来标准化每个输入来强制执行。更一般地说，重要的是要记住，从机器学习的角度来看，这种规范化方案还可以使特征更容易分类。例如，如果两个不同的输入引起两个不同的输出，如果响应位于相同的范围内，则它们更容易被分类器分离，因此处理数据以满足该条件是重要的。

与分裂归一化类似，批量归一化也证明在ConvNets中起着重要作用。特别是，已经证明批量标准化不仅加速了训练，而且在一般化方面起着非常重要的作用，它能够超越以前最先进的图像分类（特别是在ImageNet上）同时消除了对Dropout正则化的需求[88]。

相比之下，批量归一化有点类似于分裂归一化，因为它们都使得每层输入的比例相似。但是，Divisive Normalization通过将每个输入的值除以同一层内同一位置的所有其他输入来标准化每个输入的值。另一方面，批量标准化相对于在相同位置处的训练集的统计（或更准确地说，包含来自整个训练集的示例的小批量的统计）对每个输入进行标准化。批量标准化依赖于训练集的统计的事实可以解释这样的事实，即它提高了表示的泛化能力。

批量标准化的一个问题是它对小批量大小的依赖：如果选择它太小，它可能无法正确表示每次迭代的训练集;或者，如果它太大（_，即_，它会产生减慢训练的负面影响。因为网络必须在当前权重下看到所有训练样本以计算小批量统计数据）。此外，批量标准化不易应用于递归神经网络，因为它依赖于在一小批训练样本上计算的统计数据。因此，在[4]中提出了层标准化。层规范化遵循批量归一化中提出的相同过程，唯一的区别在于规范化统计的计算方式。批量标准化计算小批量的统计数据时，图层标准化使用任何一个图层中的所有要素图或隐藏单位分别计算每个输入的统计数据。因此，在批量标准化中，每个单元使用与该单元相关的不同统计量进行标准化，而层标准以相同方式标准化所有单元。

虽然层次规范被证明在语言相关的应用程序中是有效的，其中循环网络通常更合适，但它无法与使用批量标准化训练的ConvNets竞争图像处理任务[129]。作者提出的一个可能的解释是，在ConvNets中，所有单位在输出中激活单位时没有做出同等贡献;因此，在层标准化（_即_。使用所有单位来计算标准化的统计数据的情况下）的基本假设不适用于ConvNets。

##### Discussion

本小节中讨论的贡献的共同点是，他们都同意标准化在提高多层体系结构的表征能力方面的重要作用。需要注意的另一个重要的一点是，它们都有着共同的目标，即减少输入中的冗余，并且即使在以不同形式提出问题时也将其提高到相同的规模。实际上，虽然早期提出了分裂正常化，_，例如_。 [102]，明确地将问题作为冗余减少问题，诸如批量标准化[129]之类的新提议也通过在每一层白化数据来隐式地强制执行该操作。最后，从生物学角度反思归一化问题，重要的是要注意生物系统在编码自然信号的统计特性方面也是有效的，因为它们代表了具有小代码的世界。因此，人们可能会假设他们也在执行类似的分裂归一化操作，以减少冗余并获得那些有效的代码。

### 3.4汇集

实际上，任何ConvNet模型，无论是生物学启发，纯粹基于学习还是完全手工制作，都包括一个共同的步骤。汇集操作的目标是为位置和比例的变化带来一定程度的不变性，以及聚合特征映射内和跨特征映射的响应。类似于前面部分讨论的ConvNets的三个构建模块，生物学发现以及更多理论驱动的调查也支持汇集。卷积网络中这一层处理的主要争论在于汇集函数的选择。最常见的两种变体是平均和最大池。本节探讨了每种方法的优点和缺点，并讨论了相关文献中描述的其他变化。

#### 3.4.1生物学观点

从生物学的角度来看，汇集主要是由皮质复合细胞的行为驱动[74,109,67,18]。在他们的开创性工作中，Hubel和Wiesel [74]发现，就像简单细胞一样，复杂细胞也被调整到特定方向，但与简单细胞相反，复杂细胞表现出一定程度的位置不变性。他们建议通过某种汇集来实现这一结果，其中调整到相同方向的简单单元的响应在空间和/或时间上汇集，如图3.9所示。

![](img/x38.png)

图3.9：简单和复杂细胞之间差异的说明。该图表明复杂细胞反应是由简单细胞反应的组合产生的。

一些早期生物学启发的卷积网络，如福岛的新知识[49]和原始的LeNet网络[91]依赖于平均汇集。在这些努力中，平均汇集后进行二次抽样主要是由Hubel和Wiesel的调查结果推动的，它用于降低网络对位置变化的敏感度。另一方面，HMAX [117]类网络（_，例如_。[130,110,131,79]）依赖于最大池化。最大汇集策略的支持者声称，当汇集运算符的输入是一组Gabor滤波图像（_，即_，简单单元的典型模型）时，它更合理。实际上，作者认为，当应用于自然图像时，高斯尺度空间（_即_。类似于加权平均合并）揭示了不同尺度的新结构，当应用于自然图像时，它会导致特征逐渐消失。 Gabor滤波图像;见图3.10（a）。另一方面，最大池操作增强了不同尺度的滤波图像中最强的响应，如图3.10（b）所示。

| ![](img/x39.png) |
| （一个） |
| ![](img/x40.png) |
| （b）中 |

图3.10：Gabor滤波图像的平均值与最大池数。该示例示出了当应用于（（a）顶行）原始灰度值图像和（（a）底行）其Gabor滤波版本时在各种尺度下的平均合并的效果。虽然平均合并导致灰度值图像的更平滑版本，但稀疏的Gabor滤波图像逐渐消失。相比之下，该示例还示出了当应用于相同灰度值图像（（b）顶行）和（（b）底行）其Gabor滤波版本时在各种尺度下的最大合并的效果。这里，最大池化导致灰度值图像降级，而Gabor滤波版本中的稀疏边缘被增强。图[131]转载。

复杂单元的行为也可以被视为一种跨通道池，这又是将不变性注入表示的另一种方法。通过组合前一层的各种过滤操作的输出来实现跨通道池化。这个想法是由Mutch和Lowe [110]提出的，作为最突出的生物学启发网络[131]的扩展，之前在3.1节中介绍并在图3.2中说明。特别是，作者在其网络的第二层引入跨通道的最大池，其中S2简单单元的输出跨多个方向汇集，以在每个空间位置保持最大响应单元，如图3.11所示。

![](img/x41.png)

图3.11：跨渠道池图。 （左）由过滤操作产生的密集简单细胞响应，Gabor滤波器调整到不同方向（此处显示了4个方向用于说明目的）（右）使用max运算符（_即交叉通道汇集产生的稀疏简单单元响应）_ 。对于每个像素位置，保持特征图上的最大响应）。图[110]转载。

##### 讨论

总的来说，基于对复杂细胞的描述，似乎从生物学的角度来看，平均和最大汇集都是合理的，尽管有更多的工作主张平均汇集。独立于汇集运算符的选择，事实是对汇集的存在和重要性存在普遍的一致意见。一个可能更重要的问题在于接受场的选择或执行合并的单位。在更多理论驱动的工作中进一步探讨了汇集操作的这一方面，这将在下一节中描述。

#### 3.4.2理论观点

池化已经成为计算机视觉表征管道的一个组成部分，_，例如_。 [99,30,89,49,91]，目的是为图像变换引入一定程度的不变性，并对噪声和杂波具有更好的鲁棒性。从理论的角度来看，可能是影响汇集的重要性和作用的最有影响力的作品之一是Koendrink的局部无序图像概念[87]。这项工作有利于汇集在感兴趣区域（ROI），_，即_内的像素的确切位置。即使在保留全局图像结构的同时，也可以忽略池化区域。目前，几乎所有卷积体系结构都包含池化块作为其处理阶段的一部分。与生物学动机模型一样，更多理论驱动的方法通常采用平均或最大池化。

最近的工作从纯粹基于理论的角度来看待他们的网络设计_，例如_。 ScatNet [15]和SOE-Net [60]依赖于一种平均汇集形式。特别是，他们的网络依赖于加权和池操作。这些方法从频域的角度来解决汇集问题;因此，他们对平均合并的选择是由追踪信号频率内容的愿望所驱动的。平均池化允许这些网络在每层上作用于不同频率，同时对图像进行下采样以增加不变性并减少冗余。同时，他们指定池参数的受控方法允许它们在池操作期间避免混叠。值得注意的是，在SOE-Net的调查中，加权平均汇集的优势在经验上证明了简单的车厢合并和最大汇集。

有趣的是，大多数早期的卷积结构也依赖于平均汇集，_，例如_。 [49,91]，但它在许多基于学习的卷积体系结构中慢慢失宠，并被最大池化所取代。这种趋势主要是由于业绩的微小差异所致。但是，汇集在网络中的作用很重要，需要更仔细地考虑。事实上，早期工作探索汇集[77]的作用表明，汇集的类型在ConvNet架构中起着如此重要的作用，即使是随机初始化的网络也会在对象识别任务中产生竞争结果，提供了适当的汇集类型。用来。特别是，这项工作比较了平均和最大池，并证明了随机初始化的网络平均汇集产生了卓越的性能。

其他工作更系统地比较了平均和最大池的经验[128]并且表明两种类型的池之间存在互补性，这取决于输入类型和它经历的变换。因此，这项工作意味着ConvNets可以从整个架构中使用多个池选项中受益。然而，其他工作从纯粹的理论角度考虑了这个问题[12]。具体来说，这项工作检查了平均与最大池对提取特征的可分离性的影响。本文的主要结论可归纳为两点。首先，作者认为当汇集特征非常稀疏时，最大池更合适（_，例如_。当汇集前面有ReLU时）。其次，作者建议汇集基数应随输入大小而增加，并且池基数会影响汇集功能。更一般地说，已经表明，除了池化类型之外，池化大小也起着重要作用。

尽管经验上[81,27]，其他各种研究也探讨了汇集基数的重要性。实际上，汇集基数的作用首先在早期手工制作的特征提取管道[81]的背景下进行了讨论。特别是，这项工作建立在空间金字塔池[89]编码方法的基础上，同时突出了使用预定的固定大小池网格的缺点。作者建议学习合并窗口“大小作为分类器训练的一部分。更具体地说，作者建议随机挑选不同基数的不同合并区域，并训练分类器选择产生最高准确度的合并区域。这背后的主要动机基于学习的策略是使池化适应于数据集。例如，室外场景的最佳池区可能位于地平线上，这不一定适用于室内场景。同样，对于视频动作识别，它更适合于适应汇集区域到视频最显着的部分[39]。汇集窗口大小或基数的作用也直接在神经网络环境中进行探索[27]。这里，作者建议最相似的特征应该是作者建议以无人监督的方式微调其网络的汇集支持（_即_。汇集区域）。具体地，选择合并窗口以根据成对相似性矩阵将相似特征组合在一起，其中相似性度量是平方相关性。除了平均和最大池操作之外，这些调查的共同点是独立于池功能的池区域的重要性。

从纯机器学习的角度来看，其他工作涉及池的选择及其相应的参数[153,58,96]。从这个角度来看，池化被提倡作为一种正规化技术，允许在训练期间改变网络的结构。特别地，由于在训练期间反向传播的信号可能采取的路径的变化，因此汇集允许在大型架构内创建子模型。这些变化是通过诸如随机汇集[153]或在最大网络[58]和网络中的网络（NiN）[96]中使用的跨渠道汇集等方法实现的。 NiN最初是作为一种处理过度拟合和纠正ConvNets过度完整表示的方法而引入的[96]。特别是，由于每层使用了大量内核，因此注意到许多网络经常在训练后学习冗余过滤器。因此，引入NiN以通过训练网络来减少每层的冗余以了解使用加权线性组合来组合哪些特征图。与NiN类似，Maxout网络[58]引入了跨信道池，其中输出被设置为基于信道的![](img/tex182.gif)特征映射的最大值。值得注意的是，最近的一项提案还依赖于跨渠道共享来尽量减少冗余[60]，即使在完全免费学习的情况下也是如此。在这项工作中，网络基于过滤器的固定词汇表，并且跨渠道池用于将使用相同内核的过滤操作产生的特征映射组合在一起。除了最大限度地减少冗余之外，采用这种方法可以使网络规模保持可管理性，同时保持可解释性。

随机汇集（SP）[153]也被引入作为正则化技术。但是，与执行跨通道池的maxout和NiN不同，SP在特征映射中起作用。特别是，随机汇集的灵感来自广泛用于完全连接层的丢失技术，但SP应用于卷积层。它依赖于向集合操作引入随机性，该集合操作迫使反向传播的信号在训练期间在每次迭代时随机地采用不同的路径。该方法首先将要合并的每个区域内的特征图响应![](img/tex153.gif)标准化，![](img/tex183.gif)，

![](img/tex184.gif)（3.11）

然后将归一化值![](img/tex185.gif)用作多项分布的概率，该多项分布又用于对要合并的区域内的位置![](img/tex112.gif)进行采样。相应的激活![](img/tex186.gif)是合并的值。重要的是，尽管随机汇集依赖于从任何区域选择一个值![](img/tex183.gif)（_，即_。类似于最大汇集），汇总值不一定是![](img/tex183.gif)中最大的值。在此，重要的是要注意在测试期间采用不同的池策略。在测试时，概率不再用于在合并期间对位置进行采样;相反，它们被用作加权和池操作的权重。因此，随机汇集在精神上与训练期间的最大汇集更接近，并且在测试期间更接近于平均汇集。作者认为，在训练期间采用的汇集策略允许通过不同的途径创建不同的模型，而在测试期间使用的汇集允许创建在训练期间看到的所有可能模型的粗略平均近似值。总之，随机池可以看作是尝试平均和最大池的最佳结果。

尝试在平均池和最大池之间实现平衡的另一种方法建议让网络学习最佳池化方法[95]。多个汇集策略的这种想法是通过实验证明最佳汇集策略的选择受输入影响[128]。特别是，作者提出了三种不同的方法来结合平均和最大汇集的好处，即：混合，门控和树池。混合池结合了平均和最大池，独立于要汇集的区域，在那里训练网络以根据混合比例学习混合比例。

![](img/tex187.gif)（3.12）

受约束条件![](img/tex188.gif)。在门控最大平均汇集中，混合比例适合于要汇集的区域。特别地，训练网络以学习通过逐点乘法应用于输入数据的选通掩码![](img/tex189.gif)。使用这个选通掩模，混合功能现在定义为

![](img/tex190.gif)（3.13）

用![](img/tex191.gif)。

在这项工作中提出的第三个汇集策略是树池，它可以被视为门控池的极端版本​​。在树池中，不仅学习了混合比例，而且还学习了要组合的池化功能。具体地，采用树结构来学习各个函数的参数及其混合策略。三种池化方法的区别如图3.12所示。总之，这些提案背后的主要思想是让汇集战略适应所汇集的地区。遵循这一策略，作者能够证明不仅结合平均和最大汇集的价值，而且还证明了汇集函数适应汇总区域的价值。

![](img/x42.png)

图3.12：混合，门控和树池。所描述的（a）混合最大​​平均合并，（b）门控最大平均合并和（c）树合并的图示。图[〜]复制。

最后，在本节中值得一提的是最后一种类型的池，称为全局池。在一些着名的ConvNet模型中使用了全局池，以解决与ConvNet架构设计相关的更多实际问题[62,96]。例如，众所周知，标准ConvNets依靠卷积层进行特征学习/提取，完全连接的层依次使用softmax进行分类。然而，完全连接的层需要使用大量参数，因此易于过度拟合。引入了许多方法来处理由完全连接的层引起的过度拟合，也许最广泛使用的是丢失[88]。然而，在NiN [96]中引入了一种更自然地融入卷积框架的更优雅的方法，它被称为全球平均汇集。它只依赖于在整个功能映射支持中聚合最后一层功能。在所谓的SPP-Net [62]中也发现了依赖全局汇集的另一个例子。在这项工作中，空间金字塔池（SPP）[89]用于使卷积网络接受任何大小的输入图像。实际上，由于使用完全连接的层，ConvNets需要固定大小的输入。 SPP-Net在最后一个卷积层之后引入空间金字塔池以纠正这个难度。特别是，空间金字塔池用于生成独立于输入图像大小的固定大小表示，如图3.13所示。值得注意的是，NiN中使用的全局平均池，类似于在ConvNet的最后一层执行空间金字塔池，其中金字塔仅由最粗糙的层组成。

![](img/x43.png)

图3.13：空间金字塔池网络。 SPP应用于网络的最后卷积层的特征映射。由于空间区间与图像大小成比例，因此SPP生成与输入图像大小无关的相同大小的特征向量。因此，SPP-Net不要求对输入图像进行预处理，使得它们具有相同的尺寸。图[62]转载。

##### Discussion

传统上，池中使用的默认函数依赖于average或max运算符。然而，一些调查显示两者之间存在某种互补关系，表明在选择汇集操作时应考虑更多参数。由于这些观察，最近的研究一直在推动将培训的理念扩展到包括学习汇集功能及其参数。然而，该方向需要增加要学习的参数的数量，从而增加过度拟合的机会。重要的是，这种方法应谨慎对待，因为它可能会进一步模糊我们对所学习的表示的了解和理解。作为补充，可以在理论基础上指定汇集参数，以用于先前处理阶段具有足够分析细节的情况。总体而言，汇集应被视为一种将多个功能的信息汇总为紧凑形式的方法，该形式在丢弃细节的同时保留信号的重要方面。除了决定如何总结这些功能之外，很明显，更难的问题是确定应该汇集的数据是什么以及数据存在的位置。

### 3.5总体讨论

本章讨论了典型ConvNet架构中使用最广泛的构建块的作用和重要性，以便了解ConvNets的工作原理。特别是，每个块的细节都是从生物学和理论的角度来解决的。总的来说，各种共同的线索来自对所讨论的构建块的探索。特别是，似乎所有区块都从视觉皮层中发生的操作中找到了相对强烈的动机。此外，虽然所有块在ConvNets中都发挥了重要作用，但似乎卷积核的选择是最重要的方面，正如解决这一块的大量文献所证明的那样。更重要的是，似乎本章中讨论的更新的ConvNet架构（_，例如_。[15,75,28,148,60]）旨在通过结合更多控制来最小化对基于重训练的解决方案的需求。在他们的网络的各个阶段构建块。这些最近的方法反过来通过各种努力推动，这些努力通过分层可视化和消融研究揭示了基于学习的ConvNets（_，例如_。在一些广泛使用的学习型ConvNets中的主要冗余）的次优性，将在下一章讨论。