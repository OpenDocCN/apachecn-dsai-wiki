<!--yml
category: 面试
date: 2022-07-01 00:00:00
-->

# BAT 机器学习 1000 题 300-400

## 301.在以下不同的场景中,使用的分析方法不正确的有 （B）

A.根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级

B.根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式

C.用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫

D.根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女


## 302.什么是梯度爆炸？

【解析】误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。

在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。

网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。





## 303.梯度爆炸会引发什么问题？ 

【解析】在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。

梯度爆炸导致学习过程不稳定。—《深度学习》，2016.

在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。





## 304.如何确定是否出现梯度爆炸？

【解析】训练过程中出现梯度爆炸会伴随一些细微的信号，如：

模型无法从训练数据中获得更新（如低损失）。

模型不稳定，导致更新过程中的损失出现显著变化。

训练过程中，模型损失变成 NaN。

如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。

以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。

训练过程中模型梯度快速变大。

训练过程中模型权重变成 NaN 值。

训练过程中，每个节点和层的误差梯度值持续超过 1.0。





## 305.如何修复梯度爆炸问题？

【解析】有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。

1. 重新设计网络模型

在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。

使用更小的批尺寸对网络训练也有好处。

在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

2. 使用 ReLU 激活函数

在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。

使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

3. 使用长短期记忆网络

在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。

使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。

采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。

4. 使用梯度截断（Gradient Clipping）

在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。

处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。

——《Neural Network Methods in Natural Language Processing》，2017.

具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。

梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。

——《深度学习》，2016.

在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。

默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。

5. 使用权重正则化（Weight Regularization）

如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。

对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。

——On the difficulty of training recurrent neural networks，2013.

在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。

## 306. LSTM神经网络输入输出究竟是怎样的？

@YJango，本题解析来源：https://www.zhihu.com/question/41949741

Recurrent Layers——介绍（https://zhuanlan.zhihu.com/p/24720659?refer=YJango）

*   第一要明确的是神经网络所处理的单位全部都是：向量

下面就解释为什么你会看到训练数据会是矩阵和张量

*   常规feedforward 输入和输出：矩阵

    输入矩阵形状：(n_samples, dim_input)

    输出矩阵形状：(n_samples, dim_output)

注：真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。 如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。

**Feedforward 的输入输出的本质都是单个向量。**

*   常规Recurrent (RNN/LSTM/GRU) 输入和输出：张量

    输入张量形状：(time_steps, n_samples,  dim_input)

    输出张量形状：(time_steps, n_samples,  dim_output)

注：同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。 

**Recurrent 的任意时刻的输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。所以你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。**

python代码表示预测的话：


```py
import numpy as np 
#当前所累积的hidden_state,若是最初的vector，则hidden_state全为0
hidden_state=np.zeros((n_samples, dim_input))
#print(inputs.shape)：（time_steps, n_samples, dim_input)
outputs = np.zeros((time_steps, n_samples, dim_output))

for i in range(time_steps): 
    #输出当前时刻的output，同时更新当前已累积的hidden_state outputs[i],
    hidden_state = RNN.predict(inputs[i],hidden_state)
#print(outputs.shape)：(time_steps, n_samples, dim_output)
```

但需要注意的是，Recurrent nets的输出也可以是矩阵，而非三维张量，取决于你如何设计。

1.  若想用一串序列去预测另一串序列，那么输入输出都是张量 (例如语音识别或机器翻译 一个中文句子翻译成英文句子（一个单词算作一个向量），机器翻译还是个特例，因为两个序列的长短可能不同，要用到seq2seq；

2.  若想用一串序列去预测一个值，那么输入是张量，输出是矩阵 （例如，情感分析就是用一串单词组成的句子去预测说话人的心情）


**Feedforward 能做的是向量对向量的one-to-one mapping，

Recurrent 将其扩展到了序列对序列 sequence-to-sequence mapping.**

但单个向量也可以视为长度为1的序列。所以有下图几种类型：![null](http://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8BqFicicy8mwvMgXDW4odJlXgibJ5cLy3DxLs0SBdsbdGicoib0D0HTib8zorfFSRibRw0o6hUE0Fa9xHlg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

除了最左侧的one to one是feedforward 能做的，右侧都是Recurrent所扩展的


若还想知道更多

*   可以将Recurrent的横向操作视为累积已发生的事情，并且LSTM的memory cell机制会选择记忆或者忘记所累积的信息来预测某个时刻的输出。

*   以概率的视角理解的话：就是不断的conditioning on已发生的事情，以此不断缩小sample space

*   RNN的思想是: current output不仅仅取决于current input，还取决于previous state；可以理解成current output是由current input和previous hidden state两个输入计算而出的。并且每次计算后都会有信息残留于previous hidden state中供下一次计算


## 307.以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？

A.PDF描述的是连续型随机变量在特定取值区间的概率

B.CDF是PDF在特定区间上的积分

C.PMF描述的是离散型随机变量在特定取值点的概率

D.有一个分布的CDF函数H(x),则H(a)等于P(X<=a)

正确答案：A

解析：

概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。

概率密度函数（p robability density function，PDF ）是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。

累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。

## 308.线性回归的基本假设有哪些？(ABDE)

A.随机误差项是一个期望值为0的随机变量；

B.对于解释变量的所有观测值，随机误差项有相同的方差；

C.随机误差项彼此相关；

D.解释变量是确定性变量不是随机变量，与随机误差项之间相互独立；

E.随机误差项服从正态分布

## 309.处理类别型特征时，事先不知道分类变量在测试集中的分布。要将 one-hot encoding（独热码）应用到类别型特征中。那么在训练集中将独热码应用到分类变量可能要面临的困难是什么？

A. 分类变量所有的类别没有全部出现在测试集中

B. 类别的频率分布在训练集和测试集是不同的

C. 训练集和测试集通常会有一样的分布

答案为：A、B ，如果类别在测试集中出现，但没有在训练集中出现，独热码将不能进行类别编码，这是主要困难。如果训练集和测试集的频率分布不相同，我们需要多加小心。

## 310.假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？

A. ReLU

B. tanh

C. SIGMOID

D. 以上都不是

答案为：B，该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。

## 311、下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？

A. 类型 1 通常称之为假正类，类型 2 通常称之为假负类。

B. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。

C. 类型 1 错误通常在其是正确的情况下拒绝假设而出现。

答案为(A)和(C)：在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。

## 312、在下面的图像中，哪一个是多元共线（multi-collinear）特征？

![null](https://mmbiz.qpic.cn/mmbiz_png/4CIr9b8XYa2R4TIC9Rg8ichAMIxgqlxrYcbyGjvKGQSDaHuibP2hswmBK8OVXZnYK8RCwTUmbku900g1yeS2N9qw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A. 图 1 中的特征

B. 图 2 中的特征

C. 图 3 中的特征

D. 图 1、2 中的特征

E. 图 2、3 中的特征

F. 图 1、3 中的特征

答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

## 313、鉴别了多元共线特征。那么下一步可能的操作是什么？

A. 移除两个共线变量

B. 不移除两个变量，而是移除一个

C. 移除相关变量可能会导致信息损失，可以使用带罚项的回归模型（如 ridge 或 lasso regression）。

答案为（B）和（C）：因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。

## 314、给线性回归模型添加一个不重要的特征可能会造成？

A. 增加 R-square

B. 减少 R-square

答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。

## 315、假定目标变量的类别非常不平衡，即主要类别占据了训练数据的 99%。现在你的模型在测试集上表现为 99% 的准确度。那么下面哪一项表述是正确的？

A. 准确度并不适合于衡量不平衡类别问题

B. 准确度适合于衡量不平衡类别问题

C. 精确率和召回率适合于衡量不平衡类别问题

D. 精确率和召回率不适合于衡量不平衡类别问题

答案为（A）和（C）

## 316、什么是偏差与方差？

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

偏差：![null](http://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibkhBMz8tz3xYHdem1dAOM0PEoho4IHiarZbdJLMiaSicD8YF60Fmwcr93Vzj6ejF8HgA9lvLeWydulA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

方差：![null](http://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibkhBMz8tz3xYHdem1dAOM00tRR4t5ibWv9OWib3SeQ0wbgGTHvJuxWp4K6dnZmhcDmVtr3SUpEZIAA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 317、解决bias和Variance问题的方法是什么？

交叉验证

High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征

High Variance解决方案：agging、简化模型、降维

## 318.采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？

用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

## 319、xgboost怎么给特征评分？

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。

```py
# feature importance
print(model.feature_importances_)
# plot  pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)
pyplot.show() 
# plot feature importance 
plot_importance(model)
pyplot.show() 
```

Python是最好的语言 ——鲁迅


向所有的程序员致敬

![null](http://mmbiz.qpic.cn/mmbiz_jpg/lcaq0oMjdFxMDDQp9WEicgnib1r7fE5AbjQlic4sHMp2TqibAfss8yLdyAeLMs8TxSpbbSpAQJX5qp8NdaeReOwibLQ/?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 320、什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？


bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据oob（out of bag）,它可以用于取代测试集误差估计方法。

袋外数据(oob)误差的计算方法如下：

对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。

## 321、假设张三的mp3里有1000首歌，现在希望设计一种随机算法来随机播放。与普通随机模式不同的是，张三希望每首歌被随机到的概率是与一首歌的豆瓣评分（0~10分）成正比的，如朴树的《平凡之路》评分为8.9分，逃跑计划的《夜空中最亮的星》评分为9.5分，则希望听《平凡之路》的概率与《夜空中最亮的星》的概率比为89:95。现在我们已知这1000首歌的豆瓣评分：

（1）请设计一种随机算法来满足张三的需求。

（2）写代码实现自己的算法。

```c
#include <iostream>
#include <time.h>
#include <stdlib.h>


using namespace std;


int findIdx(double songs[],int n,double rnd){
    int left=0;
    int right=n-1;
    int mid;
    while(left<=right){
        mid=(left+right)/2;
        if((songs[mid-1]<=rnd) && (songs[mid]>=rnd))
            return mid;
        if(songs[mid]>rnd)
            right=mid-1;
        else
            left=mid+1;
    }
//    return mid;
}


int randomPlaySong(double sum_scores[],int n){
    double mx=sum_scores[n-1];
    double rnd= rand()*mx/(double)(RAND_MAX);
    return findIdx(sum_scores,n,rnd);
}


int main()
{
    srand(time(0));
    double scores[]={5.5,6.5,4.5,8.5,9.5,7.5,3.5,5.0,8.0,2.0};
    int n=sizeof(scores)/sizeof(scores[0]);
    double sum_scores[n];
    sum_scores[0]=scores[0];


    for(int i=1;i<n;i++)
        sum_scores[i]=sum_scores[i-1]+scores[i];


    cout<<"Calculate the probability of each song: "<<endl;
    int totalScore=sum_scores[n-1];
    for(int i=0;i<n;i++)
        cout<<scores[i]/totalScore<<" ";
    cout<<endl;


    int counts[n];
    for(int i=0;i<n;i++)
        counts[i]=0;


    int i=0;
    int idx;
    int MAX_ITER=100000000;
    while(i<MAX_ITER){
        idx=randomPlaySong(sum_scores,n);
        counts[idx]++;
        i++;
    }


    cout<<"After simulation, probability of each song: "<<endl;
    for(int i=0;i<n;i++)
        cout<<1.0*counts[i]/MAX_ITER<<" ";
    cout<<endl;


    return 0;
}
```

## 322.对于logistic regession问题：prob（t|x）=1/（1+exp（w*x+b））且label y=0或1，请给出loss function和权重w的更新公式及推导。

Logistic regression 的loss function 是log loss, 公式表达为：

![null](http://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS91gdibETiapMMzlyn0OEQMVzibzB3bEceicmEjG3lXCtwQ2ds4U8LCpcvw45jamUZDhdjyCgLFJHg0cw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

w的更新公式可以由最小化loss function得到，即：

![null](http://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS91gdibETiapMMzlyn0OEQMVzrtDxKiaugOHq0LOcosatRvAwPa1ZX2EgdRicIWmCyvJWo8mRAFrSGo1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中大括号里面的部分，等价于逻辑回归模型的对数似然函数，所以也可以用极大似然函数方法求解，

根据梯度下降法，其更新公式为：

![null](http://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS91gdibETiapMMzlyn0OEQMVz8gOb0H11DD73xzA5384C05dicbONeNSLZOpHB6exSzlrCX5rcZzasWw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 323.决策树的父节点和子节点的熵的大小关系是什么？

A. 决策树的父节点更大

B. 子节点的熵更大

C. 两者相等

D. 根据具体情况而定


正确答案：B。在特征选择时，应该给父节点信息增益最大的节点，而信息增益的计算为 IG(Y|X) = H(Y) - H(Y/X)，H(Y/X) 为该特征节点的条件熵， H(Y/X)    越小，即该特征节点的属性对整体的信息表示越“单纯”，IG更大。 则该属性可以更好的分类。H(Y/X) 越大，属性越“紊乱”，IG越小，不适合作为分类属性。

## 324.欠拟合和过拟合的原因分别有哪些？如何避免？

欠拟合的原因：模型复杂度过低，不能很好的拟合所有的数据，训练误差大；


避免欠拟合：增加模型复杂度，如采用高阶模型（预测）或者引入更多特征（分类）等。

过拟合的原因：模型复杂度过高，训练数据过少，训练误差小，测试误差大；

避免过拟合：降低模型复杂度，如加上正则惩罚项，如L1，L2，增加训练数据等。

## 325.语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用：

A. 平滑

B. 去噪

C. 随机插值

D. 增加白噪音

正确答案：A

## 326.下面关于Hive的说法正确的是( )

A. Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文本映射为一张数据库表，并提供简单的SQL查询功能

B. Hive可以直接使用SQL语句进行相关操作

C. Hive能够在大规模数据集上实现低延迟快速的查询

D. Hivez在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下

正确答案：A

Hive使用类sql语句进行相关操作，称为HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。

Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询，例如，Hive在几百MB 的数据集上执行查询一般有分钟级的时间延迟。

Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop  MapReduce 的作业执行模型，Hive 将用户的HiveQL    语句通过解释器转换为MapReduce 作业提交到Hadoop  集群上，Hadoop  监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive    并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。


## 327.关于input split和block的描述正确的是( )

A. Mapreduce 的input split就是一个block

B. input split是一种记录的逻辑划分,而block是对输入数据的物理分割,两者之间有着本质的区别

C. 由于Block是本地的,DFSCline可以不用向DataNode建立连接,直接读磁盘上的文件

D. 为了发挥计算本地化性能,应该尽量使inputSplit大小与block大小相当

正确答案：B

1. 一个split不会包含零点几或者几点几个Block，一定是包含大于等于1个整数个Block 

2. 一个split不会包含两个File的Block,不会跨越File边界 

3. split和Block的关系是一对多的关系 

4. maptasks的个数最终决定于splits的长度

## 328.推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到。

根据贝叶斯公式P(c|d)=（P(c)P(d|c)/P(d)）。

这里，分母P(d)不必计算，因为对于每个类都是相等的。 分子中，P(c)是每个类别的先验概率，可以从训练集直接统计，

P(d|c)根据独立性假设，可以写成如下 P(d|c)=￥P(wi|c)（￥符号表示对d中每个词i在c类下概率的连乘），

P(wi|c)也可以从训练集直接统计得到。 至此，对未知类别的d进行分类时，类别为c=argmaxP(c)￥P(wi|c)。

## 329.逻辑回归与多元回归分析有哪些不同？

A. 逻辑回归预测某事件发生的概率

B. 逻辑回归有较高的拟合效果

C. 逻辑回归回归系数的评估

D. 以上全选

答案：D

逻辑回归是用于分类问题，我们能计算出一个事件/样本的概率；一般来说，逻辑回归对测试数据有着较好的拟合效果；建立逻辑回归模型后，我们可以观察回归系数类标签(正类和负类)与独立变量的的关系。

## 330."过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：

A. 正确

B. 错误

答案：B

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数.

## 331题

哪些机器学习算法不需要做归一化处理？

**解析**：

概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。

至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。

引用自@管博士一般我习惯说树形模型，这里说的概率模型可能是差不多的意思。引用自@寒小阳

## 332题

对于树形结构为什么不需要归一化？

**解析**：

数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。

另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

## 333题

在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别

**解析：**

欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8G78B1MhiciciabicvlBRhyTG3CX6hKwpuEVhDZz0aKTXrHTUibVyGbdXxDXq7Mo4AWtcUAT1ibcpdydWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。

曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8G78B1MhiciciabicvlBRhyTG3mShFdqCfB1W2zTXgbOQK7kWG3Qa9QFOAwPD9U4NZwDtfzwRALxLGlg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。

通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

曼哈顿距离和欧式距离一般用途不同，无相互替代性。

另，关于各种距离的比较参看《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。

## 334题

数据归一化（或者标准化，注意归一化和标准化不同）的原因

**解析：**

要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。

有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。

有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。

本题解析来源：@我愛大泡泡，链接：http://blog.csdn.net/woaidapaopao/article/details/77806273

补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

## 335题

请简要说说一个完整机器学习项目的流程

**解析：**

**1、抽象成数学问题**

明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。

这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

**2、获取数据

**数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。

数据要有代表性，否则必然会过拟合。

而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。

而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

**3、特征预处理与特征选择**

良好的数据要能够提取出良好的特征才能真正发挥效力。

特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

**4、训练模型与调优**

直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

**5、模型诊断**

如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。

过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……

诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

**6、模型融合**

一般来说，模型融合后都能使得效果有一定提升。而且效果很好。

工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

**7、上线运行**

这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。

故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。

引用自：@寒小阳、@龙心尘

## 336题

逻辑斯特回归为什么要对特征进行离散化。

**解析：**

在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

0. 离散特征的增加和减少都很容易，易于模型的快速迭代；

1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；

4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；

6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

本题解析来源：@严林，链接：https://www.zhihu.com/question/31989952

## 337题

简单介绍下LR

**解析：**

@rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。

原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。

虽然逻辑斯蒂回归姓回归，不过其实它的真实身份是二分类器。先弄清楚一个概念：线性分类器。

给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。

如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（ wT中的T代表转置）： 

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjibIQqVeUmRd78tdtHXuMVu4ZQMvqH4g3NGwB1RueVafNAQCuibkpH6NQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

假设函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjutLMwQ1YSw4Shpv43Pp2MianxX9nehu0k2Hycu6xmoRmWW9MUBpwW7A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中x是n维特征向量，函数g就是logistic函数。

而

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjTN2CWKZbjFuYPv8Pibnd7bAyeEicRzCHNjIOqicfvp1z0xhoqFuFUt7Vg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

的图像是

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIj079rknOoYiaSibgLeM9ojp9SLphVwmiczBaZbhlr3B9Qc1sxpTofGXcicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjEr87nXZBoArBQhcMOXO4Lt33Y3Va6zKc7x327k5nfgoTVricx1lEBeA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从而，当我们要判别一个新来的特征属于哪个类时，只需求即可，若大于0.5就是y=1的类，反之属于y=0类。

更多可以参考下这几篇文章：

1 Logistic Regression 的前世今生（理论篇）：http://blog.csdn.net/cyh_24/article/details/50359055

2 机器学习算法与Python实践之（七）逻辑回归：http://blog.csdn.net/zouxy09/article/details/20319673

## 338题

overfitting怎么解决

**解析：**

overfitting就是过拟合, 其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集, 对训练集外的数据却不work, 这称之为泛化(generalization)性能不好。泛化性能是训练的效果评价中的首要目标，没有良好的泛化，就等于南辕北辙, 一切都是无用功。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjozwAT2WP7znWpZjicicbUUZxCAJdVoJ4dwHBJOE2D6po3H7PaLqkG2QQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

过拟合是泛化的反面，好比乡下快活的刘姥姥进了大观园会各种不适应，但受过良好教育的林黛玉进贾府就不会大惊小怪。实际训练中, 降低过拟合的办法一般如下：

**正则化(Regularization)**

L2正则化：目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候, 拟合函数需要顾忌每一个点, 最终形成的拟合函数波动很大, 在某些很小的区间里, 函数值的变化很剧烈, 也就是某些w非常大. 为此, L2正则化的加入就惩罚了权重变大的趋势.

L1正则化：目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

**随机失活(dropout)**

在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0), 每个w因此随机参与, 使得任意w都不是不可或缺的, 效果类似于数量巨大的模型集成。

**逐层归一化(batch normalization)**

这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层), 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全, 因而泛化效果非常好.

**提前终止(early stopping)**

理论上可能的局部极小值数量随参数的数量呈指数增长, 到达某个精确的最小值是不良泛化的一个来源. 实践表明, 追求细粒度极小值具有较高的泛化误差。

这是直观的，因为我们通常会希望我们的误差函数是平滑的, 精确的最小值处所见相应误差曲面具有高度不规则性, 而我们的泛化要求减少精确度去获得平滑最小值, 所以很多训练方法都提出了提前终止策略. 

典型的方法是根据交叉叉验证提前终止: 若每次训练前, 将训练数据划分为若干份, 取一份为测试集, 其他为训练集, 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集, 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好, 这时候训练错误率虽然还在继续下降, 但也得终止继续训练了.

@AntZ

![null](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。

通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

曼哈顿距离和欧式距离一般用途不同，无相互替代性。

另，关于各种距离的比较参看《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。

## 339题

LR和SVM的联系与区别

**解析：**

**联系：**

1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）

2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。


**区别：**

1、LR是参数模型，SVM是非参数模型。

2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。

3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

本题解析来源：@朝阳在望http://blog.csdn.net/timcompp/article/details/62237986

## 340题

什么是熵

**解析：**

从名字上来看，熵给人一种很玄乎，不知道是啥的感觉。其实，熵的定义很简单，即用来表示随机变量的不确定性。之所以给人玄乎的感觉，大概是因为为何要取这样的名字，以及怎么用。

熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。

**熵的引入**

事实上，熵的英文原文为entropy，最初由德国物理学家鲁道夫·克劳修斯提出，其表达式为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8es1INialNyibAJqfkiaQibjIjNT4P4qQVE0xMaTZkN3xavUy8qOqImywkNjbw2BsrEW7CTtEG9z6D5w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

它表示一个系系统在不受外部干扰时，其内部最稳定的状态。后来一中国学者翻译entropy时，考虑到entropy是能量Q跟温度T的商，且跟火有关，便把entropy形象的翻译成“熵”。

我们知道，任何粒子的常态都是随机运动，也就是"无序运动"，如果让粒子呈现"有序化"，必须耗费能量。所以，温度（热能）可以被看作"有序化"的一种度量，而"熵"可以看作是"无序化"的度量。

如果没有外部能量输入，封闭系统趋向越来越混乱（熵越来越大）。比如，如果房间无人打扫，不可能越来越干净（有序化），只可能越来越乱（无序化）。而要让一个系统变得更有序，必须有外部能量的输入。

1948年，香农Claude E. Shannon引入信息（熵），将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。

更多请查看《最大熵模型中的数学推导》（链接：http://blog.csdn.net/v_july_v/article/details/40508465）。

## 341题

说说梯度下降法

**解析：**

1、什么是梯度下降法
经常在机器学习中的优化问题中看到一个算法，即梯度下降法，那到底什么是梯度下降法呢？

维基百科给出的定义是梯度下降法（Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

额，问题又来了，什么是梯度？为了避免各种复杂的说辞，咱们可以这样简单理解，在单变量的实值函数的情况，梯度就是导数，或者，对于一个线性函数，也就是线的斜率。

1.1 梯度下降法示例

举个形象的例子吧，比如当我们要做一个房屋价值的评估系统，那都有哪些因素决定或影响房屋的价值呢？比如说面积、房子的大小（几室几厅）、地段、朝向等等，这些影响房屋价值的变量被称为特征(feature)。在这里，为了简单，我们假定房屋只由一个变量影响，那就是房屋的面积。

假设有一个房屋销售的数据如下：

```
面积(m^2)  销售价钱（万元）
123            250
150            320
87              160
102            220
…               …
```

插句题外话，顺便吐下槽，这套房屋价格数据在五年前可能还能买到帝都5环左右的房子，但现在只能买到二线城市的房屋了。

我们可以做出一个图，x轴是房屋的面积。y轴是房屋的售价，如下：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywznNPeptmJiapRGcfYDIBRx83VwdTPpI0wWlbsPYSSEkJ7kQ8Zxf4W7FQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果来了一个新的房子/面积，假设在房屋销售价格的记录中没有的，我们怎么办呢？

我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入面积，我们可以在将曲线上这个点对应的值返回。如果用一条直线去拟合房屋价格数据，可能如下图这个样子：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzqPkNG96AxL3q8zx8QcQ4R0XLOxDyxTIQ4UWdmM10hDqo9VMlC1oiaAg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

而图中绿色的点就是我们想要预测的点。

而图中绿色的点就是我们想要预测的点。

为了数学建模，首先给出一些概念和常用的符号。

房屋销售记录表 – 训练集(training set)或者训练数据(training data), 是我们流程中的输入数据，一般称为x
房屋销售价钱 – 输出数据，一般称为y
拟合的函数（或者称为假设或者模型），一般写做 y = h(x)
训练数据的条目数(#training set), 一条训练数据是由一对输入数据和输出数据组成的
输入数据的维度(特征的个数，#features)，n

然后便是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzHSxLvdwY1C28jZ6zahUiaxwSLWTMZma8uv5ic8D7ILEZrBGG99IDWN2A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywz5hSQ8arXSaGjhTxF1LZR6nCZYngJiaBcK4KHOGhYyuf7uuAyyP4IDdA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。

如果我们令X0 = 1，就可以用向量的方式来表示了：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzv0QkkyyRCA80jr82WbuFxViaoc6qcxuxqOAd8lFAJFXIE5xGpf630TA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，这里我们称这个函数为J函数。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzOjibRDTgtnROXEjiaZGyvRcK6ANtrRRPhicdT0KNYLVOvia7ERjuV6Liaeg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的系数1/2是为了方便求导（且在求导的时候，这个系数会消掉）。

如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。

1.2 梯度下降算法流程

梯度下降法的算法流程如下：
1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。
2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。

为了描述的更清楚，给出下面的图：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzTEmsDVQWjK2QaXYNibicqWhau7EbCkJtkB9gY0ZJ1icL5Zzrd7mABKGibA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分（让误差/损失最小嘛）。θ0，θ1表示θ向量的两个维度。

在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。

然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzVjvrMs7LIEQPTzmodyUulLooHhHkc8K0eQsmg4EJmLYHuHnfUWRGibg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzPlALEBLBROXJ4YZo1QCa05mF7fic0tHFn1mEZyd1icribITCfn0H5z0QA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。

下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywz0DZctHuCG8NU5nicV90ZgRhUrKic6r4weL0nic5ZrLibH7yhK9ydT90weg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzE8Ed8wA3N33hCMPmdNMfYC7RWzLcjhsePmbQbnVtQDKCVJbAExH4Jw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

用更简单的数学语言进行描述步骤2）是这样的：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzeia11gOoFlKWwVpo2bFWVaaH7Bgbyx9pDVAPicuzpYtt1W7TQxZQfMlA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

本题解析来源：@LeftNotEasy，链接：http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html

## 342题

牛顿法和梯度下降法有什么不同？

**解析：**

牛顿法（Newton's method）
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

具体步骤：
首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f  ' (x0)（这里f ' 表示函数 f  的导数）。

然后我们计算穿过点(x0,f(x0))并且斜率为f '(x0)的直线和x轴的交点的x坐标，也就是求如下方程的解：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywziaiaT8Cxa4zSEu4Esia0YY41ZXrU1BA25Ts8BicuAsCC8PWf2NqkUDnlFw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。

因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywz88Iwic2Qf0F8HBe6gt4kQL84DKssylDqaWMwZD6S9rwhWzvb7pCXfHg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

已经证明，如果f'是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f'(x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

![null](https://mmbiz.qpic.cn/mmbiz_gif/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzwJ7P3ILicYDyPYFMgsnfwLbxnWMDAE5hJfL92tllax506JtNx8tpibtg/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

**关于牛顿法和梯度下降法的效率对比：**
a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。

b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywz1jvjN7Iic5Vq5fE73oydJ3FOr2UicppxribyjfQ8yKcacicWWrKhicN5Llg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

牛顿法的优缺点总结：
优点：二阶收敛，收敛速度快；
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

本题解析来源：@wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

## 343题

熵、联合熵、条件熵、相对熵、互信息的定义

**解析：**

为了更好的理解，需要了解的概率必备知识有：
大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值；
P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布；
p(X = x)表示随机变量X取某个具体值的概率，简记为p(x)；
p(X = x, Y = y) 表示联合概率，简记为p(x,y)，p(Y = y|X = x)表示条件概率，简记为p(y|x)，且有：p(x,y) = p(x) * p(y|x)。
熵：如果一个随机变量X的可能取值为X = {x1, x2,…, xk}，其概率分布为P(X = xi) = pi（i = 1,2, ..., n），则随机变量X的熵定义为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzmaWhyu0Wf78TMSjMQtxwhiac2wz4vdqvhLugHCX0Al85qcQNNq3PJKg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

把最前面的负号放到最后，便成了：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzwNIqJrBH8GGCgwhu0nhfKtNKiaDjm4nicZqfm2jUk7k1cVPjFlO6R7bQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上面两个熵的公式，无论用哪个都行，而且两者等价，一个意思（这两个公式在下文中都会用到）。


联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。
条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。


且有此式子成立：H(Y|X) = H(X,Y) – H(X)，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzJ8Nyw5ob8VHic3xSP4ibqqGsvR7VTku3kM9YRudTVS9CnqXjjgGbHbmA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

简单解释下上面的推导过程。整个式子共6行，其中


第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和；


第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起；


第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-（log p(x,y) - log p(x)）写成- log (p(x,y)/p(x) ) ；


第五行推到第六行的依据是：p(x,y) = p(x) * p(y|x)，故p(x,y) / p(x) =  p(y|x)。


相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzXkw7W7Eic4RRc9VDZ9BDsluK6Swr0RAHEUnictdqYHGLNPNcRAkgkC5w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)。另外，值得一提的是，D(p||q)是必然大于等于0的。


互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzNLEEqhPUgibic7LcRvBwsicnmSPcQeZF8f82uLicGG7UkXosRBEibcXWptA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

且有I(X,Y)=D(P(X,Y) || P(X)P(Y))。下面，咱们来计算下H(Y)-I(X,Y)的结果，如下：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSictmC4bwG8tTZU94tu84ywztgbIJ8jxKcW6odW4TwcORwH7p7icEDico8TAwm7tqdybxkSicmg0vVU5w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过上面的计算过程，我们发现竟然有H(Y)-I(X,Y) = H(Y|X)。故通过条件熵的定义，有：H(Y|X) = H(X,Y) - H(X)，而根据互信息定义展开得到H(Y|X) = H(Y) - I(X,Y)，把前者跟后者结合起来，便有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。更多请查看《最大熵模型中的数学推导》（链接：http://blog.csdn.net/v_july_v/article/details/40508465）。

## 344题

说说你知道的核函数

**解析：**

通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：
多项式核

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzA2KBT4j30MDUvazicp84fsFzXOiaBps0tmQuBRwue3miblib8icJe0u4iamw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

显然刚才我们举的例子是这里多项式核的一个特例（R = 1，d = 2）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzjVkArezMDNDbwzIG4UHUXvCT9bbFWJsOu7YrtjFAZSEiaZmLNNjGvpA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中 m 是原始空间的维度。

高斯核

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzvCWNjgmUmnk2J6cjfsqv4VnOSd6VicHb8ER9rlpq9iasEgkCH0SIMWOA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。

不过，如果 σ 选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；

反过来，如果 σ 选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。

不过，总的来说，通过调控参数 σ ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzQ6ibZDafpPRD5pcibvuBIykw4teMJwibBg51VK4QDpL2RBsI1KtyHWJ4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

线性核

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzibu2ScOowibs4g4ztx3krpLhdKt8v5D94oj0mtYtWnrshGQBiaUibSYsgg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。

## 345题

什么是拟牛顿法（Quasi-Newton Methods）？

**解析：**

拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。

另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

具体步骤：
拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywz0h0Ks87P5rfICiak93BuiaX9SXW79ct9hvCfPSd8o7jvoiaO5os5Ho19w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzyhdSEDNMibibhyYNFQ5DX5FULibicynvPzWV5HuslLsU8hR3Ficrc41RLcQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中我们要求步长ak 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk 代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk 的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzpmEnxZy33EIKmcc2ymGWNa437UaHPibPloBdp9wxDN3VYpVhDQPs3Rg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywziccTicCOqB5njhymZSkaFAhVlSan5o6d2NBAGHM0Jo6Pw40ekOABK5lQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从而得到

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSictmC4bwG8tTZU94tu84ywzcqZhwSicicanNb9xlEatB6U0H2k09SOzXs2HcnjureUZWduprR9Zxv8w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。

本题解析来源：@wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

## 346题

kmeans的复杂度？

**解析：**

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgpibgRD32aOz7Pa1whMOvvL2lEicIGZtSGWGBsDKy0KKSS1tZDHyGGO7g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数
空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数

## 347题

请说说随机梯度下降法的问题和挑战？

解析：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgMGSR1m6HlevFR4f3LuJicQCw605ZB1icvSl3lqzorcW2PdM2Xkm168Kw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgqXQuYEOcFhdJZwER6HvJ4v7X0bCFzDVshhP060n2fkqCu0Udo0fZDw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgb7YPVmFwpcUADpGvRekCG0lYskibq6t1e7Vsa9Idg9DZIMA7fZPibGbg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgAYP1D11pRoicFoibEkHR7SPyloZavKjbphx5Hiast1VblF9Um5u2pW6Gg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 348题

说说共轭梯度法？

解析：

共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。
下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgmcic4QFkF4I8mErYicPtwiaiakulaaMP2Qnty5ohyc39h9evJCr0AL4bicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注：绿色为梯度下降法，红色代表共轭梯度法
本题解析来源： @wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

## 349题

对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法？

**解析：**

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDg5UbGMqY80yf1JYHU1wxl1tic2fhkOoaQfSqXiaTnMpm606pvnae5Vzjg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

没有免费的午餐定理：
对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。
也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。
但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。
本题解析来源：@抽象猴，链接：https://www.zhihu.com/question/41233373/answer/145404190

## 350题

什么是最大熵

解析：

熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。


为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。

例如，投掷一个骰子，如果问"每个面朝上的概率分别是多少"，你会说是等概率，即各点出现的概率均为1/6。因为对这个"一无所知"的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。


**3.1 无偏原则**

下面再举个大多数有关最大熵模型的文章中都喜欢举的一个例子。


例如，一篇文章中出现了“学习”这个词，那这个词是主语、谓语、还是宾语呢？换言之，已知“学习”可能是动词，也可能是名词，故“学习”可以被标为主语、谓语、宾语、定语等等。


令x1表示“学习”被标为名词， x2表示“学习”被标为动词。

令y1表示“学习”被标为主语， y2表示被标为谓语， y3表示宾语， y4表示定语。

且这些概率值加起来的和必为1，即 

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgT4JOQNwXemTWTeeOIZpnxtT6pmmGUovlibTZibQohlewNyfzL5qtDKpg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

则根据无偏原则，认为这个分布中取各个值的概率是相等的，故得到：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgtD7WDr3pcelpfmXqzia5hbhB8DlNOrPQLBVPoEXIoiabnTHdIwZWiagZg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因为没有任何的先验知识，所以这种判断是合理的。如果有了一定的先验知识呢？

即进一步，若已知：“学习”被标为定语的可能性很小，只有0.05，即

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgJkYIaT7yPOM9cRuLWQPnZCXNmkhE4a4PWIeNr9qZNBsEunMNS1Ejfw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

剩下的依然根据无偏原则，可得：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgXXOOPh4BhSnuqhTXu0ytgHWiaicz7qlmOoklPqr0ib5xoLNT3ssjnibx9g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

再进一步，当“学习”被标作名词x1的时候，它被标作谓语y2的概率为0.95，即

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgBwCkQeuwolwohY29XsePQX0sGA6OhW27gITk8B91j6hbib1AOLey7MQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

此时仍然需要坚持无偏见原则，使得概率分布尽量平均。但怎么样才能得到尽量无偏见的分布？

实践经验和理论计算都告诉我们，在完全无约束状态下，均匀分布等价于熵最大（有约束的情况下，不一定是概率相等的均匀分布。 比如，给定均值和方差，熵最大的分布就变成了正态分布 ）。
于是，问题便转化为了：计算X和Y的分布，使得H(Y|X)达到最大值，并且满足下述条件：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDg1ghLB8Dz4EcHmzCbYVJibgWGFgqYultEYLZwBHvMBADJKrMqfkaZbGA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，也就引出了最大熵模型的本质，它要解决的问题就是已知X，计算Y的概率，且尽可能让Y的概率最大（实践中，X可能是某单词的上下文信息，Y是该单词翻译成me，I，us、we的各自概率），从而根据已有信息，尽可能最准确的推测未知信息，这就是最大熵模型所要解决的问题。
相当于已知X，计算Y的最大可能的概率，转换成公式，便是要最大化下述式子H(Y|X)：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDgXC7tREibSrrphR4GN7EQxLtWrf8JJ8O5NYt0ekUIFBia0CTZZ22JGaJQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

且满足以下4个约束条件：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicapue9bSkiaBGkaS3Wb7gDg3VbmSaot7Mbjedl3T9DOCYcmlVcBncPLOunFLTFKrjjtwZFsjbPyjw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 351题

LR与线性回归的区别与联系



解析：

LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。

引用自：@AntZ

个人感觉逻辑回归和线性回归首先都是广义的线性回归，

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

引用自：@nishizhen

逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

引用自：@乖乖癞皮狗

## 352题

简单说下有监督学习和无监督学习的区别



解析：

有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）

无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)

## 353题

请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？



解析：

集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权).

决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂.

下面所提到的学习器都是决策树:

Bagging方法:

学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票;

Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化;

Boosting方法:

学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和;

Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数;

GBDT属于Boosting的优秀代表, 对函数残差近似值进行梯度下降, 用CART回归树做学习器, 集成为回归模型;

xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好。

关于决策树，这里有篇《决策树算法》（链接：http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。

引用自：@AntZ

xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：

1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数

2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性

3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

引用自：@Xijun LI

## 354题

为什么xgboost要用泰勒展开，优势在哪里？

**解析：**



解析：

xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。

引用自：@AntZ

## 355题

协方差和相关性有什么区别？



解析：

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9ib5T79shXaV0yy7Le0nxhroicYIT5UZVOJHot9NgCHibsusqk1pia7kd2AI4q2N6R0dXVcslib1wLiccQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9ib5T79shXaV0yy7Le0nxhrra9ay4mwOwWg4UJeqGl1XbAficeDxLfNaWcxpWgEMdVjWzPKmzdhppA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 356题

xgboost如何寻找最优特征？是有放回还是无放回的呢？



解析：

xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合。

## 357题

谈谈判别式模型和生成式模型？



解析：

判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。由生成模型可以得到判别模型，但由判别模型得不到生成模型。常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机

## 358题

线性分类器与非线性分类器的区别以及优劣



解析：

线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有（看线性核还是高斯核）引用自@伟祺

## 359题

L1和L2的区别

**解析：**



解析：

L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.简单总结一下就是： L1范数: 为x向量各个元素绝对值之和。 L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征，即L1范数可以使权值稀疏，方便特征提取。 L2范数可以防止过拟合，提升模型的泛化能力。L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？看导数一个是1一个是w便知, 在靠进零附近, L1以匀速下降到零, 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说, 重要性不在一个数量级上)尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零. 两者一起作用, 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之, 不养闲人也不要超人)。

## 360题

L1和L2正则先验分别服从什么分布



解析：

面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。引用自：@齐同学先验就是优化的起跑线, 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。对参数引入高斯正态先验分布相当于L2正则化, 这个大家都熟悉：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibZMn9PwHe3Lwn5fN9nGVnuUayr03qPp011fSN7Iqo3al1vCOdKiaMm7MlmPI47uY3vMNss62TZ7Mw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibZMn9PwHe3Lwn5fN9nGVnuELZCS3icxfwJ8sGLCruDKNcW89XD5sVRhQjnVXObW8GVRUN8vIaXqjg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibZMn9PwHe3Lwn5fN9nGVnu1wGugvj3OSuIp4IqNVUzxunHGWYK22rsIH6RSXtvxkZmIj8tic0bsrw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对参数引入拉普拉斯先验等价于 L1正则化, 如下图：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibZMn9PwHe3Lwn5fN9nGVnu5mN9yKGQAiaHKHhaahfM7PzTYVhLYFth0wDn3ZTSIIVzQwzQm2mrXdQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibZMn9PwHe3Lwn5fN9nGVnu06orguzSjavtV9D6aywB8HE7P2PGErVpDp9d2LwEu8qciaIvqWnarRQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从上面两图可以看出, L2先验趋向零周围, L1先验趋向零本身。

引用自：@AntZ

## 361题

简单介绍下logistics回归？




解析：

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。


假设函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDAXGj91RDM7oeicIPvqV8sf0iac2oAF0ounNXUHrJnF2RYEfL7RtNUxLg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中x是n维特征向量，函数g就是logistic函数。

而

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDswDyhicxvaNxhiaOPzKL8PUmQmBBH3jwgXqANiaRmVBM1ynwU7LDibsOJA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

的图像是


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDas8yA0fz8exibUiaRicCWSQrQjp8VKMuiaSlW7ZTByEgqXvWibITEoPpjvQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDZXppUSm5gqYyDLRYIIQVN2WXicvC0oBFLeeG2ibQrXmWytWA9VNQHyGQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从而，当我们要判别一个新来的特征属于哪个类时，只需求 hθ(x) 即可，若 hθ(x) 大于0.5就是y=1的类，反之属于y=0类。


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDC6SOH5kJv6vqZyR1fVomKhkXlbCT8bCicrNgMWC166ynicQNNYoK0LpQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 362题

说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。



解析：

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例 x∈X，而实例空间 

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDUkJHMGghEXbEvCmbeqoU4AHut5Z15R4530icgQBE9qFXFKHIBCXgTfA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。



Adaboost的算法流程如下：

步骤1. 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcD4ehd1yewyKC1PDAeK0a0FwPAfcUl9nbxR1NBHfqqMgPAibkS2rQ2ERQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

步骤2. 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮

a. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDhmiaXpeuEMDZYtA8praNU7HZjQPgRPk8O1uqKbbdBSUrPFqAtVg1Geg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

b. 计算Gm(x)在训练数据集上的分类误差率


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDfM3cIvolwb7gGqPWHzEJ4iaICzSJg7QDzkDeBltNfxr51GlsGicicR8jg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。



c. 计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDpOqmduhLnJL2pjib9Y9pt9yd0nz2aTqXTDAZ0SxUt0zhXvsEaH8W5YA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代

由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDjlialN2FYx4eWkRoGQib9iaXVBXeXFtMXRHwfic1N9KPwv7kZRyXVVzdIw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

其中，Zm是规范化因子，使得Dm+1成为一个概率分布：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDF38HvDxjGNaYTU7YS5vzo7mCk9TkOXbAo2OPD1uYBCH0DS8SHBtfCw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

步骤3. 组合各个弱分类器


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcD7IDxDEJicQ3kMDiaicYia5TGw3BVAjPV5fsDcW1KkljEefwPWVma0aZFOQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从而得到最终分类器，如下：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDM5ULa3Q3xN2UtuvRmAGzfEaiaICpICQUugOzf7jy7kfBpootxu4cqXw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

更多请查看此文：《Adaboost 算法的原理与推导》（链接：http://blog.csdn.net/v_july_v/article/details/40718799）。


## 363题

经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDn3E1icUI4Lj17ZhV2VdvtJavoicVMwHgZfEHKla1iagiaxBssIUTqknoEw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。




解析：

用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么"拼写检查"要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求P（C|W）的最大值。


而根据贝叶斯定理，有：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDdgbO5zfGVlSgNdMefdPoHuxZnIAXVMpIyeE61rmmuZd2vgbNdTYpnA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDJmzjbichDibMR6IYW5icvudjkYsib3Vs8BKj4gmMlfZF69YiboBoF39jDkg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

即可。其中：

P(c)表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。

P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见博客中的这篇文章。

所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见这里。


## 364题

为什么朴素贝叶斯如此“朴素”？


**解析：**



解析：

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。



朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是"很简单很天真"地假设样本特征彼此独立. 这个假设现实中基本上不存在, 但特征相关性很小的实际情况还是很多的, 所以这个模型仍然能够工作得很好。

引用自：@AntZ

## 365题

请大致对比下plsa和LDA的区别



解析：

pLSA中，主题分布和词分布确定后，以一定的概率

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDE8MRf2A0ibA4vT0icPib3WAWXvTy6D3mHuC10x5r1icHkfPtyfk5aFVOqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：∅kj  （由

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDasu7BVTQsL8QENwyzV5ICicwJ6rOnRKxiabXuIdf6GXHorkKS2cqw9ug/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

转换而来）

和 θik  由

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcD1rnFLqKFHpADFFTBEnlcmpnmtKrwJJC99V6F7PuLpXxmo7Kic5U2v1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

转换而来


文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDjPGjBRMDpc61m94R5iaFJDiaC5CicbluXXWWWfFt6bEwF2SVnoribMM2pw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。

文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。

还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDxGVncARib2qhkVHT5l4mMM2P368JTrQBuSnXPy6XBk1OD6WbicjTMdTw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

换言之，LDA在pLSA的基础上给这两参数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBwphnqYx5WdhdlYY0tJcDE8MRf2A0ibA4vT0icPib3WAWXvTy6D3mHuC10x5r1icHkfPtyfk5aFVOqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布，和一个词语分布的先验分布Dirichlet分布 α 。综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

更多请参见：《通俗理解LDA主题模型》（链接：http://blog.csdn.net/v_july_v/article/details/41209515）。

## 366题

简单介绍下EM算法 tics回归？




解析：

有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：　　

E步：选取一组参数，求出在该参数下隐含变量的条件概率值；　　

M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。　　

重复上面2步直至收敛。　　

公式如下所示：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIw60gvHHP5DwX5j9bXmDfOZKwbvshic58IwOKEGuKYib9wnzcRMhVRPnIQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

M步公式中下界函数的推导过程：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwIujrFMAguv0un43Vkfia2ttCj45zYbUx1OJQed4fqjocsqRhrMfBvOg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。 

GMM的E步公式如下（计算每个样本对应每个高斯的概率）：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwKC9LJrF5zR5dpNfSf0fib6gHoeYzWG6DUzouicH5IW8DLmZoWtp39Skg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

更具体的计算公式为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwSExtOhTkDlV2RISDNaQM8JyxJy0FZsG5KAK9c3FwEvptdziagoUlkJA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwvib9icjj2pYXfBF1gXHQKRYVkcpLiasdVmGm61Qt9kiaw0SCJpicp7lPf3Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

本题解析来源：@tornadomeet，链接：http://www.cnblogs.com/tornadomeet/p/3395593.html

## 367题

KNN中的K如何选取的？



解析：

关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。

如李航博士的一书「统计学习方法」上所说：如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，

与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 

K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。    在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

## 368题

防止过拟合的方法



解析：

过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 

处理方法： 


1 早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练 

2 数据集扩增：原有数据增加、原有数据加随机噪声、重采样 

3 正则化，正则化可以限制模型的复杂度

4 交叉验证

5 特征选择/特征降维

6 创建一个验证集是最基本的防止过拟合的方法。

我们最终训练得到的模型目标是要在验证集上面有好的表现，而不训练集

## 369题

什么最小二乘法？



解析：

我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。


最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwgn9ukucqibw4dHG7x6J3wX47yXLp55RAYYpArZ0OjDNbTD1lUAN3CYQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。


最小二乘法的一般形式可表示为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwu4MEf5XMmibQiaoiakYBpxLXqibP9OIFU7ZvJgSuHmfuz6PHI5guH0p7qQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

有效的最小二乘法是勒让德在 1805 年发表的，基本思想就是认为测量中有误差，所以所有方程的累积误差为

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwDL8m337CwQtDHE9lTOAo6ibwqskJbFBgwbcvQ8aTVTleAf88GqGJ1og/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们求解出导致累积误差最小的参数即可：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwd3NhXoTq1hGc72US4Yl2hte1yYGSzzOhFNF7qADG3fSgw0cibZnTEQQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

勒让德在论文中对最小二乘法的优良性做了几点说明： 

1,最小二乘使得误差平方和最小，并在各个方程的误差之间建立了一种平衡，从而防止某一个极端误差取得支配地位

2.计算中只要求偏导后求解线性方程组，计算过程明确便捷

3.最小二乘可以导出算术平均值作为估计值    对于最后一点，从统计学的角度来看是很重要的一个性质。推理如下：假设真值为θ, x1,⋯,xn为n次测量值, 每次测量的误差为ei=xi−θ，按最小二乘法，误差累积为

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwTAndNQIOFUsbpQibTnFyudnIydTeEDmrXcxaxKVPw28iay5FesN6lszg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

求解θ使L(θ)达到最小，正好是算术平均

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwgRoT9Cmn3h1FKnIqDiar3sLvdgQCZbcPHYxmm8Ewsehh8z1boLnYghA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由于算术平均是一个历经考验的方法，而以上的推理说明，算术平均是最小二乘的一个特例，所以从另一个角度说明了最小二乘方法的优良性，使我们对最小二乘法更加有信心。


最小二乘法发表之后很快得到了大家的认可接受，并迅速的在数据分析实践中被广泛使用。不过历史上又有人把最小二乘法的发明归功于高斯，这又是怎么一回事呢。高斯在1809年也发表了最小二乘法，并且声称自己已经使用这个方法多年。高斯发明了小行星定位的数学方法，并在数据分析中使用最小二乘方法进行计算，准确的预测了谷神星的位置。 

对了，最小二乘法跟SVM有什么联系呢？请参见《支持向量机通俗导论（理解SVM的三层境界）》(链接：http://blog.csdn.net/v_july_v/article/details/7624837）。

## 370题

简单说说贝叶斯定理



解析：

在引出贝叶斯定理之前，先学习几个定义：条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。

比如，在同一个样本空间Ω中的事件或者子集A与B，如果随机从Ω中选出的一个元素属于B，那么这个随机选择的元素还属于A的概率就定义为在B的前提下A的条件概率，所以：P(A|B) = |A∩B|/|B|，接着分子、分母都除以|Ω|得到

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwYhlCM9vjKSh7bV3eTSOyrLvuwltCTfkXxsS57eSdiaQDY0kIUBibta3g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

联合概率表示两个事件共同发生的概率。A与B的联合概率表示为

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIw0PSuVC2fjDLMtZtaR8JwNaKiaAiaAibHujnoawNS177icRm9UicVOrQMZ0w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

和

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwoa1soLoCEzJXO0yqNlOKf56lu45PX6tu9LZgcSK5ARpcdGgMXTMl1w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。

接着，考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。 首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示； 

其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；


类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示； 

同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。



贝叶斯定理便是基于下述贝叶斯公式：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwaGu64fIGbWtIygkyADeQl4TNFVWwxmrwdqhmMjak20gc1EZr5feMmQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上述公式的推导其实非常简单，就是从条件概率推出。    根据条件概率的定义，在事件B发生的条件下事件A发生的概率是

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwBibicCReYj2fUmzAzhyAun0Dpt3B1wKjj0SoiahpehwRaGeicpf67KOeHw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

同样地，在事件A发生的条件下事件B发生的概率

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwjhicha9v87GFTqmG2N9dBPX1fyrWicnaMaWnG16wxQV6ibHlaDiasgwVrg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

整理与合并上述两个方程式，便可以得到：

![null](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

接着，上式两边同除以P(B)，若P(B)是非零的，我们便可以得到贝叶斯定理的公式表达式：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSic37XJv3K9ZibJ7G7Sgic2xIwGr9oicY8Ez6yrSo5OAiaMVvIOYia9wSCHJSPTyFvkQJ9lNdgyIby5n1Ew/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为P(A,B) = P(A)P(B|A) = P(B)P(A|B)，所以P(A|B) = P(A)P(B|A)  / P(B)。更多请参见此文：《从贝叶斯方法谈到贝叶斯网络》（链接：http://blog.csdn.net/v_july_v/article/details/40984699）。

## 371题

标准化与归一化的区别？



解析：

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。关于什么是归一化，请参见：https://www.julyedu.com/question/big/kp_id/23/ques_id/1011

## 372题

随机森林如何处理缺失值？



解析：

方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。

## 373题

随机森林如何评估特征重要性？



解析：

衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。

## 374题

优化Kmeans？



解析：

使用kd树或者ball tree将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。

## 375题

KMeans初始类簇中心点的选取。



解析：

k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心 

2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 

3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大

4. 重复2和3直到k个聚类中心被选出来

5. 利用这k个初始的聚类中心来运行标准的k-means算法

## 376题

解释对偶的概念



解析：

一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

## 377题

如何进行特征选择？



解析：

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解常见的特征选择方式：

1. 去除方差较小的特征

2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。 

3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。

它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。 

4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

## 378题

衡量分类器的好坏？



解析：

这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。

几种常用的指标：

精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量） 

召回率 recall = TP/(TP+FN) = TP/ P 

F1值： 2/F1 = 1/recall + 1/precision 

ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N 

更详细请点击：https://siyaozhang.github.io/2017/04/04/%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8

解析来源：@我愛大泡泡，链接：http://blog.csdn.net/woaidapaopao/article/details/77806273

## 379题

机器学习和统计里面的auc的物理意义是啥？



解析：

auc是评价模型好坏的常见指标之一，详见：https://www.zhihu.com/question/39840928

## 380题

数据预处理



解析：

1.缺失值，填充缺失值fillna：

i. 离散：None,

ii. 连续：均值。

iii. 缺失值太多，则直接去除该列

2.连续值：离散化。有的模型（如决策树）需要离散值 

3.对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作 

4.皮尔逊相关系数，去除高度相关的列

## 381题

观察增益gain, alpha和gamma越大，增益越小？



解析：

xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。

大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量): 

第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失 

由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大. 

原问题是alpha而不是lambda, 这里paper上没有提到, xgboost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的:

https://zhidao.baidu.com/question/2121727290086699747.html?fr=iks&word=xgboost+lamda&ie=gbk 

lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。

这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 

gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。 

引用自@AntZ


## 382题

什麽造成梯度消失问题?



解析：

Yes you should understand backdrop－Andrej KarpathyHow does the ReLu solve the vanishing gradient problem? 

神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。 

梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8vUHDAuRuJuR5s8StcLllGCGeVdwpkAcCOo0HYnHHKMk8QL0iboqdvmak5jpnQ5zOib1MUC6yv0ZfA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

解析来源：@许韩，链接：https://www.zhihu.com/question/41233373/answer/145404190

简而言之，就是sigmoid函数f(x)的导数为f(x)*(1-f(x))， 因为f(x)的输出在0-1之间，所以随着深度的增加，从顶端传过来的导数每次都乘以两个小于1的数，很快就变得特别特别小。引用自@张雨石

## 383题

简单说说特征工程。



解析：

首先，大多数机器学习从业者主要在公司做什么呢？不是做数学推导，也不是发明多高大上的算法，而是做特征工程，如下图所示（图来自：http://www.julyedu.com/video/play/18）

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8vUHDAuRuJuR5s8StcLllGfzULa9jZTE717QFlSkDOBibFcN6WUrppIe9kWtyRH45pSAAtl0WPaiag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

进一步，特征工程主要是做如下工作（图来自机器学习第九期第五次课 特征工程）

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8vUHDAuRuJuR5s8StcLllGMfjrPzhsfWScQh3mfRaQyNJ7pTFtVicPwRFa5pqcrohPuVUibSWh295Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 384题

你知道有哪些数据处理和特征工程的处理？



解析：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8vUHDAuRuJuR5s8StcLllGZASv3piaH2SiauUSAe6Z7CyABcAcQiaaRegiakjeB1O0Xt5HE1Xcyia9AFg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

更多请查看此课程《机器学习工程师 第八期 [六大阶段、层层深入]》第7次课 特征工程。

## 385题

准备机器学习面试应该了解哪些理论知识？



解析：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8vUHDAuRuJuR5s8StcLllGcr9foh3reibjHx7pwtluOD0sRZXrR4MftSOAc5I2tIhcgMVfIxibjRtQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

看下来，这些问题的答案基本都在本BAT机器学习面试1000题系列里了。 引用自@穆文，链接：https://www.zhihu.com/question/62482926

## 386题

数据不平衡问题



解析：

这主要是由于数据分布不平衡造成的。解决方法如下： 

采样，对小样本加噪声采样，对大样本进行下采样 

数据生成，利用已知样本生成新的样本 

进行特殊的加权，如在Adaboost中或者SVM中 

采用对不平衡数据集不敏感的算法 

改变评价标准：用AUC/ROC来进行评价采用Bagging/Boosting/ensemble等方法在设计模型的时候考虑数据的先验分布

## 387题

特征比数据量还大时，选择什么样的分类器？



解析：

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。 

来源：

http://blog.sina.com.cn/s/blog_178bcad000102x70r.html

## 388题

常见的分类算法有哪些？



解析：

SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

## 389题

常见的监督学习算法有哪些？



解析：

感知机、svm、人工神经网络、决策树、逻辑回归

## 390题

说说常见的优化算法及其优缺点？



解析：

温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。 

简言之

1）随机梯度下降优点：可以一定程度上解决局部最优解的问题缺点：收敛速度较慢

2）批量梯度下降优点：容易陷入局部最优解缺点：收敛速度较快 

3）mini_batch梯度下降综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。4）牛顿法牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算  Hessian矩阵比较困难。 

5）拟牛顿法拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

具体而言从每个batch的数据来区分 

梯度下降：每次使用全部数据集进行训练 

优点：得到的是最优解 

缺点：运行速度慢，内存可能不够 

随机梯度下降：每次使用一个数据进行训练

优点：训练速度快，无内存问题 

缺点：容易震荡，可能达不到最优解 

Mini-batch梯度下降 

优点：训练速度快，无内存问题，震荡较少 

缺点：可能达不到最优解

从优化方法上来分：

随机梯度下降（SGD）

缺点选择合适的learningrate比较难对于所有的参数使用同样的learning rate容易收敛到局部最优可能困在saddle pointSGD+Momentum 

优点：积累动量，加速训练局部极值附近震荡时，由于动量，跳出陷阱梯度方向发生变化时，动量缓解动荡。

Nesterov Mementum与Mementum类似，

优点：避免前进太快提高灵敏度 

AdaGrad 

优点：控制学习率，每一个分量有各自不同的学习率适合稀疏数据 

缺点依赖一个全局学习率学习率设置太大，其影响过于敏感后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。RMSProp 

优点：解决了后期提前结束的问题。 

缺点：依然依赖全局学习率

Adam 

Adagrad和RMSProp的合体 

优点：结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点为不同的参数计算不同的自适应学习率也适用于大多非凸优化 -适用于大数据集和高维空间 

牛顿法 

牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难拟牛顿法 

拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

## 391题

特征向量的归一化方法有哪些？



解析：

线性函数转换，表达式如下：

y=(x-MinValue)/(MaxValue-MinValue) 

对数函数转换，表达式如下：

y=log10 (x) 

反余切函数转换 ，表达式如下：y=arctan(x)*2/PI 

减去均值，除以方差：

y=(x-means)/ variance

## 392题

RF与GBDT之间的区别与联系？



解析：

1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。 

2）不同点：

a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成 

b 组成随机森林的树可以并行生成，而GBDT是串行生成 

c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和 

d 随机森林对异常值不敏感，而GBDT对异常值比较敏感 

e 随机森林是减少模型的方差，而GBDT是减少模型的偏差

f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

## 393题

试证明样本空间中任意点X到超平面(w, b)的距离为式(6.2).



解析：

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8EWX8lP1LRraHeR1yQ2rOwSOSia1icD0EqQZJG9acBkvdxefXuvwrqNgKrJuWPfLicTTnaL8kOnOicEg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从网上下载或自己编程实现一一个卷积神经网络,并在手写字符识别数据 MNIST上进行实验测试

http://blog.csdn.net/Snoopy_Yuan

## 394题

请比较下EM算法、HMM、CRF



解析：

这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。 

（1）EM算法 　　

EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：

E步，求期望（expectation）；

M步，求极大（maxmization）。

本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 　　

注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。

（2）HMM算法 　　

隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。


马尔科夫三个基本问题：概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径） 

（3）条件随机场CRF 　

　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 　

之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。 

（4）HMM和CRF对比 　　

其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。

## 395题

带核的SVM为什么能分类非线性问题？



解析：

核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面, 如图:

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8EWX8lP1LRraHeR1yQ2rOwuHzJgqYtib1MxLm5ZKnTvuIZJxKegCb1IFVvtvtfibyH3BkWHiaUv0jicg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。

## 396题

请说说常用核函数及核函数的条件



解析：

我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。SVM关键是选取核函数的类型，常用核函数主要有线性内核，多项式内核，径向基内核（RBF），sigmoid核。

线性核函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8mXexDrRSD8ENrPcV98KlGGAS53qCMg0TiaNWa9WBny9aKBzicsfKCV8icFPicMvWntkQI56fUDWDXHQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的

多项式核函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8mXexDrRSD8ENrPcV98KlGHD6eXctwmAdXsHckZCzDdLeMt0KQCuicZKq0pmLe8rCMsrTaTY2ZIMg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。 

高斯（RBF）核函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8mXexDrRSD8ENrPcV98KlGTpl8CkyGnmMn4DEqkUIAjqmyUfAibYuD1LGicOHrbnX0JT1CZ6olwic9g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。 

sigmoid核函数

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8mXexDrRSD8ENrPcV98KlGGu9SbMs1p6TstjZXCJEw87ucHfrKd6ccprcgLs8zghk9SK3VRyDiaqw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。 

因此，在选用核函数的时候，如果我们对我们的数据有一定的先验知识，就利用先验来选择符合数据分布的核函数；如果不知道的话，通常使用交叉验证的方法，来试用不同的核函数，误差最下的即为效果最好的核函数，或者也可以将多个核函数结合起来，形成混合核函数。

在吴恩达的课上，也曾经给出过一系列的选择核函数的方法： 

如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM； 

如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；

如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。

## 397题

请具体说说Boosting和Bagging的区别



解析：

（1） Bagging之随机森林 　　

随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：　

1）Boostrap从袋内有放回的抽取样本值　

2）每次随机抽取一定数量的特征（通常为sqr(n)）。 　　

分类问题：采用Bagging投票的方式选择类别频次最高的 　　

回归问题：直接取每颗树结果的平均值。 

常见参数误差分析优点缺点 

1、树最大深度 

2、树的个数

3、节点上的最小样本数 

4、特征数(sqr(n))oob(out-of-bag)将各个树的未采样样本作为预测样本统计误差作为误分率可以并行计算不需要特征选择可以总结出特征重要性可以处理缺失数据不需要额外设计测试集在回归上不能输出连续结果

（2）Boosting之AdaBoost  


Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。 

（3）Boosting之GBDT   


将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。 　　

注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。

（4）Boosting之Xgboost


这个工具主要有以下几个特点： 

支持线性分类器 

可以自定义损失函数，并且可以用二阶偏导 

加入了正则化项：叶节点数、每个叶节点输出score的L2-norm 

支持特征抽样 

在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。

## 398题

逻辑回归相关问题



解析：

（1）公式推导一定要会 

（2）逻辑回归的基本概念  　　

这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。

（3）L1-norm和L2-norm 　　

其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 　　

但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。

（4）LR和SVM对比 　

首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。  　　

其次，两者都是线性模型。 　　

最后，SVM只考虑支持向量（也就是和分类相关的少数点） 

（5）LR和随机森林区别 　　

随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 

（6）常用的优化方法 　　

逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 　　

一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。

二阶方法：牛顿法、拟牛顿法： 　　

这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。 


缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。  


拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

## 399题

什么是共线性, 跟过拟合有什么关联?



解析：

共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。 

本题解析来源：@抽象猴，链接：https://www.zhihu.com/question/41233373/answer/145404190

## 400题

用贝叶斯机率说明Dropout的原理



解析：

回想一下使用Bagging学习,我们定义 k 个不同的模型,从训练集有替换采样 构造 k 个不同的数据集,然后在训练集上训练模型 i。 

Dropout的目标是在指数 级数量的神经网络上近似这个过程。Dropout训练与Bagging训练不太一样。在Bagging的情况下,所有模型是独立 的。 

在Dropout的情况下,模型是共享参数的,其中每个模型继承的父神经网络参 数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在Bagging的情况下,每一个模型在其相应训练集上训练到收敛。

在Dropout的情况下,通常大部分模型都没有显式地被训练,通常该模型很大,以致到宇宙毁灭都不 能采样所有可能的子网络。取而代之的是,可能的子网络的一小部分训练单个步骤,参数共享导致剩余的子网络能有好的参数设定。http://bi.dataguru.cn/article-10459-1.html
