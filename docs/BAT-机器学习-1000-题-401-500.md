<!--yml
category: 面试
date: 2022-07-01 00:00:00
-->

# BAT 机器学习 1000 题 401-500

## 401题

对于维度极低的特征，选择线性还是非线性分类器？



解析：

非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。

1.如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM

2.如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 

3.如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。

## 402题

请问怎么处理特征向量的缺失值



解析：

一方面，缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。 

另一方面缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理: 

1) 把NaN直接作为一个特征，假设用0表示；

2) 用均值填充；

3) 用随机森林等算法预测填充。

## 403题

SVM、LR、决策树的对比。



解析：

模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝 

损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失 

数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感

数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

## 404题

什么是ill-condition病态问题？



解析：

训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

## 405题

简述KNN最近邻分类算法的过程？



解析：

1.计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；

2.对上面所有的距离值进行排序；

3.选前 k 个最小距离的样本；

4.根据这 k 个样本的标签进行投票，得到最后的分类类别；

## 406题

常用的聚类划分方式有哪些？列举代表算法。



解析：

1.基于划分的聚类:K-means，k-medoids，CLARANS。

2.基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。 

3.基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。

4.基于网格的方法：STING，WaveCluster。 

5.基于模型的聚类：EM,SOM，COBWEB。

## 407题

什么是偏差与方差？



解析：

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。

偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

偏差：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibpN156icuF3FLGJyNIxbajaiaCyNoHHz0YXkx2fNOvVu4RFs5SvTeVydPYQukRWib2NodnV4mnc1XOw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

方差：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibpN156icuF3FLGJyNIxbajasmZZSGX2AajX848oN4FIGRP9Ytg9lJkrgle8GzR0bkmgQgQpqmqkpQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 408题

解决bias和Variance问题的方法是什么？



解析：

High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征High Variance解决方案：agging、简化模型、降维 

具体而言 

高偏差, 可以用boosting模型, 对预测残差进行优化, 直接降低了偏差. 也可以用高模型容量的复杂模型(比如非线性模型, 深度神经网络), 更多的特征, 来增加对样本的拟合度.高方差, 一般使用平均值法, 比如bagging, 或者模型简化/降维方法, 来降低方差. 

高偏差和高方差都是不好的, 我们应该加以避免. 但是它们又是此消彼长的关系, 所以必须权衡考虑. 一般情况下, 交叉验证训练可以取得比较好的平衡:将原始样本均分成K组, 将每组样本分别做一次验证集,其余的K-1组子集数据作为训练集,这样会得到K个模型, 这K个模型可以并发训练以加速. 用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标. K一般大于等于3, 而K-CV 的实验共需要建立 k 个models，并计算 k 次 test sets 的平均预测正确率。 

在实作上，k 要够大才能使各回合中的 训练样本数够多，一般而言 k=10 (作为一个经验参数)算是相当足够了。

## 409题

采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？



解析：

用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

## 410题

xgboost怎么给特征评分？



解析：

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。


```py
# feature importance  print(model.feature_importances_) 

#plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)  pyplot.show(

# plot feature importance  plot_importance(model) 

pyplot.show()
```


## 411题

什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？



解析：

bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据oob（out of bag）,它可以用于取代测试集误差估计方法。 

袋外数据(oob)误差的计算方法如下：对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。

## 412题

推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到



解析：

根据贝叶斯公式P(c|d)=（P(c)P(d|c)/P(d)）

这里，分母P(d)不必计算，因为对于每个类都是相等的。 分子中，P(c)是每个类别的先验概率，可以从训练集直接统计，P(d|c)根据独立性假设，可以写成如下 P(d|c)=￥P(wi|c)（￥符号表示对d中每个词i在c类下概率的连乘），P(wi|c)也可以从训练集直接统计得到。 至此，对未知类别的d进行分类时，类别为c=argmaxP(c)￥P(wi|c)。

## 413题

请写出你了解的机器学习特征工程操作，以及它的意义



解析：

特征工程包括数据与特征处理、特征选择和降纬三部分。 

数据与特征处理包括： 

1.  数据选择、清洗、采样- 数据格式化；- 数据清洗，填充缺失值、去掉脏数据，将不可信的样本丢掉，缺省值极多的字段考虑不用；- 采样：针对正负样本不平衡的情况，当正样本远大于负样本时，且量都很大时，使用下采样，量不大时，可采集更多的数据或oversampling或修改损失函数；采样过程中可利用分层抽样保持不同类别数据的比例。

2.  不同类型数据的特征处理- 数值型：幅度调整/归一化、log等变化、统计值（例如max、min、mean、std）、离散化、分桶等- 类别型：one-hot编码等- 时间型： 提取出连续值的持续时间和间隔时间；提取出离散值的“年”、“月”、“日”、“一年中哪个星期/季度”、“一周中的星期几”、“工作日/周末”等信息- 文本型：使用If-idf特征- 统计型：加减平均、分位线、次序、比例


意义： 

- 对数据进行预处理，可提高数据质量，提高挖掘质量。对数据进行清洗可填充缺失值、光滑噪声数据，识别和删除离群点数据，保证数据的一致性；

- 使用正确的采样方法可解决因数据不平衡带来的预测偏差；

- 对不同的数据类型进行不同的特征处理有助于提高特征的可用性，例如对数值型数据进行归一化可将数据转化到统一量纲下；对类别型数据，可用one-hot编码方法将类别数据数字化，数字化特征之后可更用来计算距离、相似性等；可从时间型数据当中提取中更多的时间特征，例如年、月和日等，这些特征对于业务场景以及模型的预测往往有很大的帮助。统计型特征处理有助于从业务场景中挖掘更丰富的信息。

特征选择包括： 

1.Filter使用方差、Pearson相关系数、互信息等方法过滤特征，评估单个特征和结果值之间的相关程度，留下Top相关的特征部分。

2.Wrapper可利用“递归特征删除算法”，把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估效果。

3.Embedded可利用正则化方式选择特征，使用带惩罚项的基模型，除了选择出特征外，同时也进行了降纬。 

意义：-剔除对结果预测不大的特征，减小冗余，选择有意义的特征输入模型，提高计算性能。



降纬：方法：主成分分析法（PCA）和线性判别分析（LDA）

意义：通过PCA或LDA方法，将较高纬度样本空间映射到较低维度的样本空间，从而达到降纬的目的，减少模型的训练时间，提高模型的计算性能。

## 414题

请写出你对VC维的理解和认识



解析：

VC维是模型的复杂程度，模型假设空间越大，VC维越高。某种程度上说，VC维给机器学习可学性提供了理论支撑。


1.测试集合的loss是否和训练集合的loss接近？VC维越小，理论越接近，越不容易overfitting。 

2.训练集合的loss是否足够小？VC维越大，loss理论越小，越不容易underfitting。 


我们对模型添加的正则项可以对模型复杂度(VC维)进行控制，平衡这两个部分。

## 415题

怎么理解“机器学习的各种模型与他们各自的损失函数一一对应？”



解析：

寒：首先你要明确 超参数 和 参数 的差别，超参数通常是你为了定义模型，需要提前敲定的东西(比如多项式拟合的最高次数，svm选择的核函数)，参数是你确定了超参数(比如用最高3次的多项式回归)，学习到的参数(比如多项式回归的系数) 

另外可以把机器学习视作 表达 + 优化，其中表达的部分，各种模型会有各种不同的形态(线性回归 逻辑回归 SVM 树模型)，但是确定了用某个模型(比如逻辑回归)去解决问题，你需要知道当前模型要达到更好的效果，优化方向在哪，这个时候就要借助损失函数了。 

下面就是一个小例子，一样的打分函数，选用不同的loss function会变成不同的模型

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicVL735r6B70LEQ21o7RKAZm79761oMJUiaYx7t9nspAdXw2UqjaVJHEibjc9Qcrtmx11NmXqGWKcFg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图取自 http://cs231n.github.io/linear-classify/ 有一个我汉化的版本 https://blog.csdn.net/han_xiao ... 99583 更深入的内容欢迎查阅更多机器学习的资料，或者参与机器学习相关的课程，来讨论

## 416题

给你一个有1000列和1百万行的训练数据集。这个数据集是基于分类问题的。经理要求你来降低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设）



解析：

答：你的面试官应该非常了解很难在有限的内存上处理高维的数据。以下是你可以使用的处理方法： 

1.由于我们的RAM很小，首先要关闭机器上正在运行的其他程序，包括网页浏览器，以确保大部分内存可以使用。

2.我们可以随机采样数据集。这意味着，我们可以创建一个较小的数据集，比如有1000个变量和30万行，然后做计算。 

3.为了降低维度，我们可以把数值变量和分类变量分开，同时删掉相关联的变量。对于数值变量，我们将使用相关性分析。对于分类变量，我们可以用卡方检验。

4.另外，我们还可以使用PCA（主成分分析），并挑选可以解释在数据集中有最大偏差的成分。

5.利用在线学习算法，如VowpalWabbit（在Python中可用）是一个可能的选择。 

6.利用Stochastic GradientDescent（随机梯度下降）法建立线性模型也很有帮助。 

7.我们也可以用我们对业务的理解来估计各预测变量对响应变量的影响大小。但是，这是一个主观的方法，如果没有找出有用的预测变量可能会导致信息的显著丢失。 

注意：对于第4和第5点，请务必阅读有关在线学习算法和随机梯度下降法的内容。这些是高阶方法。

## 417题

问2：在PCA中有必要做旋转变换吗？如果有必要，为什么？如果你没有旋转变换那些成分，会发生什么情况？



解析：

答：是的，旋转（正交）是必要的，因为它把由主成分捕获的方差之间的差异最大化。这使得主成分更容易解释。但是不要忘记我们做PCA的目的是选择更少的主成分（与特征变量个数相较而言），那些选上的主成分能够解释数据集中最大方差。 

通过做旋转，各主成分的相对位置不发生变化，它只能改变点的实际坐标。如果我们没有旋转主成分，PCA的效果会减弱，那样我们会不得不选择更多个主成分来解释数据集里的方差。注意：对PCA（主成分分析）需要了解更多。

## 418题

给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？



解析：

答：这个问题给了你足够的提示来开始思考！由于数据分布在中位数附近，让我们先假设这是一个正态分布。 

我们知道，在一个正态分布中，约有68％的数据位于跟平均数（或众数、中位数）1个标准差范围内的，那样剩下的约32%的数据是不受影响的。 

因此，约有32%的数据将不受到缺失值的影响。

## 419题

给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？



解析：

答：如果你分析过足够多的数据集，你应该可以判断出来癌症检测结果是不平衡数据。在不平衡数据集中，精度不应该被用来作为衡量模型的标准，因为96％（按给定的）可能只有正确预测多数分类，但我们感兴趣是那些少数分类（4％），是那些被诊断出癌症的人。 

因此，为了评价模型的性能，应该用灵敏度（真阳性率），特异性（真阴性率），F值用来确定这个分类器的“聪明”程度。如果在那4%的数据上表现不好，我们可以采取以下步骤：

1.我们可以使用欠采样、过采样或SMOTE让数据平衡。

2.我们可以通过概率验证和利用AUC-ROC曲线找到最佳阀值来调整预测阀值。

3.我们可以给分类分配权重，那样较少的分类获得较大的权重。 

4.我们还可以使用异常检测。

注意：要更多地了解不平衡分类

## 420题

为什么朴素贝叶斯如此“朴素”？



解析：

答：朴素贝叶斯太‘朴素’了，因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的。

## 421题

解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？



解析：

先验概率就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。 

例如，在一个数据集中，因变量是二进制的（1和0）。例如，1（垃圾邮件）的比例为70％和0（非垃圾邮件）的为30％。因此，我们可以估算出任何新的电子邮件有70％的概率被归类为垃圾邮件。 

似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。边际似然估计就是，“FREE”这个词在任何消息中使用的概率

## 422题

你正在一个时间序列数据集上工作。经理要求你建立一个高精度的模型。你开始用决策树算法，因为你知道它在所有类型数据上的表现都不错。后来，你尝试了时间序列回归模型，并得到了比决策树模型更高的精度。 

这种情况会发生吗？为什么？



解析：

众所周知，时间序列数据有线性关系。另一方面，决策树算法是已知的检测非线性交互最好的算法。


为什么决策树没能提供好的预测的原因是它不能像回归模型一样做到对线性关系的那么好的映射。

因此，我们知道了如果我们有一个满足线性假设的数据集，一个线性回归模型能提供强大的预测。

## 423题

给你分配了一个新的项目，是关于帮助食品配送公司节省更多的钱。问题是，公司的送餐队伍没办法准时送餐。结果就是他们的客户很不高兴。

最后为了使客户高兴，他们只好以免餐费了事。哪个机器学习算法能拯救他们？



解析：

你的大脑里可能已经开始闪现各种机器学习的算法。但是等等！这样的提问方式只是来测试你的机器学习基础。这不是一个机器学习的问题，而是一个路径优化问题。

机器学习问题由三样东西组成： 

1.模式已经存在。

2.不能用数学方法解决（指数方程都不行）。

3.有相关的数据。

## 424题

你意识到你的模型受到低偏差和高方差问题的困扰。应该使用哪种算法来解决问题呢？为什么？



解析：

低偏差意味着模型的预测值接近实际值。换句话说，该模型有足够的灵活性，以模仿训练数据的分布。貌似很好，但是别忘了，一个灵活的模型没有泛化能力。这意味着，当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。 

在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。 

另外，为了应对大方差，我们可以： 

1.使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。

2.使用可变重要性图表中的前n个特征。可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。

## 425题

给你一个数据集。该数据集包含很多变量，你知道其中一些是高度相关的。经理要求你用PCA。你会先去掉相关的变量吗？为什么？



解析：

答：你可能会说不，但是这有可能是不对的。丢弃相关变量会对PCA有实质性的影响，因为有相关变量的存在，由特定成分解释的方差被放大。 

例如：在一个数据集有3个变量，其中有2个是相关的。如果在该数据集上用PCA，第一主成分的方差会是与其不相关变量的差异的两倍。此外，加入相关的变量使PCA错误地提高那些变量的重要性，这是有误导性的。

## 426题

花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM （Gradient Boosted Models），想着boosting算法会显示魔力。不幸的是，没有一个模型比基准模型表现得更好。最后，你决定将这些模型结合到一起。尽管众所周知，结合模型通常精度高，但你就很不幸运。你到底错在哪里？



解析：

答：据我们所知，组合的学习模型是基于合并弱的学习模型来创造一个强大的学习模型的想法。但是，只有当各模型之间没有相关性的时候组合起来后才比较强大。由于我们已经试了5个 GBM，但没有提高精度，表明这些模型是相关的。

具有相关性的模型的问题是，所有的模型提供相同的信息。例如：如果模型1把User1122归类为 1，模型2和模型3很有可能会做有同样分类，即使它的实际值应该是0，因此，只有弱相关的模型结合起来才会表现更好。

## 427题

KNN和KMEANS聚类（kmeans clustering）有什么不同？



解析：

答：不要被它们的名字里的“K”误导。 

你应该知道，这两种算法之间的根本区别是，KMEANS本质上是无监督学习而KNN是监督学习。KMEANS是聚类算法。KNN是分类（或回归）算法。

KMEAN算法把一个数据集分割成簇，使得形成的簇是同构的，每个簇里的点相互靠近。该算法试图维持这些簇之间有足够的可分离性。由于无监督的性质，这些簇没有任何标签。NN算法尝试基于其k（可以是任何数目）个周围邻居来对未标记的观察进行分类。它也被称为懒惰学习法，因为它涉及最小的模型训练。因此，它不用训练数据对未看见的数据集进行泛化。

## 428题

真阳性率和召回有什么关系？写出方程式。



解析：

答：真阳性率=召回。是的，它们有相同的公式（TP / TP + FN）。注意：要了解更多关于估值矩阵的知识。

## 429题

你建了一个多元回归模型。你的模型R2为并不如你设想的好。为了改进，你去掉截距项，模型R的平方从0.3变为0.8。这是否可能？怎样才能达到这个结果？



解析：

答：是的，这有可能。我们需要了解截距项在回归模型里的意义。截距项显示模型预测没有任何自变量，比如平均预测。公式R² = 1 – ∑(y – y´)²/∑(y – ymean)²中的y´是预测值。 

当有截距项时，R²值评估的是你的模型基于均值模型的表现。在没有截距项（ymean）时，当分母很大时，该模型就没有这样的估值效果了，∑(y – y´)²/∑(y – ymean)²式的值会变得比实际的小，而R2会比实际值大。

## 430题

在分析了你的模型后，经理告诉你，你的模型有多重共线性。你会如何验证他说的是真的？在不丢失任何信息的情况下，你还能建立一个更好的模型吗？



解析：

答：要检查多重共线性，我们可以创建一个相关矩阵，用以识别和除去那些具有75％以上相关性（决定阈值是主观的）的变量。此外，我们可以计算VIF（方差膨胀因子）来检查多重共线性的存在。 

VIF值<= 4表明没有多重共线性，而值> = 10意味着严重的多重共线性。 

此外，我们还可以用容差作为多重共线性的指标。但是，删除相关的变量可能会导致信息的丢失。为了留住这些变量，我们可以使用惩罚回归模型，如Ridge和Lasso回归。 

我们还可以在相关变量里添加一些随机噪声，使得变量变得彼此不同。但是，增加噪音可能会影响预测的准确度，因此应谨慎使用这种方法。

## 431题

什么时候Ridge回归优于Lasso回归？



解析：

答：你可以引用ISLR的作者Hastie和Tibshirani的话，他们断言在对少量变量有中等或大尺度的影响的时候用lasso回归。在对多个变量只有小或中等尺度影响的时候，使用Ridge回归。 

从概念上讲，我们可以说，Lasso回归（L1）同时做变量选择和参数收缩，而ridge回归只做参数收缩，并最终在模型中包含所有的系数。在有相关变量时，ridge回归可能是首选。此外，ridge回归在用最小二乘估计有更高的偏差的情况下效果最好。因此，选择合适的模型取决于我们的模型的目标。

## 432题

全球平均温度的上升导致世界各地的海盗数量减少。这是否意味着海盗的数量减少引起气候变化？



解析：

答：看完这个问题后，你应该知道这是一个“因果关系和相关性”的经典案例。我们不能断定海盗的数量减少是引起气候变化的原因，因为可能有其他因素（潜伏或混杂因素）影响了这一现象。全球平均温度和海盗数量之间有可能有相关性，但基于这些信息，我们不能说因为全球平均气温的上升而导致了海盗的消失。 

注意：多了解关于因果关系和相关性的知识。

## 433题

如何在一个数据集上选择重要的变量？给出解释。



解析：

答：以下是你可以使用的选择变量的方法：

1.选择重要的变量之前除去相关变量 

2.用线性回归然后基于P值选择变量 

3.使用前向选择，后向选择，逐步选择 

4.使用随机森林和Xgboost，然后画出变量重要性图 

5.使用lasso回归 

6.测量可用的特征集的的信息增益，并相应地选择前n个特征量。

## 434题

是否有可能捕获连续变量和分类变量之间的相关性？如果可以的话，怎样做？



解析：

是的，我们可以用ANCOVA（协方差分析）技术来捕获连续型变量和分类变量之间的相关性。

## 435题

Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？



解析：

答：最根本的区别是，随机森林算法使用bagging技术做出预测。 GBM采用boosting技术做预测。在bagging技术中，数据集用随机采样的方法被划分成使n个样本。然后，使用单一的学习算法，在所有样本上建模。接着利用投票或者求平均来组合所得到的预测。 

Bagging是平行进行的。而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在后续一轮中得到校正。这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。在另一方面，GBM提高了精度，同时减少了模型的偏差和方差。

注意：多了解关于基于树的建模知识。

## 436题

运行二元分类树算法很容易，但是你知道一个树是如何做分割的吗，即树如何决定把哪些变量分到哪个根节点和后续节点上？



解析：

答：分类树利用基尼系数与节点熵来做决定。简而言之，树算法找到最好的可能特征，它可以将数据集分成最纯的可能子节点。树算法找到可以把数据集分成最纯净的可能的子节点的特征量。基尼系数是，如果总体是完全纯的，那么我们从总体中随机选择2个样本，而这2个样本肯定是同一类的而且它们是同类的概率也是1。我们可以用以下方法计算基尼系数： 

1.利用成功和失败的概率的平方和(p^2+q^2)计算子节点的基尼系数

2.利用该分割的节点的加权基尼分数计算基尼系数以分割 

熵是衡量信息不纯的一个标准（二分类）：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS82UicQEHndHQ7j4jPopVkfF7ZaJQzBb8FdFwa5RMYguoZlyWweUT7ITc5knpyibUibEBVx8KfyMmjiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里的p和q是分别在该节点成功和失败的概率。当一个节点是均匀时熵为零。当2个类同时以50%对50%的概率出现在同一个节点上的时候，它是最大值。熵越低越好。

## 437题

你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？



解析：

答：该模型过度拟合。训练误差为0.00意味着分类器已在一定程度上模拟了训练数据，这样的分类器是不能用在未看见的数据上的。 

因此，当该分类器用于未看见的样本上时，由于找不到已有的模式，就会返回的预测有很高的错误率。在随机森林算法中，用了多于需求个数的树时，这种情况会发生。因此，为了避免这些情况，我们要用交叉验证来调整树的数量。

## 438题

你有一个数据集，变量个数p大于观察值个数n。为什么用OLS是一个不好的选择？用什么技术最好？为什么？



解析：

答：在这样的高维数据集中，我们不能用传统的回归技术，因为它们的假设往往不成立。当p>nN，我们不能计算唯一的最小二乘法系数估计，方差变成无穷大，因此OLS无法在此使用的。 

为了应对这种情况，我们可以使用惩罚回归方法，如lasso、LARS、ridge，这些可以缩小系数以减少方差。准确地说，当最小二乘估计具有较高方差的时候，ridge回归最有效。

其他方法还包括子集回归、前向逐步回归。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS82UicQEHndHQ7j4jPopVkfFnwfApia7Ifia2OfzQON2NYicbUaxv9eb2J8dlKSRIn44FRIadTs33qhoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 439题

什么是凸包？（提示：想一想SVM）其他方法还包括子集回归、前向逐步回归。



解析：

答：当数据是线性可分的，凸包就表示两个组数据点的外边界。

一旦凸包建立，我们得到的最大间隔超平面（MMH）作为两个凸包之间的垂直平分线。 MMH是能够最大限度地分开两个组的线。

## 440题

我们知道，一位有效编码会增加数据集的维度。但是，标签编码不会。为什么？



解析：

答：对于这个问题不要太纠结。这只是在问这两者之间的区别。

用一位有效编码编码，数据集的维度（也即特征）增加是因为它为分类变量中存在的的每一级都创建了一个变量。例如：假设我们有一个变量“颜色”。这变量有3个层级，即红色、蓝色和绿色。 

对“颜色”变量进行一位有效编码会生成含0和1值的Color.Red，Color.Blue和Color.Green 三个新变量。在标签编码中，分类变量的层级编码为0和1，因此不生成新变量。标签编码主要是用于二进制变量。

## 441题

你会在时间序列数据集上使用什么交叉验证技术？是用k倍或LOOCV？



解析：

答：都不是。对于时间序列问题，k倍可能会很麻烦，因为第4年或第5年的一些模式有可能跟第3年的不同，而对数据集的重复采样会将分离这些趋势，我们可能最终是对过去几年的验证，这就不对了。 

相反，我们可以采用如下所示的5倍正向链接策略： 

```
fold 1 : training [1], test [2] 
fold 2 : training [1 2], test [3] 
fold 3 : training [1 2 3], test [4] 
fold 4 : training [1 2 3 4], test [5] 
fold 5 : training [1 2 3 4 5], test [6] 
```

1，2，3，4，5，6代表的是年份。

## 442题

给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？



解析：

答：我们可以用下面的方法来处理： 

1.把缺失值分成单独的一类，这些缺失值说不定会包含一些趋势信息。

2.我们可以毫无顾忌地删除它们。

3.或者，我们可以用目标变量来检查它们的分布，如果发现任何模式，我们将保留那些缺失值并给它们一个新的分类，同时删除其他缺失值。

## 443题

“买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？



解析：

答：这种推荐引擎的基本想法来自于协同过滤。 

协同过滤算法考虑用于推荐项目的“用户行为”。它们利用的是其他用户的购买行为和针对商品的交易历史记录、评分、选择和购买信息。针对商品的其他用户的行为和偏好用来推荐项目（商品）给新用户。在这种情况下，项目（商品）的特征是未知的。 

注意：了解更多关于推荐系统的知识。

## 444题

你怎么理解第一类和第二类错误？



解析：

答：第一类错误是当原假设为真时，我们却拒绝了它，也被称为“假阳性”。第二类错误是当原假设为是假时，我们接受了它，也被称为“假阴性”。 

在混淆矩阵里，我们可以说，当我们把一个值归为阳性（1）但其实它是阴性（0）时，发生第一类错误。而当我们把一个值归为阴性（0）但其实它是阳性（1）时，发生了第二类错误。

## 445题

当你在解决一个分类问题时，出于验证的目的，你已经将训练集随机抽样地分成训练集和验证集。你对你的模型能在未看见的数据上有好的表现非常有信心，因为你的验证精度高。但是，在得到很差的精度后，你大失所望。什么地方出了错？



解析：

答：在做分类问题时，我们应该使用分层抽样而不是随机抽样。随机抽样不考虑目标类别的比例。相反，分层抽样有助于保持目标变量在所得分布样本中的分布。

## 446题

在应用机器学习算法之前纠正和清理数据的步骤是什么？



解析：

1.将数据导入

2.看数据：重点看元数据，即对字段解释、数据来源等信息；导入数据后，提取部分数据进行查看

3.缺失值清洗- 根据需要对缺失值进行处理，可以删除数据或填充数据- 重新取数：如果某些非常重要的字段缺失，需要和负责采集数据的人沟通，是否可以再获得 

4.数据格式清洗：统一数据的时间、日期、全半角等显示格式 

5.逻辑错误的数据- 重复的数据- 不合理的值 

6.不一致错误的处理：指对矛盾内容的修正，最常见的如身份证号和出生年月日不对应不同业务中数据清洗的任务略有不同，比如数据有不同来源的话，数据格式清洗和不一致错误的处理就尤为突出。数据预处理是数据类岗位工作内容中重要的部分。

## 447题

在 K-Means 中如何拾取 k？



解析：

K-Means 算法的最大缺点是不能自动选择分类数 k，常见的确定 k 的方法有： 

- 根据先验知识来确定 

- k=2N ，N 为样本数 

- 拐点法：把聚类结果的 F-test 值对聚类个数的曲线画出来，选择图中的拐点 

- 基于信息准则判断，如果模型有似然函数，则可以用 BIC、DIC 来进行决策具体的 k 的选择往往和业务联系紧密，如希望能将用户进行分类，就有先验的分类要求

## 448题

如何理解模型的过拟合与欠拟合，以及如何解决？



解析：

欠拟合（underfiting / high bias）训练误差和验证误差都很大，这种情况称为欠拟合。出现欠拟合的原因是模型尚未学习到数据的真实结构。因此，模拟在训练集和验证集上的性能都很差。 

解决办法 

1 做特征工程，添加跟多的特征项。如果欠拟合是由于特征项不够，没有足够的信息支持模型做判断。 

2 增加模型复杂度。如果模型太简单，不能够应对复杂的任务。可以使用更复杂的模型，减小正则化系数。具体来说可以使用核函数，集成学习方法。 

3 集成学习方法boosting（如GBDT）能有效解决high bias过拟合（overfiting / high variance）模型在训练集上表现很好，但是在验证集上却不能保持准确，也就是模型泛化能力很差。这种情况很可能是模型过拟合。 

造成原因主要有以下几种： 

1 训练数据集样本单一，样本不足。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。

2 训练数据中噪声干扰过大。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。

3 模型过于复杂。模型太复杂，已经能够死记硬背记录下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。 

针对过拟合的上述原因，对应的预防和解决办法如下： 

1 在训练和建立模型的时候，从相对简单的模型开始，不要一开始就把特征做的非常多，模型参数跳的非常复杂。

2 增加样本，要覆盖全部的数据类型。数据经过清洗之后再进行模型训练，防止噪声数据干扰模型。 

3 正则化。在模型算法中添加惩罚函数来防止过拟合。常见的有L1，L2正则化。 

4 集成学习方法bagging(如随机森林）能有效防止过拟合 

5 减少特征个数(不是太推荐)注意：降维不能解决过拟合。降维只是减小了特征的维度，并没有减小特征所有的信息。

## 449题

以下哪种方法属于判别式模型(discriminative model)（ ）

A、隐马模型(HMM)

B、朴素贝叶斯

C、LDA

D、支持向量机



正确答案是：D

解析：

已知输入变量x，判别模型(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。生成模型（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。


常见的判别模型有线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）；常见的生成模型有朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）。


A选项的隐马尔科夫模型和 B选项的朴素贝叶斯属于生成模型。C选项的LDA，如果是指Linear Discriminative Analysis，那么属于判别模型，如果是指 Latent Dirichlet Allocation，那么属于生成模型。D选项的支持向量机属于判别模型。

## 450题

以P(w)表示词条w的概率，假设已知P（南京）=0.8，P（市长）=0.6，P（江大桥）=0.4：P（南京市）=0.3，P（长江大桥）=0.5：如果假设前后两个词的出现是独立的，那么分词结果就是（ ）

A、南京市*长江*大桥

B、南京*市长*江大桥

C、南京市长*江大桥

D、南京市*长江大桥



正确答案是： B

解析：

该题考察的是最大概率分词，其基本思想是：一个待切分的汉字串可能包含多种分词结果，将其中概率最大的作为该字串的分词结果。若某候选词在训练语料中未出现，其概率为0。

A分词结果的概率为P(A)=P(南京市)*P(长江)*P(大桥)，由于“长江”未在语料中出现，所以P(长江)=0，从而P(A)=0;


同理可以算出B, C, D分词结果的概率分别是：    P(B)=P(南京)*P(市长)*P(江大桥)=0.8*0.6*0.4=0.192；

P(C)=P(南京市长)*P(江大桥)=0*0.4=0；    P(D)=P(南京市)*P(长江大桥)=0.3*0.5=0.15。

因为P(B)最大，所以为正确的分词结果。

## 451题

基于统计的分词方法为（ ）

A、正向量最大匹配法

B、逆向量最大匹配法

C、最少切分

D、条件随机场



正确答案是：D

解析：

中文分词的基本方法可以分为基于语法规则的方法、基于词典的方法和基于统计的方法。

基于语法规则的分词法基本思想是在分词的同时进行句法、语义分析, 利用句法信息和语义信息来进行词性标注, 以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂, 基于语法和规则的分词法所能达到的精确度远远还不能令人满意, 目前这种分词系统应用较少。

在基于词典的方法中，可以进一步分为最大匹配法，最大概率法，最短路径法等。最大匹配法指的是按照一定顺序选取字符串中的若干个字当做一个词，去词典中查找。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分。最大概率法指的是一个待切分的汉字串可能包含多种分词结果，将其中概率最大的那个作为该字串的分词结果。最短路径法指的是在词图上选择一条词数最少的路径。

基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合，相邻的字同时出现的次数越多, 就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。常用的方法有HMM（隐马尔科夫模型），MAXENT（最大熵模型），MEMM（最大熵隐马尔科夫模型），CRF（条件随机场）。

本题中，基于统计的方法为条件随机场。ABC三个选项为基于词典的方法。

## 452题

下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）

A、特征灵活

B、速度快

C、可容纳较多上下文信息

D、全局最优



正确答案是：B

解析：

HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。


## 453题

隐马尔可夫模型（HMM），设其观察值空间为

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8RHN4pBsIo26UpZr8MvSFIynFyTTAib3PdqLURKDIY0viaX1rpc6NHrpt5CeWzEq7DM2mgSZKwKjjw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

状态空间为

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8RHN4pBsIo26UpZr8MvSFIJ67Ja2pViaUnQnic8Gc3VWhNiaKv3I1z6TOibDllAic5ceSmqDJ6qZ4EP4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果用维特比算法(Viterbi algorithm)进行解码，时间复杂度为（ ）

A、O(NK)

B、O(NK^2)

C、O(N^2K)


D、以上都不是




正确答案是：D

解析：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8RHN4pBsIo26UpZr8MvSFIBl9WGv4Ktu23kQv0l8bD1E7IHzJPjxDqCUCh57R07hqEfI9ibRvsPzA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 454题

在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）

（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）

A、 Accuracy:(TP+TN)/all

B、 F-value:2*recall*precision/(recall+precision)

C、 G-mean:sqrt(precision*recall)

D、 AUC:ROC曲线下面积



正确答案是：A

解析：

对于分类器，主要的评价指标有precision，recall，F-score，以及ROC曲线等。



在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么(TP+TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。因此A选项是不合理的。在样本不均衡时，可以采用BCD选项方法来评价。

## 455题

下面关于ID3算法中说法错误的是（ ）

A、ID3算法要求特征必须离散化

B、信息增益可以用熵，而不是GINI系数来计算

C、选取信息增益最大的特征，作为树的根节点

D、ID3算法是一个二叉树模型



正确答案是：D

解析：

ID3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：

使用所有没有使用的属性并计算与之相关的样本熵值

选取其中熵值最小的属性

生成包含该属性的节点

D3算法对数据的要求：

1)所有属性必须为离散量；

2)所有的训练例的所有属性必须有一个明确的值；

3)相同的因素必须得到相同的结论且训练例必须唯一。

## 456题

如下表是用户是否使用某产品的调查结果（ ）请计算年龄、地区、学历、收入中对用户是否使用调查产品信息增益最大的属性。（

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8Wd5SuDzf7rrTwTicYD4wtH0NUbcLSGRdhIxzicce17y2Rv1pHjicc1Knuy39ibxWcAIzqLEFnQBcM7A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

）

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8Wd5SuDzf7rrTwTicYD4wtHXmLZCQAic0oX3N9FAY1oFNnnvY5TfZ1Edms2qXYSoPrNWgtaCXg65cA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、年龄

B、地区

C、学历

D、收入



正确答案是：C

解析：

信息增益最大，也就是分类以后信息最少，熵最小。没有划分时，原始数据熵为

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8Wd5SuDzf7rrTwTicYD4wtH3rphuKWLWokSxV1KAibYI7w1hH9ONWDq1QicUuvvspLgcdTicm06tLs2g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果按照年龄进行划分，划分后的熵为


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8Wd5SuDzf7rrTwTicYD4wtHvI9QbG71QJcsicaRI7jq7akEcnoIo1mrOyzB22TQmUcChmTLNmdo2sQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

分别按照熵的方法计算出划分以后的熵值，可以发现按照学历划分以后，熵为0，其他选项都大于0。因此，信息增益最大的属性是学历。



如果不进行计算，可以由观察得出，按照学历划分以后，所有的用户都能正确分类，此时熵最小，信息增益最大。如果按照其他属性分类，都出现了错分的情况，对应的熵大于0。


## 457题

在其它条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（ ）

A、增加训练集数量

B、减少神经网络隐藏层节点数

C、删除稀疏的特征

D、SVM算法中使用高斯核/RBF核代替



正确答案是：D

解析：

机器学习中发生过拟合的主要原因有： 

（1）使用过于复杂的模型； 

（2）数据噪声较大； 

（3）训练数据少。



由此对应的降低过拟合的方法有： 

（1）简化模型假设，或者使用惩罚项限制模型复杂度； 

（2）进行数据清洗，减少噪声； 

（3）收集更多训练数据。 

本题中，A对应于增加训练数据，B为简化模型假设，C为数据清洗。D选项中，高斯核的使用增加了模型复杂度，容易引起过拟合。选择合适的核函数以及软边缘参数C就是训练SVM的重要因素。一般来讲，核函数越复杂，模型越偏向于过拟合；C越大模型越偏向于过拟合，反之则拟合不足。

## 458题

如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）

A、无偏的，有效的

B、无偏的，非有效的

C、有偏的，有效的

D、有偏的，非有效的



正确答案是：B

解析：

OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数OLS估计量的有效性。

## 459题

一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）

A、0.2375

B、0.3275

C、0.5273

D、0.5372



正确答案是：A

解析：

由H(Y|X)= -∑P(X,Y)logP(Y|X)= -∑P(Y|X)P(X)logP(Y|X)，将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，得到H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。

## 460题

Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。

A、M-1维空间

B、一维空间

C、三维空间

D、二维空间



正确答案是： B

解析：

Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。

## 466题

以下哪些方法不可以直接来对文本分类？

A、Kmeans

B、决策树

C、支持向量机

D、KNN



正确答案是：A

解析：

Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。

## 467题

已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）

A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小

B、在经主分量分解后,协方差矩阵成为对角矩阵

C、主分量分析就是K-L变换

D、主分量是通过求协方差矩阵的特征值得到



正确答案是：C

解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。解析参考自：@BlackEyes_SGC

## 468题

关于logit 回归和SVM 不正确的是（ ）

A、Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。

B、Logit回归的输出就是样本属于正类别的几率，可以计算出概率。

C、SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。

D、SVM可以通过正则化系数控制模型的复杂度，避免过拟合。



正确答案是：A

解析：

Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。

## 469题

以下不属于影响聚类算法结果的主要因素有（）

A、已知类别的样本质量

B、分类准则

C、特征选取

D、模式相似性测度



正确答案是：A

解析：

都已知了，就不必再进行聚类了。

## 470题

模式识别中，不属于马式距离较之于欧式距离的优点的是（ ）

A、平移不变性

B、尺度不变性

C、考虑了模式的分布



正确答案是：A

## 471题

影响基本K-均值算法的主要因素有（）

A、样本输入顺序

B、模式相似性测度

C、聚类准则



正确答案是： B

## 472题

在统计模式分类问题中，当先验概率未知时，可以使用（）

A、最小损失准则

B、最小最大损失准则

C、最小误判概率准则



正确答案是： B

## 473题

如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）

A、已知类别样本质量

B、分类准则

C、量纲



正确答案是： B

## 474题

以下属于欧式距离特性的有（）

A、旋转不变性

B、尺度缩放不变性

C、不受量纲影响的特性



正确答案是：A

## 475题

以下( )不属于线性分类器最佳准则？

A、感知准则函数

B、贝叶斯分类

C、支持向量机

D、Fisher准则



正确答案是： B

解析：

线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 

支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题） 

Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 

根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 476题

一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：

A、二分类问题

B、多分类问题

C、层次聚类问题

D、k-中心点聚类问题

E、回归问题

F、结构分析问题



正确答案是： B

解析：

二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。 

层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。 

K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。 

回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。 

结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。 

多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 477题

关于 logit 回归和 SVM 不正确的是（）

A、Logit回归目标函数是最小化后验概率

B、Logit回归可以用于预测事件发生概率的大小

C、SVM目标是结构风险最小化

D、SVM可以有效避免模型过拟合



正确答案是：A

解析：

A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误 

B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确 

C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。 

D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 478题

有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )

A、2x+y=4

B、x+2y=5

C、x+2y=3

D、2x-y=0



正确答案是：C

解析：

这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C.

## 479题

下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？

A、准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率

B、召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率

C、正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高

D、为了解决准确率和召回率冲突问题，引入了F1分数



正确答案是：C

解析：

对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：

TP——将正类预测为正类数 

FN——将正类预测为负类数 

FP——将负类预测为正类数 

TN——将负类预测为负类数 

由此： 

精准率定义为：P = TP / (TP + FP) 

召回率定义为：R = TP / (TP + FN) 

F1值定义为： F1 = 2 P R / (P + R) 

精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。

## 480题

以下几种模型方法属于判别式模型(Discriminative Model)的有( ) 

1)混合高斯模型 

2)条件随机场模型 

3)区分度训练 

4)隐马尔科夫模型

A、2,3

B、3,4

C、1,4

D、1,2



正确答案是：A

解析：

常见的判别式模型有： 

Logistic regression（logistical 回归） 

Linear discriminant analysis（线性判别分析）

Supportvector machines（支持向量机） 

Boosting（集成学习） 

Conditional random fields（条件随机场） 

Linear regression（线性回归） 

Neural networks（神经网络）

常见的生成式模型有: 

Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型） 

Hidden Markov model（隐马尔可夫） 

NaiveBayes（朴素贝叶斯） 

AODE（平均单依赖估计） 

Latent Dirichlet allocation（LDA主题模型） 

Restricted Boltzmann Machine（限制波兹曼机） 

生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 481题

Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）

A、各类别的先验概率P(C)是相等的

B、以0为均值，sqr(2)/2为标准差的正态分布

C、特征变量X的各个维度是类别条件独立随机变量

D、P(X|C)是高斯分布



正确答案是：C

解析：

朴素贝叶斯的条件就是每个变量相互独立。

来源@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 482题

关于支持向量机SVM,下列说法错误的是（）

A、L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

B、Hinge 损失函数，作用是最小化经验分类错误

C、分类间隔为1/||w||，||w||代表向量的模

D、当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习



正确答案是：C

解析：

A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 

B正确。 

C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 

D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 483题

在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()


A、EM算法

B、维特比算法

C、前向后向算法

D、极大似然估计



正确答案是：D

解析：

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法 

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计 

前向后向算法：用来算概率 

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数 

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 484题

在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果（）

A、以做特征选择,并在一定程度上防止过拟合

B、能解决维度灾难问题

C、能加快计算速度

D、可以获得更准确的结果



正确答案是：D

解析：

Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. 

在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。 

对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《范数规则化》（链接：http://blog.csdn.net/zouxy09/article/details/24971995/）。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 485题

机器学习中L1正则化和L2正则化的区别是？

A、使用L1可以得到稀疏的权值

B、使用L1可以得到平滑的权值

C、使用L2可以得到稀疏的权值



正确答案是：A

解析：

L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0. 

L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。

L1正则化/Lasso 

L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

L2正则化/Ridge regression

L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。

对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。

可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。 

因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 486题

位势函数法的积累势函数K(x)的作用相当于Bayes判决中的（）

A、后验概率

B、先验概率

C、类概率密度

D、类概率密度与先验概率的和



正确答案是：A

解析：

具体的，势函数详解请看——《势函数法》。 

来源：@刘炫320，链接：http://blog.csdn.net/column/details/16442.html

## 487题

隐马尔可夫模型三个基本问题以及相应的算法说法错误的是（ ）

A、评估—前向后向算法

B、解码—维特比算法

C、学习—Baum-Welch算法

D、学习—前向后向算法



正确答案是：D

解析：

评估问题，可以使用前向算法、后向算法、前向后向算法。

## 488题

在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？

A、增加训练集量

B、减少神经网络隐藏层节点数

C、删除稀疏的特征

D、SVM算法中使用高斯核/RBF核代替线性核



正确答案是：D

解析：

一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。

B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合 

D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 

本题题目及解析来源：@刘炫320链接：http://blog.csdn.net/column/details/16442.html

## 489题

下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。

A、AR模型

B、MA模型

C、ARMA模型

D、GARCH模型



正确答案是：D

解析：

AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。 

MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。 

ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。 

GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。 

本题题目及解析来源：@刘炫320链接：http://blog.csdn.net/column/details/16442.html

## 490题

以下说法中错误的是（）

A、SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性

B、在adaboost算法中，所有被分错样本的权重更新比例不相同

C、boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重

D、给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的



正确答案是：C

解析：

A 软间隔分类器对噪声是有鲁棒性的。 

B 请参考http://blog.csdn.net/v_july_v/article/details/40718799 

C boosting是根据分类器正确率确定权重，bagging不是。 

D 训练集变大会提高模型鲁棒性。

## 491题

你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicpAQDMJicBB359FFapDgYCVnZ8m6zDxGhMWQdOL2RTZLd61ZHOrib5NLuV0IIDE9cwibj73yWb2aN2A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、第一个 w2 成了 0，接着 w1 也成了 0

B、第一个 w1 成了 0，接着 w2 也成了 0

C、w1 和 w2 同时成了 0

D、即使在 C 成为大值之后，w1 和 w2 都不能成 0



正确答案是：C

解析：

答案是C。L1正则化的函数如下图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。

## 492题

在 k-均值算法中，以下哪个选项可用于获得全局最小？

A、尝试为不同的质心（centroid）初始化运行算法

B、调整迭代的次数

C、找到集群的最佳数量

D、以上所有



正确答案是：D

解析：

答案（D）：所有都可以用来调试以找到全局最小。

## 493题

假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。

A、如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它

B、对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大

C、log-loss 越低，模型越好

D、以上都是



正确答案是：D

## 494题

下面哪个选项中哪一项属于确定性算法？

A、PCA

B、K-Means

C、以上都不是



正确答案是：A

解析：

答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

## 495题

两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？

A、正确

B、错误



正确答案是：A

解析：

答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

## 496题

下面哪个/些超参数的增加可能会造成随机森林数据过拟合？

A、树的数量

B、树的深度

C、学习速率



正确答案是： B

解析：

答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。

## 497题

下列哪个不属于常用的文本分类的特征选择算法？

A、卡方检验值

B、互信息

C、信息增益

D、主成分分析



正确答案是：D

解析：

常采用特征选择方法。常见的六种特征选择方法： 

1）DF(Document Frequency) 文档频率DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性 

2）MI(Mutual Information) 互信息法互信息法用于衡量特征词与文档类别直接的信息量。如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。 

3）(Information Gain) 信息增益法通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。 

4）CHI(Chi-square) 卡方检验法利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。 

5）WLLR(Weighted Log Likelihood Ration)加权对数似然 

6）WFO（Weighted Frequency and Odds）加权频率和可能性 

本题解析来源：http://blog.csdn.net/ztf312/article/details/50890099

## 498题

机器学习中做特征选择时，可能用到的方法有？

A、卡方

B、信息增益

C、平均互信息

D、期望交叉熵

E、以上都有



正确答案是：E

## 499题

下列方法中，不可以用于特征降维的方法包括

A、主成分分析PCA

B、线性判别分析LDA

C、深度学习SparseAutoEncoder

D、矩阵奇异值分解SVD



正确答案是：C

解析：

特征降维方法主要有：PCA，LLE，Isomap 

SVD和PCA类似，也可以看成一种降维方法 

LDA:线性判别分析，可用于降维 

AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出  L2组成，中间则是权重连接。

Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别  进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。 

Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse  惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。 

结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9kEmCdQIdsMJE39ibqqaK11JA6pHDPgCUbiaick3qrdhxBZRLiavC95UD9o1Hfx6Csib5iat4QLA7MIicUQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 500题

下列哪些不特别适合用来对高维数据进行降维

A、LASSO

B、主成分分析法

C、聚类分析

D、小波分析法

E、线性判别法

F、拉普拉斯特征映射



正确答案是：C

解析：

lasso通过参数缩减达到降维的目的； 

pca就不用说了 

线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维； 

小波分析有一些变换的操作降低其他干扰可以看做是降维拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html
